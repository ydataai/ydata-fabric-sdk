{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>YData Fabric is a Data-Centric AI development platform that accelerates AI development by helping data practitioners achieve production-quality data.</p> <p>Much like for software engineering the quality of code is a must for the success of software development, Fabric accounts for the data quality requirements for data-driven applications. It introduces standards, processes, and acceleration to empower data science, analytics, and data engineering teams.</p> <p></p>"},{"location":"#try-fabric","title":"Try Fabric","text":"<ul> <li>Get started with Fabric Community</li> </ul>"},{"location":"#why-adopt-ydata-fabric","title":"Why adopt YData Fabric?","text":"<p>With Fabric, you can standardize the understanding of your data, quickly identify data quality issues, streamline and version your data preparation workflows and finally leverage synthetic data for privacy-compliance or as a tool to boost ML performance. Fabric is a development environment that supports a faster and easier process of preparing data for AI development. Data practitioners are using Fabric to:</p> <ul> <li>Establish a centralized and collaborative repository for data projects.</li> <li>Create and share comprehensive documentation of data, encompassing data schema, structure, and personally identifiable information (PII).</li> <li>Prevent data quality issues with standardized data quality profiling, providing visual understanding and warnings on potential issues.</li> <li>Accelerate data preparation with customizable recipes.</li> <li>Improve machine learning performance with optimal data preparation through solutions such as synthetic data.</li> <li>Shorten access to data with privacy-compliant synthetic data generatio.</li> <li>Build and streamline data preparation workflows effortlessly through a user-friendly drag-and-drop interface.</li> <li>Efficiently manage business rules, conduct comparisons, and implement version control for data workflows using pipelines.</li> </ul>"},{"location":"#key-features","title":"\ud83d\udcdd Key features","text":""},{"location":"#data-catalog","title":"Data Catalog","text":"<p>Fabric Data Catalog provides a centralized perspective on datasets within a project-basis, optimizing data management through seamless integration with the organization's existing data architectures via scalable connectors (e.g., MySQL, Google Cloud Storage, AWS S3). It standardizes data quality profiling, streamlining the processes of efficient data cleaning and preparation, while also automating the identification of Personally Identifiable Information (PII) to facilitate compliance with privacy regulations.</p> <p>Explore how a Data Catalog through a centralized repository of your datasets, schema validation, and automated data profiling.</p>"},{"location":"#labs","title":"Labs","text":"<p>Fabric's Labs environments provide collaborative, scalable, and secure workspaces layered on a flexible infrastructure, enabling users to seamlessly switch between CPUs and GPUs based on their computational needs. Labs are familiar environments that empower data developers with powerful IDEs (Jupyter Notebooks, Visual Code or H2O flow) and a seamless experience with the tools they already love combined with YData's cutting-edge SDK for data preparation.</p> <p>Learn how to use the Labs to generate synthetic data in a familiar Python interface.</p>"},{"location":"#synthetic-data","title":"Synthetic data","text":"<p>Synthetic data, enabled by YData Fabric, provides data developers with a user-friendly interfaces (UI and code) for generating artificial datasets, offering a versatile solution across formats like tabular, time-series and multi-table datasets. The generated synthetic data holds the same value of the original and aligns intricately with specific business rules, contributing to machine learning models enhancement, mitigation of privacy concerns and more robustness for data developments. Fabric offers synthetic data that is ease to adapt and configure, allows customization in what concerns privacy-utility trade-offs.</p> <p>Learn how you to create high-quality synthetic data within a user-friendly UI using Fabric\u2019s data synthesis flow.</p>"},{"location":"#pipelines","title":"Pipelines","text":"<p>Fabric Pipelines streamlines data preparation workflows by automating, orchestrating, and optimizing data pipelines, providing benefits such as flexibility, scalability, monitoring, and reproducibility for efficient and reliable data processing. The intuitive drag-and-drop interface, leveraging Jupyter notebooks or Python scripts, expedites the pipeline setup process, providing data developers with a quick and user-friendly experience.</p> <p>Explore how you can leverage Fabric Pipelines to build versionable and reproducible data preparation workflows for ML development.</p>"},{"location":"#tutorials","title":"Tutorials","text":"<p>To understand how to best apply Fabric to your use cases, start by exploring the following tutorials:</p> <ul> <li> <p>Handling Imbalanced Data for Improved Fraud DetectionLearn how to implement high-performant fraud detection models by incorporating synthetic data to balance your datasets.</p> </li> <li> <p>Prediction with Quality Inspection Learn how to develop data preparation workflows with automated data quality checks and Pipelines.</p> </li> <li> <p>Generating Synthetic Data for Financial TransactionsLearn how to use synthetic data generation to replicate your existing relational databases while ensuring referential integrity.</p> </li> </ul> <p>You can find additional examples and use cases at YData Academy GitHub Repository.</p>"},{"location":"#support","title":"\ud83d\ude4b Support","text":"<p>Facing an issue? We\u2019re committed to providing all the support you need to ensure a smooth experience using Fabric:</p> <ul> <li>Create a support ticket: our team will help you move forward!</li> <li>Contact a Fabric specialist: for personalized guidance or full access to the platform</li> </ul>"},{"location":"examples/synthesize_tabular_data/","title":"Synthesize tabular data","text":"<p>Use YData's RegularSynthesizer to generate tabular synthetic data</p> <pre><code>import os\nfrom ydata.sdk.dataset import get_dataset\nfrom ydata.sdk.synthesizers import RegularSynthesizer\n# Do not forget to add your token as env variables\nos.environ[\"YDATA_TOKEN\"] = '&lt;TOKEN&gt;'  # Remove if already defined\ndef main():\n\"\"\"In this example, we demonstrate how to train a synthesizer from a pandas\n    DataFrame.\n    After training a Regular Synthesizer, we request a sample.\n    \"\"\"\nX = get_dataset('census')\n# We initialize a regular synthesizer\n# As long as the synthesizer does not call `fit`, it exists only locally\nsynth = RegularSynthesizer()\n# We train the synthesizer on our dataset\nsynth.fit(X)\n# We request a synthetic dataset with 50 rows\nsample = synth.sample(n_samples=50)\nprint(sample.shape)\nif __name__ == \"__main__\":\nmain()\n</code></pre>"},{"location":"examples/synthesize_timeseries_data/","title":"Synthesize time-series data","text":"<p>Use YData's TimeSeriesSynthesizer to generate time-series synthetic data</p> <p>Tabular data is the most common type of data we encounter in data problems.</p> <p>When thinking about tabular data, we assume independence between different records, but this does not happen in reality. Suppose we check events from our day-to-day life, such as room temperature changes, bank account transactions, stock price fluctuations, and air quality measurements in our neighborhood. In that case, we might end up with datasets where measures and records evolve and are related through time. This type of data is known to be sequential or time-series data.</p> <p>Thus, sequential or time-series data refers to any data containing elements ordered into sequences in a structured format. Dissecting any time-series dataset, we see differences in variables' behavior that need to be understood for an effective generation of synthetic data. Typically any time-series dataset is composed of the following:</p> <ul> <li>Variables that define the order of time (these can be simple with one variable or composed)</li> <li>Time-variant variables</li> <li>Variables that refer to entities (single or multiple entities)</li> <li>Variables that are attributes (those that don't depend on time but rather on the entity)</li> </ul> <p>Below find an example:</p> <pre><code>import os\nfrom ydata.sdk.dataset import get_dataset\nfrom ydata.sdk.synthesizers import TimeSeriesSynthesizer\n# Do not forget to add your token as env variable\nos.environ[\"YDATA_TOKEN\"] = '&lt;TOKEN&gt;'\nX = get_dataset('occupancy')\n# We initialize a time series synthesizer\n# As long as the synthesizer does not call `fit`, it exists only locally\nsynth = TimeSeriesSynthesizer()\n# We train the synthesizer on our dataset\n# sortbykey -&gt; variable that define the time order for the sequence\nsynth.fit(X, sortbykey='date')\n# By default it is requested a synthetic sample with the same length as the original data\n# The TimeSeriesSynthesizer is designed to replicate temporal series and therefore the original time-horizon is respected\nsample = synth.sample(n_entities=1)\n</code></pre>"},{"location":"examples/synthesize_with_anonymization/","title":"Anonymization","text":"<p>YData Synthesizers offers a way to anonymize sensitive information such that the original values are not present in the synthetic data but replaced by fake values.</p> <p>Does the model retain the original values?</p> <p>No! The anonymization is performed before the model training such that it never sees the original values.</p> <p>The anonymization is performed by specifying which columns need to be anonymized and how to perform the anonymization. The anonymization rules are defined as a dictionary with the following format:</p> <p><code>{column_name: anonymization_rule}</code></p> <p>While here are some predefined anonymization rules such as <code>name</code>, <code>email</code>, <code>company</code>, it is also possible to create a rule using a regular expression. The anonymization rules have to be passed to a synthesizer in its <code>fit</code> method using the parameter <code>anonymize</code>.</p> <p>What is the difference between anonymization and privacy?</p> <p>Anonymization makes sure sensitive information are hidden from the data. Privacy makes sure it is not possible to infer the original data points from the synthetic data points via statistical attacks.</p> <p>Therefore, for data sharing anonymization and privacy controls are complementary.</p> <p>The example below demonstrates how to anonymize the column <code>Name</code> by fake names and the column <code>Ticket</code> by a regular expression: <pre><code>import os\nfrom ydata.sdk.dataset import get_dataset\nfrom ydata.sdk.synthesizers import RegularSynthesizer\n# Do not forget to add your token as env variables\nos.environ[\"YDATA_TOKEN\"] = '&lt;TOKEN&gt;'  # Remove if already defined\ndef main():\n\"\"\"In this example, we demonstrate how to train a synthesizer from a pandas\n    DataFrame.\n    After training a Regular Synthesizer, we request a sample.\n    \"\"\"\nX = get_dataset('titanic')\n# We initialize a regular synthesizer\n# As long as the synthesizer does not call `fit`, it exists only locally\nsynth = RegularSynthesizer()\n# We define anonymization rules, which is a dictionary with format:\n# {column_name: anonymization_rule, ...}\n# while here are some predefined anonymization rules like: name, email, company\n# it is also possible to create a rule using a regular expression\nrules = {\n\"Name\": \"name\",\n\"Ticket\": \"[A-Z]{2}-[A-Z]{4}\"\n}\n# We train the synthesizer on our dataset\nsynth.fit(\nX,\nname=\"titanic_synthesizer\",\nanonymize=rules\n)\n# We request a synthetic dataset with 50 rows\nsample = synth.sample(n_samples=50)\nprint(sample[[\"Name\", \"Ticket\"]].head(3))\nif __name__ == \"__main__\":\nmain()\n</code></pre></p>"},{"location":"examples/synthesize_with_conditional_sampling/","title":"Conditional sampling","text":"<p>YData Synthesizers support conditional sampling. The <code>fit</code> method has an optional parameter named <code>condition_on</code>, which receives a list of features to condition upon. Furthermore, the <code>sample</code> method receives the conditions to be applied through another optional parameter also named <code>condition_on</code>. For now, two types of conditions are supported:</p> <ul> <li>Condition upon a categorical (or string) feature. The parameters are the name of the feature and a list of values (i.e., categories) to be considered. Each category also has its percentage of representativeness. For example, if we want to condition upon two categories, we need to define the percentage of rows each of these categories will have on the synthetic dataset. Naturally, the sum of such percentages needs to be 1. The default percentage is also 1 since it is the required value for a single category.</li> <li>Condition upon a numerical feature. The parameters are the name of the feature and the minimum and maximum of the range to be considered. This feature will present a uniform distribution on the synthetic dataset, limited by the specified range.</li> </ul> <p>The example below demonstrates how to train and sample from a synthesizer using conditional sampling:</p> <pre><code>import os\nfrom ydata.sdk.dataset import get_dataset\nfrom ydata.sdk.synthesizers import RegularSynthesizer\n# Do not forget to add your token as env variables.\nos.environ[\"YDATA_TOKEN\"] = '&lt;TOKEN&gt;'  # Remove if already defined.\ndef main():\n\"\"\"In this example, we demonstrate how to train and\n    sample from a synthesizer using conditional sampling.\"\"\"\nX = get_dataset('census')\n# We initialize a regular synthesizer.\n# As long as the synthesizer does not call `fit`, it exists only locally.\nsynth = RegularSynthesizer()\n# We train the synthesizer on our dataset setting\n# the features to condition upon.\nsynth.fit(\nX,\nname=\"census_synthesizer\",\ncondition_on=[\"sex\", \"native-country\", \"age\"]\n)\n# We request a synthetic dataset with specific condition rules.\nsample = synth.sample(\nn_samples=500,\ncondition_on={\n\"sex\": {\n\"categories\": [\"Female\"]\n},\n\"native-country\": {\n\"categories\": [(\"United-States\", 0.6),\n(\"Mexico\", 0.4)]\n},\n\"age\": {\n\"minimum\": 55,\n\"maximum\": 60\n}\n}\n)\nprint(sample)\nif __name__ == \"__main__\":\nmain()\n</code></pre>"},{"location":"examples/synthesize_with_privacy_control/","title":"Privacy control","text":"<p>YData Synthesizers offers 3 different levels of privacy:</p> <ol> <li>high privacy: the model is optimized for privacy purposes,</li> <li>high fidelity (default): the model is optimized for high fidelity,</li> <li>balanced: tradeoff between privacy and fidelity.</li> </ol> <p>The default privacy level is high fidelity. The privacy level can be changed by the user at the moment a synthesizer level is trained by using the parameter <code>privacy_level</code>. The parameter expect a <code>PrivacyLevel</code> value.</p> <p>What is the difference between anonymization and privacy?</p> <p>Anonymization makes sure sensitive information are hidden from the data. Privacy makes sure it is not possible to infer the original data points from the synthetic data points via statistical attacks.</p> <p>Therefore, for data sharing anonymization and privacy controls are complementary.</p> <p>The example below demonstrates how to train a synthesizer configured for high privacy:</p> <pre><code>import os\nfrom ydata.sdk.dataset import get_dataset\nfrom ydata.sdk.synthesizers import PrivacyLevel, RegularSynthesizer\n# Do not forget to add your token as env variables\nos.environ[\"YDATA_TOKEN\"] = '&lt;TOKEN&gt;'  # Remove if already defined\ndef main():\n\"\"\"In this example, we demonstrate how to train a synthesizer\n    with a high-privacy setting from a pandas DataFrame.\n    After training a Regular Synthesizer, we request a sample.\n    \"\"\"\nX = get_dataset('titanic')\n# We initialize a regular synthesizer\n# As long as the synthesizer does not call `fit`, it exists only locally\nsynth = RegularSynthesizer()\n# We train the synthesizer on our dataset setting the privacy level to high\nsynth.fit(\nX,\nname=\"titanic_synthesizer\",\nprivacy_level=PrivacyLevel.HIGH_PRIVACY\n)\n# We request a synthetic dataset with 50 rows\nsample = synth.sample(n_samples=50)\nprint(sample)\nif __name__ == \"__main__\":\nmain()\n</code></pre>"},{"location":"get-started/","title":"Get started with Fabric","text":"<p>The get started is here to help you if you are not yet familiar with YData Fabric or if you just want to learn more about data quality, data preparation workflows and how you can start leveraging synthetic data. Mention to YData Fabric Community</p>"},{"location":"get-started/#create-your-first-data-with-the-data-catalog","title":"\ud83d\udcda Create your first Data with the Data Catalog","text":""},{"location":"get-started/#create-your-first-synthetic-data-generator","title":"\u2699\ufe0f Create your first Synthetic Data generator","text":""},{"location":"get-started/#create-your-first-lab","title":"\ud83e\uddea Create your first Lab","text":""},{"location":"get-started/#create-your-first-data-pipeline","title":"\ud83c\udf00 Create your first data Pipeline","text":""},{"location":"get-started/create_lab/","title":"How to create your first Lab environment","text":"<p>Labs are code environments for a more flexible development of data-driven solutions while leveraging Fabric capabilities combined with already loved tools such as scikit-learn, numpy and pandas. To create your first Lab, you can use the \u201cCreate Lab\u201d from Fabric\u2019s home, or you can access it from the Labs module by selecting it on the left side menu, and clicking the \u201cCreate Lab\u201d button.</p> <p> </p> <p>Next, a menu with different IDEs will be shown. As a quickstart select Jupyter Lab. As labs are development environments you will be also asked what language you would prefer your environment to support: R or Python. Select Python.</p> <p>Bundles are environments with pre-installed packages. Select YData bundle, so we can leverage some other Fabric features such as Data Profiling, Synthetic Data and Pipelines.</p> <p>As a last step, you will be asked to configure the infrastructure resources for this new environment as well as giving it a Display Name. We will keep the defaults, but you have flexibility to select GPU acceleration or whether you need more computational resources for your developments.</p> <p>Finally, your Lab will be created and added to the \"Labs\" list, as per the image below. The status of the lab will be \ud83d\udfe1 while preparing, and this process takes a few minutes, as the infrastructure is being allocated to your development environment. As soon as the status changes to \ud83d\udfe2, you can open your lab by clicking in the button as shown below:</p> <p>Create a new notebook in the JupyterLab and give it a name. You are now ready to start your developments!</p> <p>Congrats! \ud83d\ude80 You have now successfully created your first Lab a code environment, so you can benefit from the most advanced Fabric features as well as compose complex data workflows. Get ready for your journey of improved quality data for AI.</p>"},{"location":"get-started/create_pipeline/","title":"How to create your first Pipeline","text":"<p> Check this quickstart video on how to create your first Pipeline.</p> <p>The best way to get started with Pipelines is to use the interactive Pipeline editor available in the Labs with Jupyter Lab set as IDE. If you don't have a Lab yet, or you don't know how to create one, check our quickstart guide on how to create your first lab.</p> <p>Open an already existing lab.</p> <p>A Pipeline comprises one or more nodes that are connected (or not!) with each other to define execution dependencies. Each pipeline node is and should be implemented as a component that is expected to manage a single task, such as read the data, profiling the data, training a model, or even publishing a model to production environments.</p> <p>In this tutorial we will build a simple and generic pipeline that use a Dataset from Fabric's Data Catalog and profile to check it's quality. We have the notebooks template already available. For that you need to access the \"Academy\" folder as per the image below.</p> <p>Make sure to copy all the files in the folder \"3 - Pipelines/quickstart\" to the root folder of your lab, as per the image below.</p> <p>Now that we have our notebooks we need to make a small change in the notebook \"1. Read dataset\". Go back to your Data Catalog, from one of the datasets in your Catalog list, select the three vertical dots and click in \"Explore in Labs\" as shown in the image below.</p> <p>The following screen will be shown. Click in copy.</p> <p>Now that we have copied the code, let's get back to our \"1. Read data.ipynb\" notebook, and replace the first code cell by with the new code. This will allow us to use a dataset from the Data Catalog in our pipeline.</p> <p>With our notebooks ready, we can now configure our Pipeline. For this quickstart we will be leveraging an already existing pipeline - double-click the file my_first_pipeline.pipeline. You should see a pipeline as depicted in the images below. To create a new Pipeline, you can open the lab launcher tab and select \"Pipeline Editor\".</p> <p>Before running the pipeline, we need to check each component/step properties and configurations. Right-click each one of the steps, select \"Open Properties\", and a menu will be depicted in your right side. Make sure that you have \"YData - CPU\" selected as the Runtime Image as show below.</p> <p>We are now ready to create and run our first pipeline. In the top left corner of the pipeline editor, the run button will be available for you to click.</p> <p>Accept the default values shown in the run dialog and start the run</p> <p>If the following message is shown, it means that you have create a run of your first pipeline.</p> <p>Now that you have created your first pipeline, you can select the Pipeline from Fabric's left side menu.</p> <p>Your most recent pipeline will be listed, as shown in below image.</p> <p>To check the run of your pipeline, jump into the \"Run\" tab. You will be able to see your first pipeline running!</p> <p>By clicking on top of the record you will be able to see the progress of the run step-by-step, and visualize the outputs of each and every step by clicking on each step and selecting the Visualizations tab.</p> <p>Congrats! \ud83d\ude80 You have now successfully created your first Pipeline a code environment, so you can benefit from Fabric's orchestration engine to crate scalable, versionable and comparable data workflows. Get ready for your journey of improved quality data for AI.</p>"},{"location":"get-started/create_syntheticdata_generator/","title":"How to create your first Synthetic Data generator","text":"<p> Check this quickstart video on how to create your first Synthetic Data generator.</p> <p>To generate your first synthetic data, you need to have a Dataset already available in your Data Catalog. Check this tutorial to see how you can add your first dataset to Fabric\u2019s Data Catalog.</p> <p>With your first dataset created, you are now able to start the creation of your Synthetic Data generator. You can either select \"Synthetic Data\" from your left side menu, or you can select \"Create Synthetic Data\" in your project Home as shown in the image below.</p> <p>You'll be asked to select the dataset you wish to generate synthetic data from and verify the columns you'd like to include in the synthesis process, validating their Variable and Data Types.</p> <p>Data types are relevant for synthetic data quality</p> <p>Data Types are important to be revisited and aligned with the objectives for the synthetic data as they can highly impact the quality of the generated data. For example, let's say we have a column that is a \"Name\", while is some situations it would make sense to consider it a String, under the light of a dataset where \"Name\" refers to the name of the product purchases, it might be more beneficial to set it as a Category.</p> <p>Finally, as the last step of our process it comes the Synthetic Data specific configurations, for this particular case we only need to define a Display Name, and we can finish the process by clicking in the \"Save\" button as per the image below.</p> <p>Your Synthetic Data generator is now training and listed under \"Synthetic Data\". While the model is being trained, the Status will be \ud83d\udfe1, as soon as the training is completed successfully it will transition to \ud83d\udfe2 as per the image below.</p> <p>Once the Synthetic Data generator has finished training, you're ready to start generating your first synthetic dataset. You can start by exploring an overview of the model configurations and even download a PDF report with a comprehensive overview of your Synthetic Data Quality Metrics. Next, you can generate synthetic data samples by accessing the Generation tab or click on \"Go to Generation\".</p> <p>In this section, you are able to generate as many synthetic samples as you want. For that you need to define the number rows to generate and click \"Generate\", as depicted in the image below.</p> <p>A new line in your \"Sample History\" will be shown and as soon as the sample generation is completed you will be able to \"Compare\" your synthetic data with the original data, add as a Dataset with \"Add to Data Catalog\" and last but not the least download it as a file with \"Download csv\".</p> <p>Congrats! \ud83d\ude80 You have now successfully created your first Synthetic Data generator with Fabric. Get ready for your journey of improved quality data for AI.</p>"},{"location":"get-started/fabric_community/","title":"Get started with Fabric Community","text":"<p>Fabric Community is a SaaS version that allows you to explore all the functionalities of Fabric first-hand: free, forever, for everyone. You\u2019ll be able to validate your data quality with automated profiling, unlock data sharing and improve your ML models with synthetic data, and increase your productivity with seamless integration:</p> <ul> <li>Build 1 personal project;</li> <li>Create your first Data Catalog and benefit from automated data profiling;</li> <li>Train and generate synthetic data up to 2 models and datasets with 50 columns and 100K rows;</li> <li>Optimize synthetic data quality for your use cases with an evaluation PDF report;</li> <li>Create 1 development environment (Labs) and integrate it with your familiar ML packages and workflows.</li> </ul>"},{"location":"get-started/fabric_community/#register","title":"Register","text":"<p>To register for Fabric Community:</p> <ul> <li>Access the Fabric Community Try Now and create your YData account by submitting the form</li> <li>Check your email for your login credentials</li> <li>Login into fabric.ydata.ai and enjoy!</li> </ul> <p></p> <p>Once you login, you'll access the Home page and get started with your data preparation!</p> <p></p>"},{"location":"get-started/upload_csv/","title":"How to create your first Dataset from a CSV file","text":"<p> Check this quickstart video on how to create your first Dataset from a CSV file.</p> <p>To create your first dataset in the Data Catalog, you can start by clicking on \"Add Dataset\" from the Home section. Or click to Data Catalog (on the left side menu) and click \u201cAdd Dataset\u201d.</p> <p>After that the below modal will be shown. You will need to select a connector. To upload a CSV file, we need to select \u201cUpload CSV\u201d.</p> <p>Once you've selected the \u201cUpload CSV\u201d connector, a new screen will appear, enabling you to upload your file and designate a name for your connector. This file upload connector will subsequently empower you to create one or more datasets from the same file at a later stage.</p> <p>With the Connector created, you'll be able to add a dataset and specify its properties:</p> <ul> <li>Name: The name of your dataset;</li> <li>Separator: This is an important parameter to make sure that we can parse your CSV correctly. The default value is \u201c,\u201d.</li> <li>Data Type: Whether your dataset contains tabular or time-series (i.e., containing temporal dependency) data.</li> </ul> <p>Your created Connector (\u201cCensus File\u201d) and Dataset (\u201cCensus\u201d) will be added to the Data Catalog. As soon as the status is green, you can navigate your Dataset. Click in Open Dataset as per the image below.</p> <p>Within the Dataset details, you can gain valuable insights through our automated data quality profiling. This includes comprehensive metadata and an overview of your data, encompassing details like row count, identification of duplicates, and insights into the overall quality of your dataset.</p> <p></p> <p>Or perhaps, you want to further explore through visualization, the profile of your data with both univariate and multivariate of your data.</p> <p>Congrats! \ud83d\ude80 You have now successfully created your first Connector and Dataset in Fabric\u2019s Data Catalog. Get ready for your journey of improved quality data for AI.</p>"},{"location":"sdk/","title":"Overview","text":"<p>YData SDK for improved data quality everywhere!</p> <p>ydata-sdk is here! Create a YData account so you can start using today!</p> <p>Create account</p>"},{"location":"sdk/#overview","title":"Overview","text":"<p>The YData SDK is an ecosystem of methods that allows users to, through a python interface, adopt a Data-Centric approach towards the AI development. The solution includes a set of integrated components for data ingestion, standardized data quality evaluation and data improvement, such as synthetic data generation, allowing an iterative improvement of the datasets used in high-impact business applications.</p> <p>Synthetic data can be used as Machine Learning performance enhancer, to augment or mitigate the presence of bias in real data. Furthermore, it can be used as a Privacy Enhancing Technology, to enable data-sharing initiatives or even to fuel testing environments.</p> <p>Under the YData-SDK hood, you can find a set of algorithms and metrics based on statistics and deep learning based techniques, that will help you to accelerate your data preparation.</p>"},{"location":"sdk/#current-functionality","title":"Current functionality","text":"<p>YData SDK is currently composed by the following main modules:</p> <ul> <li> <p>Datasources</p> <ul> <li>YData\u2019s SDK includes several connectors for easy integration with existing data sources. It supports several storage types, like filesystems and RDBMS. Check the list of connectors.</li> <li>SDK\u2019s Datasources run on top of Dask, which allows it to deal with not only small workloads but also larger volumes of data.</li> </ul> </li> <li> <p>Synthesizers</p> <ul> <li>Simplified interface to train a generative model and learn in a data-driven manner the behavior, the patterns and original data distribution. Optimize your model for privacy or utility use-cases.</li> <li>From a trained synthesizer, you can generate synthetic samples as needed and parametrise the number of records needed.</li> <li>Anonymization and privacy preserving capabilities to ensure that synthetic datasets does not contain Personal Identifiable Information (PII) and can safely be shared!</li> <li>Conditional sampling can be used to restrict the domain and values of specific features in the sampled data.</li> </ul> </li> <li> <p>Synthetic data quality report Coming soon</p> <ul> <li>An extensive synthetic data quality report that measures 3 dimensions: privacy, utility and fidelity of the generated data. The report can be downloaded in PDF format for ease of sharing and compliance purposes or as a JSON to enable the integration in data flows.</li> </ul> </li> <li> <p>Profiling Coming soon</p> <ul> <li>A set of metrics and algorithms summarizes datasets quality in three main dimensions: warnings, univariate analysis and a multivariate perspective.</li> </ul> </li> </ul>"},{"location":"sdk/#supported-data-formats","title":"Supported data formats","text":"TabularTime-SeriesTransactionalRelational databases <p> The RegularSynthesizer is perfect to synthesize high-dimensional data, that is time-indepentent with high quality results.</p> <p>Know more</p> <p> The TimeSeriesSynthesizer is perfect to synthesize both regularly and not evenly spaced time-series, from smart-sensors to stock.</p> <p>Know more</p> <p> The TimeSeriesSynthesizer supports transactional data, known to have highly irregular time intervals between records and directional relations between entities.</p> <p>Coming soon</p> <p>Know more</p> <p> The MultiTableSynthesizer is perfect to learn how to replicate the data within a relational database schema.</p> <p>Coming soon</p> <p>Know more</p>"},{"location":"sdk/installation/","title":"Installation","text":"<p>YData SDK is generally available through both Pypi and Conda allowing an easy process of installation. This experience allows combining YData SDK with other packages such as Pandas, Numpy or Scikit-Learn.</p> <p>YData SDK is available for the public through a token-based authentication system. If you don\u2019t have one yet, you can get your free license key during the installation process. You can check what features are available in the free version here.</p>"},{"location":"sdk/installation/#installing-the-package","title":"Installing the package","text":"<p>YData SDK supports python versions bigger than python 3.8, and can be installed in Windows, Linux or MacOS operating systems.</p> <p>Prior to the package installation, it is recommended the creation of a virtual or conda environment:</p> pyenv <pre><code>pyenv virtualenv 3.10 ydatasdk\n</code></pre> <p>And install <code>ydata-sdk</code></p> pypi <pre><code>pip install ydata-sdk\n</code></pre>"},{"location":"sdk/installation/#authentication","title":"Authentication","text":"<p>Once you've installed <code>ydata-sdk</code> package you will need a token to run the functionalities. YData SDK uses a token based authentication system. To get access to your token, you need to create a YData account.</p> <p>YData SDK offers a free-trial and an enterprise version. To access your free-trial token, you need to create a YData account.</p> <p>The token will be available here, after login:</p> <p></p> <p>With your account toke copied, you can set a new environment variable <code>YDATA_TOKEN</code> in the beginning of your development session.</p> <pre><code>    import os\nos.setenv['YDATA_TOKEN'] = '{add-your-token}'\n</code></pre> <p>Once you have set your token, you are good to go to start exploring the incredible world of data-centric AI and smart synthetic data generation!</p> <p>Check out our quickstart guide!</p>"},{"location":"sdk/quickstart/","title":"Quickstart","text":"<p>YData SDK allows you to with an easy and familiar interface, to adopt a Data-Centric AI approach for the development of Machine Learning solutions. YData SDK features were designed to support structure data, including tabular data, time-series and transactional data.</p>"},{"location":"sdk/quickstart/#read-data","title":"Read data","text":"<p>To start leveraging the package features you should consume your data either through the Connectors or pandas.Dataframe. The list of available connectors can be found here [add a link].</p> From pandas dataframeFrom a connector <pre><code>    # Example for a Google Cloud Storage Connector\ncredentials = \"{insert-credentials-file-path}\"\n# We create a new connector for Google Cloud Storage\nconnector = Connector(connector_type='gcs', credentials=credentials)\n# Create a Datasource from the connector\n# Note that a connector can be re-used for several datasources\nX = DataSource(connector=connector, path='gs://&lt;my_bucket&gt;.csv')\n</code></pre> <pre><code>    # Load a small dataset\nX = pd.read_csv('{insert-file-path.csv}')\n# Init a synthesizer\nsynth = RegularSynthesizer()\n# Train the synthesizer with the pandas Dataframe as input\n# The data is then sent to the cluster for processing\nsynth.fit(X)\n</code></pre> <p>The synthesis process returns a <code>pandas.DataFrame</code> object. Note that if you are using the <code>ydata-sdk</code> free version, all of your data is sent to a remote cluster on YData's infrastructure.</p>"},{"location":"sdk/quickstart/#data-synthesis-flow","title":"Data synthesis flow","text":"<p>The process of data synthesis can be described into the following steps:</p> <pre><code>stateDiagram-v2\n  state read_data\n  read_data --&gt; init_synth\n  init_synth --&gt; train_synth\n  train_synth --&gt; generate_samples\n  generate_samples --&gt; [*]</code></pre> <p>The code snippet below shows how easy can be to start generating new synthetic data. The package includes a set of examples datasets for a quickstart.</p> <pre><code>    from ydata.sdk.dataset import get_dataset\n#read the example data\nX = get_dataset('census')\n# Init a synthesizer\nsynth = RegularSynthesizer()\n# Fit the synthesizer to the input data\nsynth.fit(X)\n# Sample new synthetic data. The below request ask for new 1000 synthetic rows\nsynth.sample(n_samples=1000)\n</code></pre> <p>Do I need to prepare my data before synthesis?</p> <p>The sdk ensures that the original behaviour is replicated. For that reason, there is no need to preprocess outlier observations or missing data.</p> <p>By default all the missing data is replicated as NaN.</p>"},{"location":"sdk/modules/connectors/","title":"Connectors","text":"<p>YData SDK allows users to consume data assets from remote storages through Connectors. YData Connectors support different types of storages, from filesystems to RDBMS'.</p> <p>Below the list of available connectors:</p> Connector Name Type Supported File Types Useful Links Notes AWS S3 Remote object storage CSV, Parquet https://aws.amazon.com/s3/ Google Cloud Storage Remote object storage CSV, Parquet https://cloud.google.com/storage Azure Blob Storage Remote object storage CSV, Parquet https://azure.microsoft.com/en-us/services/storage/blobs/ File Upload Local CSV - Maximum file size is 220MB. Bigger files should be uploaded and read from remote object storages MySQL RDBMS Not applicable https://www.mysql.com/ Supports reading whole schemas or specifying a query Azure SQL Server RDBMS Not applicable https://azure.microsoft.com/en-us/services/sql-database/campaign/ Supports reading whole schemas or specifying a query PostgreSQL RDBMS Not applicable https://www.postgresql.org/ Supports reading whole schemas or specifying a query Snowflake RDBMS Not applicable https://docs.snowflake.com/en/sql-reference-commands Supports reading whole schemas or specifying a query Google BigQuery Data warehouse Not applicable https://cloud.google.com/bigquery Azure Data Lake Data lake CSV, Parquet https://azure.microsoft.com/en-us/services/storage/data-lake-storage/ <p>More details can be found at Connectors APi Reference Docs.</p>"},{"location":"sdk/modules/synthetic_data/","title":"Synthetic data generation","text":""},{"location":"sdk/modules/synthetic_data/#data-formats","title":"Data formats","text":""},{"location":"sdk/modules/synthetic_data/#tabular-data","title":"Tabular data","text":""},{"location":"sdk/modules/synthetic_data/#time-series-data","title":"Time-series data","text":""},{"location":"sdk/modules/synthetic_data/#transactions-data","title":"Transactions data","text":""},{"location":"sdk/modules/synthetic_data/#best-practices","title":"Best practices","text":""},{"location":"sdk/reference/api/common/client/","title":"Get client","text":"<p>Deduce how to initialize or retrieve the client.</p> <p>This is meant to be a zero configuration for the user.</p> Create and set a client globally <pre><code>from ydata.sdk.client import get_client\nget_client(set_as_global=True)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>client_or_creds</code> <code>Optional[Union[Client, dict, str, Path]]</code> <p>Client to forward or credentials for initialization</p> <code>None</code> <code>set_as_global</code> <code>bool</code> <p>If <code>True</code>, set client as global</p> <code>False</code> <code>wait_for_auth</code> <code>bool</code> <p>If <code>True</code>, wait for the user to authenticate</p> <code>True</code> <p>Returns:</p> Type Description <code>Client</code> <p>Client instance</p> Source code in <code>ydata/sdk/common/client/utils.py</code> <pre><code>def get_client(client_or_creds: Optional[Union[Client, Dict, str, Path]] = None, set_as_global: bool = False, wait_for_auth: bool = True) -&gt; Client:\n\"\"\"Deduce how to initialize or retrieve the client.\n    This is meant to be a zero configuration for the user.\n    Example: Create and set a client globally\n            ```py\n            from ydata.sdk.client import get_client\n            get_client(set_as_global=True)\n            ```\n    Args:\n        client_or_creds (Optional[Union[Client, dict, str, Path]]): Client to forward or credentials for initialization\n        set_as_global (bool): If `True`, set client as global\n        wait_for_auth (bool): If `True`, wait for the user to authenticate\n    Returns:\n        Client instance\n    \"\"\"\nclient = None\nglobal WAITING_FOR_CLIENT\ntry:\n# If a client instance is set globally, return it\nif not set_as_global and Client.GLOBAL_CLIENT is not None:\nreturn Client.GLOBAL_CLIENT\n# Client exists, forward it\nif isinstance(client_or_creds, Client):\nreturn client_or_creds\n# Explicit credentials\n''' # For the first version, we deactivate explicit credentials via string or file for env var only\n        if isinstance(client_or_creds, (dict, str, Path)):\n            if isinstance(client_or_creds, str):  # noqa: SIM102\n                if Path(client_or_creds).is_file():\n                    client_or_creds = Path(client_or_creds)\n            if isinstance(client_or_creds, Path):\n                client_or_creds = json.loads(client_or_creds.open().read())\n            return Client(credentials=client_or_creds)\n        # Last try with environment variables\n        #if client_or_creds is None:\n        client = _client_from_env(wait_for_auth=wait_for_auth)\n        '''\ncredentials = environ.get(TOKEN_VAR)\nif credentials is not None:\nclient = Client(credentials=credentials)\nexcept ClientHandshakeError as e:\nwait_for_auth = False  # For now deactivate wait_for_auth until the backend is ready\nif wait_for_auth:\nWAITING_FOR_CLIENT = True\nstart = time()\nlogin_message_printed = False\nwhile client is None:\nif not login_message_printed:\nprint(\nf\"The token needs to be refreshed - please validate your token by browsing at the following URL:\\n\\n\\t{e.auth_link}\")\nlogin_message_printed = True\nwith suppress(ClientCreationError):\nsleep(BACKOFF)\nclient = get_client(wait_for_auth=False)\nnow = time()\nif now - start &gt; CLIENT_INIT_TIMEOUT:\nWAITING_FOR_CLIENT = False\nbreak\nif client is None and not WAITING_FOR_CLIENT:\nsys.tracebacklimit = None\nraise ClientCreationError\nreturn client\n</code></pre> <p>Main Client class used to abstract the connection to the backend.</p> <p>A normal user should not have to instanciate a <code>Client</code> by itself. However, in the future it will be useful for power-users to manage projects and connections.</p> <p>Parameters:</p> Name Type Description Default <code>credentials</code> <code>Optional[dict]</code> <p>(optional) Credentials to connect</p> <code>None</code> <code>project</code> <code>Optional[Project]</code> <p>(optional) Project to connect to. If not specified, the client will connect to the default user's project.</p> <code>None</code> Source code in <code>ydata/sdk/common/client/client.py</code> <pre><code>@typechecked\nclass Client(metaclass=SingletonClient):\n\"\"\"Main Client class used to abstract the connection to the backend.\n    A normal user should not have to instanciate a [`Client`][ydata.sdk.common.client.Client] by itself.\n    However, in the future it will be useful for power-users to manage projects and connections.\n    Args:\n        credentials (Optional[dict]): (optional) Credentials to connect\n        project (Optional[Project]): (optional) Project to connect to. If not specified, the client will connect to the default user's project.\n    \"\"\"\ncodes = codes\ndef __init__(self, credentials: Optional[Union[str, Dict]] = None, project: Optional[Project] = None, set_as_global: bool = False):\nself._base_url = environ.get(\"YDATA_BASE_URL\", DEFAULT_URL)\nself._scheme = 'https'\nself._headers = {'Authorization': credentials}\nself._http_client = httpClient(\nheaders=self._headers, timeout=Timeout(10, read=None))\nself._handshake()\nself._project = project if project is not None else self._get_default_project(\ncredentials)\nself.project = project\nif set_as_global:\nself.__set_global()\ndef post(self, endpoint: str, data: Optional[Dict] = None, json: Optional[Dict] = None, files: Optional[Dict] = None, raise_for_status: bool = True) -&gt; Response:\n\"\"\"POST request to the backend.\n        Args:\n            endpoint (str): POST endpoint\n            data (Optional[dict]): (optional) multipart form data\n            json (Optional[dict]): (optional) json data\n            files (Optional[dict]): (optional) files to be sent\n            raise_for_status (bool): raise an exception on error\n        Returns:\n            Response object\n        \"\"\"\nurl_data = self.__build_url(endpoint, data=data, json=json, files=files)\nresponse = self._http_client.post(**url_data)\nif response.status_code != Client.codes.OK and raise_for_status:\nself.__raise_for_status(response)\nreturn response\ndef get(self, endpoint: str, params: Optional[Dict] = None, cookies: Optional[Dict] = None, raise_for_status: bool = True) -&gt; Response:\n\"\"\"GET request to the backend.\n        Args:\n            endpoint (str): GET endpoint\n            cookies (Optional[dict]): (optional) cookies data\n            raise_for_status (bool): raise an exception on error\n        Returns:\n            Response object\n        \"\"\"\nurl_data = self.__build_url(endpoint, params=params, cookies=cookies)\nresponse = self._http_client.get(**url_data)\nif response.status_code != Client.codes.OK and raise_for_status:\nself.__raise_for_status(response)\nreturn response\ndef get_static_file(self, endpoint: str, raise_for_status: bool = True) -&gt; Response:\n\"\"\"Retrieve a static file from the backend.\n        Args:\n            endpoint (str): GET endpoint\n            raise_for_status (bool): raise an exception on error\n        Returns:\n            Response object\n        \"\"\"\nurl_data = self.__build_url(endpoint)\nurl_data['url'] = f'{self._scheme}://{self._base_url}/static-content{endpoint}'\nresponse = self._http_client.get(**url_data)\nif response.status_code != Client.codes.OK and raise_for_status:\nself.__raise_for_status(response)\nreturn response\ndef _handshake(self):\n\"\"\"Client handshake.\n        It is used to determine is the client can connect with its\n        current authorization token.\n        \"\"\"\nresponse = self.get('/profiles', params={}, raise_for_status=False)\nif response.status_code == Client.codes.FOUND:\nparser = LinkExtractor()\nparser.feed(response.text)\nraise ClientHandshakeError(auth_link=parser.link)\ndef _get_default_project(self, token: str):\nresponse = self.get('/profiles/me', params={}, cookies={'access_token': token})\ndata: Dict = response.json()\nreturn data['myWorkspace']\ndef __build_url(self, endpoint: str, params: Optional[Dict] = None, data: Optional[Dict] = None, json: Optional[Dict] = None, files: Optional[Dict] = None, cookies: Optional[Dict] = None) -&gt; Dict:\n\"\"\"Build a request for the backend.\n        Args:\n            endpoint (str): backend endpoint\n            params (Optional[dict]): URL parameters\n            data (Optional[Project]): (optional) multipart form data\n            json (Optional[dict]): (optional) json data\n            files (Optional[dict]): (optional) files to be sent\n            cookies (Optional[dict]): (optional) cookies data\n        Returns:\n            dictionary containing the information to perform a request\n        \"\"\"\n_params = params if params is not None else {\n'ns': self._project\n}\nurl_data = {\n'url': f'{self._scheme}://{self._base_url}/api{endpoint}',\n'headers': self._headers,\n'params': _params,\n}\nif data is not None:\nurl_data['data'] = data\nif json is not None:\nurl_data['json'] = json\nif files is not None:\nurl_data['files'] = files\nif cookies is not None:\nurl_data['cookies'] = cookies\nreturn url_data\ndef __set_global(self) -&gt; None:\n\"\"\"Sets a client instance as global.\"\"\"\n# If the client is stateful, close it gracefully!\nClient.GLOBAL_CLIENT = self\ndef __raise_for_status(self, response: Response) -&gt; None:\n\"\"\"Raise an exception if the response is not OK.\n        When an exception is raised, we try to convert it to a ResponseError which is\n        a wrapper around a backend error. This usually gives enough context and provides\n        nice error message.\n        If it cannot be converted to ResponseError, it is re-raised.\n        Args:\n            response (Response): response to analyze\n        \"\"\"\ntry:\nresponse.raise_for_status()\nexcept HTTPStatusError as e:\nwith suppress(Exception):\ne = ResponseError(**response.json())\nraise e\n</code></pre>"},{"location":"sdk/reference/api/common/client/#ydata.sdk.common.client.client.Client.__build_url","title":"<code>__build_url(endpoint, params=None, data=None, json=None, files=None, cookies=None)</code>","text":"<p>Build a request for the backend.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>backend endpoint</p> required <code>params</code> <code>Optional[dict]</code> <p>URL parameters</p> <code>None</code> <code>data</code> <code>Optional[Project]</code> <p>(optional) multipart form data</p> <code>None</code> <code>json</code> <code>Optional[dict]</code> <p>(optional) json data</p> <code>None</code> <code>files</code> <code>Optional[dict]</code> <p>(optional) files to be sent</p> <code>None</code> <code>cookies</code> <code>Optional[dict]</code> <p>(optional) cookies data</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict</code> <p>dictionary containing the information to perform a request</p> Source code in <code>ydata/sdk/common/client/client.py</code> <pre><code>def __build_url(self, endpoint: str, params: Optional[Dict] = None, data: Optional[Dict] = None, json: Optional[Dict] = None, files: Optional[Dict] = None, cookies: Optional[Dict] = None) -&gt; Dict:\n\"\"\"Build a request for the backend.\n    Args:\n        endpoint (str): backend endpoint\n        params (Optional[dict]): URL parameters\n        data (Optional[Project]): (optional) multipart form data\n        json (Optional[dict]): (optional) json data\n        files (Optional[dict]): (optional) files to be sent\n        cookies (Optional[dict]): (optional) cookies data\n    Returns:\n        dictionary containing the information to perform a request\n    \"\"\"\n_params = params if params is not None else {\n'ns': self._project\n}\nurl_data = {\n'url': f'{self._scheme}://{self._base_url}/api{endpoint}',\n'headers': self._headers,\n'params': _params,\n}\nif data is not None:\nurl_data['data'] = data\nif json is not None:\nurl_data['json'] = json\nif files is not None:\nurl_data['files'] = files\nif cookies is not None:\nurl_data['cookies'] = cookies\nreturn url_data\n</code></pre>"},{"location":"sdk/reference/api/common/client/#ydata.sdk.common.client.client.Client.__raise_for_status","title":"<code>__raise_for_status(response)</code>","text":"<p>Raise an exception if the response is not OK.</p> <p>When an exception is raised, we try to convert it to a ResponseError which is a wrapper around a backend error. This usually gives enough context and provides nice error message.</p> <p>If it cannot be converted to ResponseError, it is re-raised.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>Response</code> <p>response to analyze</p> required Source code in <code>ydata/sdk/common/client/client.py</code> <pre><code>def __raise_for_status(self, response: Response) -&gt; None:\n\"\"\"Raise an exception if the response is not OK.\n    When an exception is raised, we try to convert it to a ResponseError which is\n    a wrapper around a backend error. This usually gives enough context and provides\n    nice error message.\n    If it cannot be converted to ResponseError, it is re-raised.\n    Args:\n        response (Response): response to analyze\n    \"\"\"\ntry:\nresponse.raise_for_status()\nexcept HTTPStatusError as e:\nwith suppress(Exception):\ne = ResponseError(**response.json())\nraise e\n</code></pre>"},{"location":"sdk/reference/api/common/client/#ydata.sdk.common.client.client.Client.__set_global","title":"<code>__set_global()</code>","text":"<p>Sets a client instance as global.</p> Source code in <code>ydata/sdk/common/client/client.py</code> <pre><code>def __set_global(self) -&gt; None:\n\"\"\"Sets a client instance as global.\"\"\"\n# If the client is stateful, close it gracefully!\nClient.GLOBAL_CLIENT = self\n</code></pre>"},{"location":"sdk/reference/api/common/client/#ydata.sdk.common.client.client.Client.get","title":"<code>get(endpoint, params=None, cookies=None, raise_for_status=True)</code>","text":"<p>GET request to the backend.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>GET endpoint</p> required <code>cookies</code> <code>Optional[dict]</code> <p>(optional) cookies data</p> <code>None</code> <code>raise_for_status</code> <code>bool</code> <p>raise an exception on error</p> <code>True</code> <p>Returns:</p> Type Description <code>Response</code> <p>Response object</p> Source code in <code>ydata/sdk/common/client/client.py</code> <pre><code>def get(self, endpoint: str, params: Optional[Dict] = None, cookies: Optional[Dict] = None, raise_for_status: bool = True) -&gt; Response:\n\"\"\"GET request to the backend.\n    Args:\n        endpoint (str): GET endpoint\n        cookies (Optional[dict]): (optional) cookies data\n        raise_for_status (bool): raise an exception on error\n    Returns:\n        Response object\n    \"\"\"\nurl_data = self.__build_url(endpoint, params=params, cookies=cookies)\nresponse = self._http_client.get(**url_data)\nif response.status_code != Client.codes.OK and raise_for_status:\nself.__raise_for_status(response)\nreturn response\n</code></pre>"},{"location":"sdk/reference/api/common/client/#ydata.sdk.common.client.client.Client.get_static_file","title":"<code>get_static_file(endpoint, raise_for_status=True)</code>","text":"<p>Retrieve a static file from the backend.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>GET endpoint</p> required <code>raise_for_status</code> <code>bool</code> <p>raise an exception on error</p> <code>True</code> <p>Returns:</p> Type Description <code>Response</code> <p>Response object</p> Source code in <code>ydata/sdk/common/client/client.py</code> <pre><code>def get_static_file(self, endpoint: str, raise_for_status: bool = True) -&gt; Response:\n\"\"\"Retrieve a static file from the backend.\n    Args:\n        endpoint (str): GET endpoint\n        raise_for_status (bool): raise an exception on error\n    Returns:\n        Response object\n    \"\"\"\nurl_data = self.__build_url(endpoint)\nurl_data['url'] = f'{self._scheme}://{self._base_url}/static-content{endpoint}'\nresponse = self._http_client.get(**url_data)\nif response.status_code != Client.codes.OK and raise_for_status:\nself.__raise_for_status(response)\nreturn response\n</code></pre>"},{"location":"sdk/reference/api/common/client/#ydata.sdk.common.client.client.Client.post","title":"<code>post(endpoint, data=None, json=None, files=None, raise_for_status=True)</code>","text":"<p>POST request to the backend.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>POST endpoint</p> required <code>data</code> <code>Optional[dict]</code> <p>(optional) multipart form data</p> <code>None</code> <code>json</code> <code>Optional[dict]</code> <p>(optional) json data</p> <code>None</code> <code>files</code> <code>Optional[dict]</code> <p>(optional) files to be sent</p> <code>None</code> <code>raise_for_status</code> <code>bool</code> <p>raise an exception on error</p> <code>True</code> <p>Returns:</p> Type Description <code>Response</code> <p>Response object</p> Source code in <code>ydata/sdk/common/client/client.py</code> <pre><code>def post(self, endpoint: str, data: Optional[Dict] = None, json: Optional[Dict] = None, files: Optional[Dict] = None, raise_for_status: bool = True) -&gt; Response:\n\"\"\"POST request to the backend.\n    Args:\n        endpoint (str): POST endpoint\n        data (Optional[dict]): (optional) multipart form data\n        json (Optional[dict]): (optional) json data\n        files (Optional[dict]): (optional) files to be sent\n        raise_for_status (bool): raise an exception on error\n    Returns:\n        Response object\n    \"\"\"\nurl_data = self.__build_url(endpoint, data=data, json=json, files=files)\nresponse = self._http_client.post(**url_data)\nif response.status_code != Client.codes.OK and raise_for_status:\nself.__raise_for_status(response)\nreturn response\n</code></pre>"},{"location":"sdk/reference/api/common/types/","title":"Types","text":""},{"location":"sdk/reference/api/connectors/connector/","title":"Connector","text":"<p>             Bases: <code>ModelFactoryMixin</code></p> <p>A <code>Connector</code> allows to connect and access data stored in various places. The list of available connectors can be found here.</p> <p>Parameters:</p> Name Type Description Default <code>connector_type</code> <code>Union[ConnectorType, str]</code> <p>Type of the connector to be created</p> <code>None</code> <code>credentials</code> <code>dict</code> <p>Connector credentials</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>(optional) Connector name</p> <code>None</code> <code>client</code> <code>Client</code> <p>(optional) Client to connect to the backend</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>uid</code> <code>UID</code> <p>UID fo the connector instance (creating internally)</p> <code>type</code> <code>ConnectorType</code> <p>Type of the connector</p> Source code in <code>ydata/sdk/connectors/connector.py</code> <pre><code>class Connector(ModelFactoryMixin):\n\"\"\"A [`Connector`][ydata.sdk.connectors.Connector] allows to connect and\n    access data stored in various places. The list of available connectors can\n    be found [here][ydata.sdk.connectors.ConnectorType].\n    Arguments:\n        connector_type (Union[ConnectorType, str]): Type of the connector to be created\n        credentials (dict): Connector credentials\n        name (Optional[str]): (optional) Connector name\n        client (Client): (optional) Client to connect to the backend\n    Attributes:\n        uid (UID): UID fo the connector instance (creating internally)\n        type (ConnectorType): Type of the connector\n    \"\"\"\ndef __init__(self, connector_type: Union[ConnectorType, str] = None, credentials: Optional[Dict] = None,  name: Optional[str] = None, client: Optional[Client] = None):\nself._init_common(client=client)\nself._model: Optional[mConnector] = self._create_model(\nconnector_type, credentials, name, client=client)\n@init_client\ndef _init_common(self, client: Optional[Client] = None):\nself._client = client\nself._logger = create_logger(__name__, level=LOG_LEVEL)\n@property\ndef uid(self) -&gt; UID:\nreturn self._model.uid\n@property\ndef type(self) -&gt; str:\nreturn self._model.type\n@staticmethod\n@init_client\ndef get(uid: UID, client: Optional[Client] = None) -&gt; \"Connector\":\n\"\"\"Get an existing connector.\n        Arguments:\n            uid (UID): Connector identifier\n            client (Client): (optional) Client to connect to the backend\n        Returns:\n            Connector\n        \"\"\"\nconnectors: ConnectorsList = Connector.list(client=client)\ndata = connectors.get_by_uid(uid)\nmodel = mConnector(**data)\nconnector = ModelFactoryMixin._init_from_model_data(Connector, model)\nreturn connector\n@staticmethod\ndef _init_connector_type(connector_type: Union[ConnectorType, str]) -&gt; ConnectorType:\nif isinstance(connector_type, str):\ntry:\nconnector_type = ConnectorType(connector_type)\nexcept Exception:\nc_list = \", \".join([c.value for c in ConnectorType])\nraise InvalidConnectorError(\nf\"ConnectorType '{connector_type}' does not exist.\\nValid connector types are: {c_list}.\")\nreturn connector_type\n@staticmethod\ndef _init_credentials(connector_type: ConnectorType, credentials: Union[str, Path, Dict, Credentials]) -&gt; Credentials:\n_credentials = None\nif isinstance(credentials, str):\ncredentials = Path(credentials)\nif isinstance(credentials, Path):\ntry:\n_credentials = json_loads(credentials.open().read())\nexcept Exception:\nraise CredentialTypeError(\n'Could not read the credentials. Please, check your path or credentials structure.')\ntry:\nfrom ydata.sdk.connectors._models.connector_map import TYPE_TO_CLASS\ncredential_cls = TYPE_TO_CLASS.get(connector_type.value)\n_credentials = credential_cls(**_credentials)\nexcept Exception:\nraise CredentialTypeError(\n\"Could not create the credentials. Verify the path or the structure your credentials.\")\nreturn _credentials\n@staticmethod\ndef create(connector_type: Union[ConnectorType, str], credentials: Union[str, Path, Dict, Credentials], name: Optional[str] = None, client: Optional[Client] = None) -&gt; \"Connector\":\n\"\"\"Create a new connector.\n        Arguments:\n            connector_type (Union[ConnectorType, str]): Type of the connector to be created\n            credentials (dict): Connector credentials\n            name (Optional[str]): (optional) Connector name\n            client (Client): (optional) Client to connect to the backend\n        Returns:\n            New connector\n        \"\"\"\nmodel = Connector._create_model(\nconnector_type=connector_type, credentials=credentials, name=name, client=client)\nconnector = ModelFactoryMixin._init_from_model_data(\nConnector, model)\nreturn connector\n@classmethod\n@init_client\ndef _create_model(cls, connector_type: Union[ConnectorType, str], credentials: Union[str, Path, Dict, Credentials], name: Optional[str] = None, client: Optional[Client] = None) -&gt; mConnector:\n_name = name if name is not None else str(uuid4())\n_connector_type = Connector._init_connector_type(connector_type)\n_credentials = Connector._init_credentials(_connector_type, credentials)\npayload = {\n\"type\": _connector_type.value,\n\"credentials\": _credentials.dict(by_alias=True),\n\"name\": _name\n}\nresponse = client.post('/connector/', json=payload)\ndata: list = response.json()\nreturn mConnector(**data)\n@staticmethod\n@init_client\ndef list(client: Optional[Client] = None) -&gt; ConnectorsList:\n\"\"\"List the connectors instances.\n        Arguments:\n            client (Client): (optional) Client to connect to the backend\n        Returns:\n            List of connectors\n        \"\"\"\nresponse = client.get('/connector')\ndata: list = response.json()\nreturn ConnectorsList(data)\ndef __repr__(self):\nreturn self._model.__repr__()\n</code></pre>"},{"location":"sdk/reference/api/connectors/connector/#ydata.sdk.connectors.connector.Connector.create","title":"<code>create(connector_type, credentials, name=None, client=None)</code>  <code>staticmethod</code>","text":"<p>Create a new connector.</p> <p>Parameters:</p> Name Type Description Default <code>connector_type</code> <code>Union[ConnectorType, str]</code> <p>Type of the connector to be created</p> required <code>credentials</code> <code>dict</code> <p>Connector credentials</p> required <code>name</code> <code>Optional[str]</code> <p>(optional) Connector name</p> <code>None</code> <code>client</code> <code>Client</code> <p>(optional) Client to connect to the backend</p> <code>None</code> <p>Returns:</p> Type Description <code>Connector</code> <p>New connector</p> Source code in <code>ydata/sdk/connectors/connector.py</code> <pre><code>@staticmethod\ndef create(connector_type: Union[ConnectorType, str], credentials: Union[str, Path, Dict, Credentials], name: Optional[str] = None, client: Optional[Client] = None) -&gt; \"Connector\":\n\"\"\"Create a new connector.\n    Arguments:\n        connector_type (Union[ConnectorType, str]): Type of the connector to be created\n        credentials (dict): Connector credentials\n        name (Optional[str]): (optional) Connector name\n        client (Client): (optional) Client to connect to the backend\n    Returns:\n        New connector\n    \"\"\"\nmodel = Connector._create_model(\nconnector_type=connector_type, credentials=credentials, name=name, client=client)\nconnector = ModelFactoryMixin._init_from_model_data(\nConnector, model)\nreturn connector\n</code></pre>"},{"location":"sdk/reference/api/connectors/connector/#ydata.sdk.connectors.connector.Connector.get","title":"<code>get(uid, client=None)</code>  <code>staticmethod</code>","text":"<p>Get an existing connector.</p> <p>Parameters:</p> Name Type Description Default <code>uid</code> <code>UID</code> <p>Connector identifier</p> required <code>client</code> <code>Client</code> <p>(optional) Client to connect to the backend</p> <code>None</code> <p>Returns:</p> Type Description <code>Connector</code> <p>Connector</p> Source code in <code>ydata/sdk/connectors/connector.py</code> <pre><code>@staticmethod\n@init_client\ndef get(uid: UID, client: Optional[Client] = None) -&gt; \"Connector\":\n\"\"\"Get an existing connector.\n    Arguments:\n        uid (UID): Connector identifier\n        client (Client): (optional) Client to connect to the backend\n    Returns:\n        Connector\n    \"\"\"\nconnectors: ConnectorsList = Connector.list(client=client)\ndata = connectors.get_by_uid(uid)\nmodel = mConnector(**data)\nconnector = ModelFactoryMixin._init_from_model_data(Connector, model)\nreturn connector\n</code></pre>"},{"location":"sdk/reference/api/connectors/connector/#ydata.sdk.connectors.connector.Connector.list","title":"<code>list(client=None)</code>  <code>staticmethod</code>","text":"<p>List the connectors instances.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>(optional) Client to connect to the backend</p> <code>None</code> <p>Returns:</p> Type Description <code>ConnectorsList</code> <p>List of connectors</p> Source code in <code>ydata/sdk/connectors/connector.py</code> <pre><code>@staticmethod\n@init_client\ndef list(client: Optional[Client] = None) -&gt; ConnectorsList:\n\"\"\"List the connectors instances.\n    Arguments:\n        client (Client): (optional) Client to connect to the backend\n    Returns:\n        List of connectors\n    \"\"\"\nresponse = client.get('/connector')\ndata: list = response.json()\nreturn ConnectorsList(data)\n</code></pre>"},{"location":"sdk/reference/api/connectors/connector/#connectortype","title":"ConnectorType","text":"<p>             Bases: <code>Enum</code></p>"},{"location":"sdk/reference/api/connectors/connector/#ydata.sdk.connectors.ConnectorType.AWS_S3","title":"<code>AWS_S3 = 'aws-s3'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>AWS S3 connector</p>"},{"location":"sdk/reference/api/connectors/connector/#ydata.sdk.connectors.ConnectorType.AZURE_BLOB","title":"<code>AZURE_BLOB = 'azure-blob'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Azure Blob connector</p>"},{"location":"sdk/reference/api/connectors/connector/#ydata.sdk.connectors.ConnectorType.AZURE_SQL","title":"<code>AZURE_SQL = 'azure-sql'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>AzureSQL connector</p>"},{"location":"sdk/reference/api/connectors/connector/#ydata.sdk.connectors.ConnectorType.BIGQUERY","title":"<code>BIGQUERY = 'google-bigquery'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>BigQuery connector</p>"},{"location":"sdk/reference/api/connectors/connector/#ydata.sdk.connectors.ConnectorType.FILE","title":"<code>FILE = 'file'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>File connector (placeholder)</p>"},{"location":"sdk/reference/api/connectors/connector/#ydata.sdk.connectors.ConnectorType.GCS","title":"<code>GCS = 'gcs'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Google Cloud Storage connector</p>"},{"location":"sdk/reference/api/connectors/connector/#ydata.sdk.connectors.ConnectorType.MYSQL","title":"<code>MYSQL = 'mysql'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>MySQL connector</p>"},{"location":"sdk/reference/api/connectors/connector/#ydata.sdk.connectors.ConnectorType.SNOWFLAKE","title":"<code>SNOWFLAKE = 'snowflake'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Snowflake connector</p>"},{"location":"sdk/reference/api/datasources/datasource/","title":"DataSource","text":"<p>             Bases: <code>ModelFactoryMixin</code></p> <p>A <code>DataSource</code> represents a dataset to be used by a Synthesizer as training data.</p> <p>Parameters:</p> Name Type Description Default <code>connector</code> <code>Connector</code> <p>Connector from which the datasource is created</p> required <code>datatype</code> <code>Optional[Union[DataSourceType, str]]</code> <p>(optional) DataSource type</p> <code>TABULAR</code> <code>name</code> <code>Optional[str]</code> <p>(optional) DataSource name</p> <code>None</code> <code>wait_for_metadata</code> <code>bool</code> <p>If <code>True</code>, wait until the metadata is fully calculated</p> <code>True</code> <code>client</code> <code>Client</code> <p>(optional) Client to connect to the backend</p> <code>None</code> <code>**config</code> <p>Datasource specific configuration</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>uid</code> <code>UID</code> <p>UID fo the datasource instance</p> <code>datatype</code> <code>DataSourceType</code> <p>Data source type</p> <code>status</code> <code>Status</code> <p>Status of the datasource</p> <code>metadata</code> <code>Metadata</code> <p>Metadata associated to the datasource</p> Source code in <code>ydata/sdk/datasources/datasource.py</code> <pre><code>class DataSource(ModelFactoryMixin):\n\"\"\"A [`DataSource`][ydata.sdk.datasources.DataSource] represents a dataset\n    to be used by a Synthesizer as training data.\n    Arguments:\n        connector (Connector): Connector from which the datasource is created\n        datatype (Optional[Union[DataSourceType, str]]): (optional) DataSource type\n        name (Optional[str]): (optional) DataSource name\n        wait_for_metadata (bool): If `True`, wait until the metadata is fully calculated\n        client (Client): (optional) Client to connect to the backend\n        **config: Datasource specific configuration\n    Attributes:\n        uid (UID): UID fo the datasource instance\n        datatype (DataSourceType): Data source type\n        status (Status): Status of the datasource\n        metadata (Metadata): Metadata associated to the datasource\n    \"\"\"\ndef __init__(self, connector: Connector, datatype: Optional[Union[DataSourceType, str]] = DataSourceType.TABULAR, name: Optional[str] = None, wait_for_metadata: bool = True, client: Optional[Client] = None, **config):\ndatasource_type = CONNECTOR_TO_DATASOURCE.get(connector.type)\nself._init_common(client=client)\nself._model: Optional[mDataSource] = self._create_model(\nconnector=connector, datasource_type=datasource_type, datatype=datatype, config=config, name=name, client=self._client)\nif wait_for_metadata:\nself._model = DataSource._wait_for_metadata(self)._model\n@init_client\ndef _init_common(self, client: Optional[Client] = None):\nself._client = client\nself._logger = create_logger(__name__, level=LOG_LEVEL)\n@property\ndef uid(self) -&gt; UID:\nreturn self._model.uid\n@property\ndef datatype(self) -&gt; DataSourceType:\nreturn self._model.datatype\n@property\ndef status(self) -&gt; Status:\ntry:\nself._model = self.get(self._model.uid, self._client)._model\nreturn self._model.status\nexcept Exception:  # noqa: PIE786\nreturn Status.UNKNOWN\n@property\ndef metadata(self) -&gt; Metadata:\nreturn self._model.metadata\n@staticmethod\n@init_client\ndef list(client: Optional[Client] = None) -&gt; DataSourceList:\n\"\"\"List the  [`DataSource`][ydata.sdk.datasources.DataSource]\n        instances.\n        Arguments:\n            client (Client): (optional) Client to connect to the backend\n        Returns:\n            List of datasources\n        \"\"\"\ndef __process_data(data: list) -&gt; list:\nto_del = ['metadata']\nfor e in data:\nfor k in to_del:\ne.pop(k, None)\nreturn data\nresponse = client.get('/datasource')\ndata: list = response.json()\ndata = __process_data(data)\nreturn DataSourceList(data)\n@staticmethod\n@init_client\ndef get(uid: UID, client: Optional[Client] = None) -&gt; \"DataSource\":\n\"\"\"Get an existing [`DataSource`][ydata.sdk.datasources.DataSource].\n        Arguments:\n            uid (UID): DataSource identifier\n            client (Client): (optional) Client to connect to the backend\n        Returns:\n            DataSource\n        \"\"\"\nresponse = client.get(f'/datasource/{uid}')\ndata: list = response.json()\ndatasource_type = CONNECTOR_TO_DATASOURCE.get(\nConnectorType(data['connector']['type']))\nmodel = DataSource._model_from_api(data, datasource_type)\ndatasource = ModelFactoryMixin._init_from_model_data(DataSource, model)\nreturn datasource\n@classmethod\ndef create(cls, connector: Connector, datatype: Optional[Union[DataSourceType, str]] = DataSourceType.TABULAR, name: Optional[str] = None, wait_for_metadata: bool = True, client: Optional[Client] = None, **config) -&gt; \"DataSource\":\n\"\"\"Create a new [`DataSource`][ydata.sdk.datasources.DataSource].\n        Arguments:\n            connector (Connector): Connector from which the datasource is created\n            datatype (Optional[Union[DataSourceType, str]]): (optional) DataSource type\n            name (Optional[str]): (optional) DataSource name\n            wait_for_metadata (bool): If `True`, wait until the metadata is fully calculated\n            client (Client): (optional) Client to connect to the backend\n            **config: Datasource specific configuration\n        Returns:\n            DataSource\n        \"\"\"\ndatasource_type = CONNECTOR_TO_DATASOURCE.get(connector.type)\nreturn cls._create(connector=connector, datasource_type=datasource_type, datatype=datatype, config=config, name=name, wait_for_metadata=wait_for_metadata, client=client)\n@classmethod\ndef _create(cls, connector: Connector, datasource_type: Type[mDataSource], datatype: Optional[Union[DataSourceType, str]] = DataSourceType.TABULAR, config: Optional[Dict] = None, name: Optional[str] = None, wait_for_metadata: bool = True, client: Optional[Client] = None) -&gt; \"DataSource\":\nmodel = DataSource._create_model(\nconnector, datasource_type, datatype, config, name, client)\ndatasource = ModelFactoryMixin._init_from_model_data(DataSource, model)\nif wait_for_metadata:\ndatasource._model = DataSource._wait_for_metadata(datasource)._model\nreturn datasource\n@classmethod\n@init_client\ndef _create_model(cls, connector: Connector, datasource_type: Type[mDataSource], datatype: Optional[Union[DataSourceType, str]] = DataSourceType.TABULAR, config: Optional[Dict] = None, name: Optional[str] = None, client: Optional[Client] = None) -&gt; mDataSource:\n_name = name if name is not None else str(uuid4())\n_config = config if config is not None else {}\npayload = {\n\"name\": _name,\n\"connector\": {\n\"uid\": connector.uid,\n\"type\": connector.type.value\n},\n\"dataType\": datatype.value\n}\nif connector.type != ConnectorType.FILE:\n_config = datasource_type(**config).to_payload()\npayload.update(_config)\nresponse = client.post('/datasource/', json=payload)\ndata: list = response.json()\nreturn DataSource._model_from_api(data, datasource_type)\n@staticmethod\ndef _wait_for_metadata(datasource):\nlogger = create_logger(__name__, level=LOG_LEVEL)\nwhile datasource.status not in [Status.AVAILABLE, Status.FAILED, Status.UNAVAILABLE]:\nlogger.info(f'Calculating metadata [{datasource.status}]')\ndatasource = DataSource.get(uid=datasource.uid, client=datasource._client)\nsleep(BACKOFF)\nreturn datasource\n@staticmethod\ndef _resolve_api_status(api_status: Dict) -&gt; Status:\nstatus = Status(api_status.get('state', Status.UNKNOWN.name))\nvalidation = ValidationState(api_status.get('validation', {}).get(\n'state', ValidationState.UNKNOWN.name))\nif validation == ValidationState.FAILED:\nstatus = Status.FAILED\nreturn status\n@staticmethod\ndef _model_from_api(data: Dict, datasource_type: Type[mDataSource]) -&gt; mDataSource:\ndata['datatype'] = data.pop('dataType')\ndata['state'] = data['status']\ndata['status'] = DataSource._resolve_api_status(data['status'])\ndata = filter_dict(datasource_type, data)\nmodel = datasource_type(**data)\nreturn model\ndef __repr__(self):\nreturn self._model.__repr__()\n</code></pre>"},{"location":"sdk/reference/api/datasources/datasource/#ydata.sdk.datasources.datasource.DataSource.create","title":"<code>create(connector, datatype=DataSourceType.TABULAR, name=None, wait_for_metadata=True, client=None, **config)</code>  <code>classmethod</code>","text":"<p>Create a new <code>DataSource</code>.</p> <p>Parameters:</p> Name Type Description Default <code>connector</code> <code>Connector</code> <p>Connector from which the datasource is created</p> required <code>datatype</code> <code>Optional[Union[DataSourceType, str]]</code> <p>(optional) DataSource type</p> <code>TABULAR</code> <code>name</code> <code>Optional[str]</code> <p>(optional) DataSource name</p> <code>None</code> <code>wait_for_metadata</code> <code>bool</code> <p>If <code>True</code>, wait until the metadata is fully calculated</p> <code>True</code> <code>client</code> <code>Client</code> <p>(optional) Client to connect to the backend</p> <code>None</code> <code>**config</code> <p>Datasource specific configuration</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataSource</code> <p>DataSource</p> Source code in <code>ydata/sdk/datasources/datasource.py</code> <pre><code>@classmethod\ndef create(cls, connector: Connector, datatype: Optional[Union[DataSourceType, str]] = DataSourceType.TABULAR, name: Optional[str] = None, wait_for_metadata: bool = True, client: Optional[Client] = None, **config) -&gt; \"DataSource\":\n\"\"\"Create a new [`DataSource`][ydata.sdk.datasources.DataSource].\n    Arguments:\n        connector (Connector): Connector from which the datasource is created\n        datatype (Optional[Union[DataSourceType, str]]): (optional) DataSource type\n        name (Optional[str]): (optional) DataSource name\n        wait_for_metadata (bool): If `True`, wait until the metadata is fully calculated\n        client (Client): (optional) Client to connect to the backend\n        **config: Datasource specific configuration\n    Returns:\n        DataSource\n    \"\"\"\ndatasource_type = CONNECTOR_TO_DATASOURCE.get(connector.type)\nreturn cls._create(connector=connector, datasource_type=datasource_type, datatype=datatype, config=config, name=name, wait_for_metadata=wait_for_metadata, client=client)\n</code></pre>"},{"location":"sdk/reference/api/datasources/datasource/#ydata.sdk.datasources.datasource.DataSource.get","title":"<code>get(uid, client=None)</code>  <code>staticmethod</code>","text":"<p>Get an existing <code>DataSource</code>.</p> <p>Parameters:</p> Name Type Description Default <code>uid</code> <code>UID</code> <p>DataSource identifier</p> required <code>client</code> <code>Client</code> <p>(optional) Client to connect to the backend</p> <code>None</code> <p>Returns:</p> Type Description <code>DataSource</code> <p>DataSource</p> Source code in <code>ydata/sdk/datasources/datasource.py</code> <pre><code>@staticmethod\n@init_client\ndef get(uid: UID, client: Optional[Client] = None) -&gt; \"DataSource\":\n\"\"\"Get an existing [`DataSource`][ydata.sdk.datasources.DataSource].\n    Arguments:\n        uid (UID): DataSource identifier\n        client (Client): (optional) Client to connect to the backend\n    Returns:\n        DataSource\n    \"\"\"\nresponse = client.get(f'/datasource/{uid}')\ndata: list = response.json()\ndatasource_type = CONNECTOR_TO_DATASOURCE.get(\nConnectorType(data['connector']['type']))\nmodel = DataSource._model_from_api(data, datasource_type)\ndatasource = ModelFactoryMixin._init_from_model_data(DataSource, model)\nreturn datasource\n</code></pre>"},{"location":"sdk/reference/api/datasources/datasource/#ydata.sdk.datasources.datasource.DataSource.list","title":"<code>list(client=None)</code>  <code>staticmethod</code>","text":"<p>List the  <code>DataSource</code> instances.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>(optional) Client to connect to the backend</p> <code>None</code> <p>Returns:</p> Type Description <code>DataSourceList</code> <p>List of datasources</p> Source code in <code>ydata/sdk/datasources/datasource.py</code> <pre><code>@staticmethod\n@init_client\ndef list(client: Optional[Client] = None) -&gt; DataSourceList:\n\"\"\"List the  [`DataSource`][ydata.sdk.datasources.DataSource]\n    instances.\n    Arguments:\n        client (Client): (optional) Client to connect to the backend\n    Returns:\n        List of datasources\n    \"\"\"\ndef __process_data(data: list) -&gt; list:\nto_del = ['metadata']\nfor e in data:\nfor k in to_del:\ne.pop(k, None)\nreturn data\nresponse = client.get('/datasource')\ndata: list = response.json()\ndata = __process_data(data)\nreturn DataSourceList(data)\n</code></pre>"},{"location":"sdk/reference/api/datasources/datasource/#status","title":"Status","text":"<p>             Bases: <code>StringEnum</code></p> <p>Represent the status of a <code>DataSource</code>.</p>"},{"location":"sdk/reference/api/datasources/datasource/#ydata.sdk.datasources.Status.AVAILABLE","title":"<code>AVAILABLE = 'available'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The <code>DataSource</code> is available and ready to be used.</p>"},{"location":"sdk/reference/api/datasources/datasource/#ydata.sdk.datasources.Status.DELETED","title":"<code>DELETED = 'deleted'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The <code>DataSource</code> is to be deleted or has been deleted.</p>"},{"location":"sdk/reference/api/datasources/datasource/#ydata.sdk.datasources.Status.FAILED","title":"<code>FAILED = 'failed'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The <code>DataSource</code> preparation or validation has failed.</p>"},{"location":"sdk/reference/api/datasources/datasource/#ydata.sdk.datasources.Status.PREPARING","title":"<code>PREPARING = 'preparing'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The <code>DataSource</code> is being prepared.</p>"},{"location":"sdk/reference/api/datasources/datasource/#ydata.sdk.datasources.Status.UNAVAILABLE","title":"<code>UNAVAILABLE = 'unavailable'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The <code>DataSource</code> is unavailable at the moment.</p>"},{"location":"sdk/reference/api/datasources/datasource/#ydata.sdk.datasources.Status.UNKNOWN","title":"<code>UNKNOWN = 'unknown'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The <code>DataSource</code> status could not be retrieved.</p>"},{"location":"sdk/reference/api/datasources/datasource/#ydata.sdk.datasources.Status.VALIDATING","title":"<code>VALIDATING = 'validating'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The <code>DataSource</code> is being validated.</p>"},{"location":"sdk/reference/api/datasources/datasource/#datasourcetype","title":"DataSourceType","text":"<p>             Bases: <code>StringEnum</code></p>"},{"location":"sdk/reference/api/datasources/datasource/#ydata.sdk.datasources.DataSourceType.TABULAR","title":"<code>TABULAR = 'tabular'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The <code>DataSource</code> is tabular (i.e. it does not have a temporal dimension).</p>"},{"location":"sdk/reference/api/datasources/datasource/#ydata.sdk.datasources.DataSourceType.TIMESERIES","title":"<code>TIMESERIES = 'timeseries'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The <code>DataSource</code> has a temporal dimension.</p>"},{"location":"sdk/reference/api/datasources/metadata/","title":"Metadata","text":"<p>             Bases: <code>BaseModel</code></p> <p>The Metadata object contains descriptive information about a.</p> <p><code>DataSource</code></p> <p>Attributes:</p> Name Type Description <code>columns</code> <code>List[Column]</code> <p>columns information</p>"},{"location":"sdk/reference/api/synthesizers/base/","title":"Synthesizer","text":"<p>             Bases: <code>ABC</code>, <code>ModelFactoryMixin</code></p> <p>Main synthesizer class.</p> <p>This class cannot be directly instanciated because of the specificities between <code>RegularSynthesizer</code> and <code>TimeSeriesSynthesizer</code> <code>sample</code> methods.</p>"},{"location":"sdk/reference/api/synthesizers/base/#ydata.sdk.synthesizers.synthesizer.BaseSynthesizer--methods","title":"Methods","text":"<ul> <li><code>fit</code>: train a synthesizer instance.</li> <li><code>sample</code>: request synthetic data.</li> <li><code>status</code>: current status of the synthesizer instance.</li> </ul> Note <p>The synthesizer instance is created in the backend only when the <code>fit</code> method is called.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>(optional) Client to connect to the backend</p> <code>None</code> Source code in <code>ydata/sdk/synthesizers/synthesizer.py</code> <pre><code>@typechecked\nclass BaseSynthesizer(ABC, ModelFactoryMixin):\n\"\"\"Main synthesizer class.\n    This class cannot be directly instanciated because of the specificities between [`RegularSynthesizer`][ydata.sdk.synthesizers.RegularSynthesizer] and [`TimeSeriesSynthesizer`][ydata.sdk.synthesizers.TimeSeriesSynthesizer] `sample` methods.\n    Methods\n    -------\n    - `fit`: train a synthesizer instance.\n    - `sample`: request synthetic data.\n    - `status`: current status of the synthesizer instance.\n    Note:\n            The synthesizer instance is created in the backend only when the `fit` method is called.\n    Arguments:\n        client (Client): (optional) Client to connect to the backend\n    \"\"\"\ndef __init__(self, client: Optional[Client] = None):\nself._init_common(client=client)\nself._model: Optional[mSynthesizer] = None\n@init_client\ndef _init_common(self, client: Optional[Client] = None):\nself._client = client\nself._logger = create_logger(__name__, level=LOG_LEVEL)\ndef fit(self, X: Union[DataSource, pdDataFrame],\nprivacy_level: PrivacyLevel = PrivacyLevel.HIGH_FIDELITY,\ndatatype: Optional[Union[DataSourceType, str]] = None,\nsortbykey: Optional[Union[str, List[str]]] = None,\nentities: Optional[Union[str, List[str]]] = None,\ngenerate_cols: Optional[List[str]] = None,\nexclude_cols: Optional[List[str]] = None,\ndtypes: Optional[Dict[str, Union[str, DataType]]] = None,\ntarget: Optional[str] = None,\nname: Optional[str] = None,\nanonymize: Optional[dict] = None,\ncondition_on: Optional[List[str]] = None) -&gt; None:\n\"\"\"Fit the synthesizer.\n        The synthesizer accepts as training dataset either a pandas [`DataFrame`][pandas.DataFrame] directly or a YData [`DataSource`][ydata.sdk.datasources.DataSource].\n        When the training dataset is a pandas [`DataFrame`][pandas.DataFrame], the argument `datatype` is required as it cannot be deduced.\n        The argument`sortbykey` is mandatory for [`TimeSeries`][ydata.sdk.datasources.DataSourceType.TIMESERIES].\n        By default, if `generate_cols` or `exclude_cols` are not specified, all columns are generated by the synthesizer.\n        The argument `exclude_cols` has precedence over `generate_cols`, i.e. a column `col` will not be generated if it is in both list.\n        Arguments:\n            X (Union[DataSource, pandas.DataFrame]): Training dataset\n            privacy_level (PrivacyLevel): Synthesizer privacy level (defaults to high fidelity)\n            datatype (Optional[Union[DataSourceType, str]]): (optional) Dataset datatype - required if `X` is a [`pandas.DataFrame`][pandas.DataFrame]\n            sortbykey (Union[str, List[str]]): (optional) column(s) to use to sort timeseries datasets\n            entities (Union[str, List[str]]): (optional) columns representing entities ID\n            generate_cols (List[str]): (optional) columns that should be synthesized\n            exclude_cols (List[str]): (optional) columns that should not be synthesized\n            dtypes (Dict[str, Union[str, DataType]]): (optional) datatype mapping that will overwrite the datasource metadata column datatypes\n            target (Optional[str]): (optional) Target for the dataset\n            name (Optional[str]): (optional) Synthesizer instance name\n            anonymize (Optional[str]): (optional) fields to anonymize and the anonymization strategy\n            condition_on: (Optional[List[str]]): (optional) list of features to condition upon\n        \"\"\"\nif self._is_initialized():\nraise AlreadyFittedError()\n_datatype = DataSourceType(datatype) if isinstance(\nX, pdDataFrame) else DataSourceType(X.datatype)\ndataset_attrs = self._init_datasource_attributes(\nsortbykey, entities, generate_cols, exclude_cols, dtypes)\nself._validate_datasource_attributes(X, dataset_attrs, _datatype, target)\n# If the training data is a pandas dataframe, we first need to create a data source and then the instance\nif isinstance(X, pdDataFrame):\nif X.empty:\nraise EmptyDataError(\"The DataFrame is empty\")\n_X = LocalDataSource(source=X, datatype=_datatype, client=self._client)\nelse:\nif datatype != _datatype:\nwarn(\"When the training data is a DataSource, the argument `datatype` is ignored.\",\nDataSourceTypeWarning)\n_X = X\nif _X.status != dsStatus.AVAILABLE:\nraise DataSourceNotAvailableError(\nf\"The datasource '{_X.uid}' is not available (status = {_X.status.value})\")\nif isinstance(dataset_attrs, dict):\ndataset_attrs = DataSourceAttrs(**dataset_attrs)\nself._fit_from_datasource(\nX=_X, dataset_attrs=dataset_attrs, target=target, name=name,\nanonymize=anonymize, privacy_level=privacy_level, condition_on=condition_on)\n@staticmethod\ndef _init_datasource_attributes(\nsortbykey: Optional[Union[str, List[str]]],\nentities: Optional[Union[str, List[str]]],\ngenerate_cols: Optional[List[str]],\nexclude_cols: Optional[List[str]],\ndtypes: Optional[Dict[str, Union[str, DataType]]]) -&gt; DataSourceAttrs:\ndataset_attrs = {\n'sortbykey': sortbykey if sortbykey is not None else [],\n'entities': entities if entities is not None else [],\n'generate_cols': generate_cols if generate_cols is not None else [],\n'exclude_cols': exclude_cols if exclude_cols is not None else [],\n'dtypes': {k: DataType(v) for k, v in dtypes.items()} if dtypes is not None else {}\n}\nreturn DataSourceAttrs(**dataset_attrs)\n@staticmethod\ndef _validate_datasource_attributes(X: Union[DataSource, pdDataFrame], dataset_attrs: DataSourceAttrs, datatype: DataSourceType, target: Optional[str]):\ncolumns = []\nif isinstance(X, pdDataFrame):\ncolumns = X.columns\nif datatype is None:\nraise DataTypeMissingError(\n\"Argument `datatype` is mandatory for pandas.DataFrame training data\")\ndatatype = DataSourceType(datatype)\nelse:\ncolumns = [c.name for c in X.metadata.columns]\nif target is not None and target not in columns:\nraise DataSourceAttrsError(\n\"Invalid target: column '{target}' does not exist\")\nif datatype == DataSourceType.TIMESERIES:\nif not dataset_attrs.sortbykey:\nraise DataSourceAttrsError(\n\"The argument `sortbykey` is mandatory for timeseries datasource.\")\ninvalid_fields = {}\nfor field, v in dataset_attrs.dict().items():\nfield_columns = v if field != 'dtypes' else v.keys()\nnot_in_cols = [c for c in field_columns if c not in columns]\nif len(not_in_cols) &gt; 0:\ninvalid_fields[field] = not_in_cols\nif len(invalid_fields) &gt; 0:\nerror_msgs = [\"\\t- Field '{}': columns {} do not exist\".format(\nf, ', '.join(v)) for f, v in invalid_fields.items()]\nraise DataSourceAttrsError(\n\"The dataset attributes are invalid:\\n {}\".format('\\n'.join(error_msgs)))\n@staticmethod\ndef _metadata_to_payload(\ndatatype: DataSourceType, ds_metadata: Metadata,\ndataset_attrs: Optional[DataSourceAttrs] = None, target: str | None = None\n) -&gt; dict:\n\"\"\"Transform a the metadata and dataset attributes into a valid\n        payload.\n        Arguments:\n            datatype (DataSourceType): datasource type\n            ds_metadata (Metadata): datasource metadata object\n            dataset_attrs ( Optional[DataSourceAttrs] ): (optional) Dataset attributes\n            target (Optional[str]): (optional) target column name\n        Returns:\n            metadata payload dictionary\n        \"\"\"\ncolumns = [\n{\n'name': c.name,\n'generation': True and c.name not in dataset_attrs.exclude_cols,\n'dataType': DataType(dataset_attrs.dtypes[c.name]).value if c.name in dataset_attrs.dtypes else c.datatype,\n'varType': c.vartype,\n}\nfor c in ds_metadata.columns]\nmetadata = {\n'columns': columns,\n'target': target\n}\nif dataset_attrs is not None:\nif datatype == DataSourceType.TIMESERIES:\nmetadata['sortBy'] = [c for c in dataset_attrs.sortbykey]\nmetadata['entity'] = [c for c in dataset_attrs.entities]\nreturn metadata\ndef _fit_from_datasource(\nself,\nX: DataSource,\nprivacy_level: PrivacyLevel = PrivacyLevel.HIGH_FIDELITY,\ndataset_attrs: Optional[DataSourceAttrs] = None,\ntarget: Optional[str] = None,\nname: Optional[str] = None,\nanonymize: Optional[dict] = None,\ncondition_on: Optional[List[str]] = None\n) -&gt; None:\n_name = name if name is not None else str(uuid4())\nmetadata = self._metadata_to_payload(\nDataSourceType(X.datatype), X.metadata, dataset_attrs, target)\npayload = {\n'name': _name,\n'dataSourceUID': X.uid,\n'metadata': metadata,\n'extraData': {},\n'privacyLevel': privacy_level.value\n}\nif anonymize is not None:\npayload[\"extraData\"][\"anonymize\"] = anonymize\nif condition_on is not None:\npayload[\"extraData\"][\"condition_on\"] = condition_on\nresponse = self._client.post('/synthesizer/', json=payload)\ndata: list = response.json()\nself._model, _ = self._model_from_api(X.datatype, data)\nwhile self.status not in [Status.READY, Status.FAILED]:\nself._logger.info('Training the synthesizer...')\nsleep(BACKOFF)\nif self.status == Status.FAILED:\nraise FittingError('Could not train the synthesizer')\n@staticmethod\ndef _model_from_api(datatype: str, data: Dict) -&gt; Tuple[mSynthesizer, Type[\"BaseSynthesizer\"]]:\nfrom ydata.sdk.synthesizers._models.synthesizer_map import TYPE_TO_CLASS\nsynth_cls = TYPE_TO_CLASS.get(SynthesizerType(datatype).value)\ndata['status'] = synth_cls._resolve_api_status(data['status'])\ndata = filter_dict(mSynthesizer, data)\nreturn mSynthesizer(**data), synth_cls\n@abstractmethod\ndef sample(self) -&gt; pdDataFrame:\n\"\"\"Abstract method to sample from a synthesizer.\"\"\"\ndef _sample(self, payload: Dict) -&gt; pdDataFrame:\n\"\"\"Sample from a synthesizer.\n        Arguments:\n            payload (dict): payload configuring the sample request\n        Returns:\n            pandas `DataFrame`\n        \"\"\"\nresponse = self._client.post(\nf\"/synthesizer/{self.uid}/sample\", json=payload)\ndata: Dict = response.json()\nsample_uid = data.get('uid')\nsample_status = None\nwhile sample_status not in ['finished', 'failed']:\nself._logger.info('Sampling from the synthesizer...')\nresponse = self._client.get(f'/synthesizer/{self.uid}/history')\nhistory: Dict = response.json()\nsample_data = next((s for s in history if s.get('uid') == sample_uid), None)\nsample_status = sample_data.get('status', {}).get('state')\nsleep(BACKOFF)\nresponse = self._client.get_static_file(\nf'/synthesizer/{self.uid}/sample/{sample_uid}/sample.csv')\ndata = StringIO(response.content.decode())\nreturn read_csv(data)\n@property\ndef uid(self) -&gt; UID:\n\"\"\"Get the status of a synthesizer instance.\n        Returns:\n            Synthesizer status\n        \"\"\"\nif not self._is_initialized():\nreturn Status.NOT_INITIALIZED\nreturn self._model.uid\n@property\ndef status(self) -&gt; Status:\n\"\"\"Get the status of a synthesizer instance.\n        Returns:\n            Synthesizer status\n        \"\"\"\nif not self._is_initialized():\nreturn Status.NOT_INITIALIZED\ntry:\nself = self.get(self._model.uid, self._client)\nreturn self._model.status\nexcept Exception:  # noqa: PIE786\nreturn Status.UNKNOWN\n@staticmethod\n@init_client\ndef get(uid: str, client: Optional[Client] = None) -&gt; \"BaseSynthesizer\":\n\"\"\"List the synthesizer instances.\n        Arguments:\n            uid (str): synthesizer instance uid\n            client (Client): (optional) Client to connect to the backend\n        Returns:\n            Synthesizer instance\n        \"\"\"\nresponse = client.get(f'/synthesizer/{uid}')\ndata: list = response.json()\nmodel, synth_cls = BaseSynthesizer._model_from_api(\ndata['dataSource']['dataType'], data)\nreturn ModelFactoryMixin._init_from_model_data(synth_cls, model)\n@staticmethod\n@init_client\ndef list(client: Optional[Client] = None) -&gt; SynthesizersList:\n\"\"\"List the synthesizer instances.\n        Arguments:\n            client (Client): (optional) Client to connect to the backend\n        Returns:\n            List of synthesizers\n        \"\"\"\ndef __process_data(data: list) -&gt; list:\nto_del = ['metadata', 'report', 'mode']\nfor e in data:\nfor k in to_del:\ne.pop(k, None)\nreturn data\nresponse = client.get('/synthesizer')\ndata: list = response.json()\ndata = __process_data(data)\nreturn SynthesizersList(data)\ndef _is_initialized(self) -&gt; bool:\n\"\"\"Determine if a synthesizer is instanciated or not.\n        Returns:\n            True if the synthesizer is instanciated\n        \"\"\"\nreturn self._model is not None\n@staticmethod\ndef _resolve_api_status(api_status: Dict) -&gt; Status:\n\"\"\"Determine the status of the Synthesizer.\n        The status of the synthesizer instance is determined by the state of\n        its different components.\n        Arguments:\n            api_status (dict): json from the endpoint GET /synthesizer\n        Returns:\n            Synthesizer Status\n        \"\"\"\nstatus = Status(api_status.get('state', Status.UNKNOWN.name))\nif status == Status.PREPARE:\nif PrepareState(api_status.get('prepare', {}).get(\n'state', PrepareState.UNKNOWN.name)) == PrepareState.FAILED:\nreturn Status.FAILED\nelif status == Status.TRAIN:\nif TrainingState(api_status.get('training', {}).get(\n'state', TrainingState.UNKNOWN.name)) == TrainingState.FAILED:\nreturn Status.FAILED\nelif status == Status.REPORT:\nreturn Status.READY\nreturn status\n</code></pre>"},{"location":"sdk/reference/api/synthesizers/base/#ydata.sdk.synthesizers.synthesizer.BaseSynthesizer.status","title":"<code>status: Status</code>  <code>property</code>","text":"<p>Get the status of a synthesizer instance.</p> <p>Returns:</p> Type Description <code>Status</code> <p>Synthesizer status</p>"},{"location":"sdk/reference/api/synthesizers/base/#ydata.sdk.synthesizers.synthesizer.BaseSynthesizer.uid","title":"<code>uid: UID</code>  <code>property</code>","text":"<p>Get the status of a synthesizer instance.</p> <p>Returns:</p> Type Description <code>UID</code> <p>Synthesizer status</p>"},{"location":"sdk/reference/api/synthesizers/base/#ydata.sdk.synthesizers.synthesizer.BaseSynthesizer.fit","title":"<code>fit(X, privacy_level=PrivacyLevel.HIGH_FIDELITY, datatype=None, sortbykey=None, entities=None, generate_cols=None, exclude_cols=None, dtypes=None, target=None, name=None, anonymize=None, condition_on=None)</code>","text":"<p>Fit the synthesizer.</p> <p>The synthesizer accepts as training dataset either a pandas <code>DataFrame</code> directly or a YData <code>DataSource</code>. When the training dataset is a pandas <code>DataFrame</code>, the argument <code>datatype</code> is required as it cannot be deduced.</p> <p>The argument<code>sortbykey</code> is mandatory for <code>TimeSeries</code>.</p> <p>By default, if <code>generate_cols</code> or <code>exclude_cols</code> are not specified, all columns are generated by the synthesizer. The argument <code>exclude_cols</code> has precedence over <code>generate_cols</code>, i.e. a column <code>col</code> will not be generated if it is in both list.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[DataSource, DataFrame]</code> <p>Training dataset</p> required <code>privacy_level</code> <code>PrivacyLevel</code> <p>Synthesizer privacy level (defaults to high fidelity)</p> <code>HIGH_FIDELITY</code> <code>datatype</code> <code>Optional[Union[DataSourceType, str]]</code> <p>(optional) Dataset datatype - required if <code>X</code> is a <code>pandas.DataFrame</code></p> <code>None</code> <code>sortbykey</code> <code>Union[str, List[str]]</code> <p>(optional) column(s) to use to sort timeseries datasets</p> <code>None</code> <code>entities</code> <code>Union[str, List[str]]</code> <p>(optional) columns representing entities ID</p> <code>None</code> <code>generate_cols</code> <code>List[str]</code> <p>(optional) columns that should be synthesized</p> <code>None</code> <code>exclude_cols</code> <code>List[str]</code> <p>(optional) columns that should not be synthesized</p> <code>None</code> <code>dtypes</code> <code>Dict[str, Union[str, DataType]]</code> <p>(optional) datatype mapping that will overwrite the datasource metadata column datatypes</p> <code>None</code> <code>target</code> <code>Optional[str]</code> <p>(optional) Target for the dataset</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>(optional) Synthesizer instance name</p> <code>None</code> <code>anonymize</code> <code>Optional[str]</code> <p>(optional) fields to anonymize and the anonymization strategy</p> <code>None</code> <code>condition_on</code> <code>Optional[List[str]]</code> <p>(Optional[List[str]]): (optional) list of features to condition upon</p> <code>None</code> Source code in <code>ydata/sdk/synthesizers/synthesizer.py</code> <pre><code>def fit(self, X: Union[DataSource, pdDataFrame],\nprivacy_level: PrivacyLevel = PrivacyLevel.HIGH_FIDELITY,\ndatatype: Optional[Union[DataSourceType, str]] = None,\nsortbykey: Optional[Union[str, List[str]]] = None,\nentities: Optional[Union[str, List[str]]] = None,\ngenerate_cols: Optional[List[str]] = None,\nexclude_cols: Optional[List[str]] = None,\ndtypes: Optional[Dict[str, Union[str, DataType]]] = None,\ntarget: Optional[str] = None,\nname: Optional[str] = None,\nanonymize: Optional[dict] = None,\ncondition_on: Optional[List[str]] = None) -&gt; None:\n\"\"\"Fit the synthesizer.\n    The synthesizer accepts as training dataset either a pandas [`DataFrame`][pandas.DataFrame] directly or a YData [`DataSource`][ydata.sdk.datasources.DataSource].\n    When the training dataset is a pandas [`DataFrame`][pandas.DataFrame], the argument `datatype` is required as it cannot be deduced.\n    The argument`sortbykey` is mandatory for [`TimeSeries`][ydata.sdk.datasources.DataSourceType.TIMESERIES].\n    By default, if `generate_cols` or `exclude_cols` are not specified, all columns are generated by the synthesizer.\n    The argument `exclude_cols` has precedence over `generate_cols`, i.e. a column `col` will not be generated if it is in both list.\n    Arguments:\n        X (Union[DataSource, pandas.DataFrame]): Training dataset\n        privacy_level (PrivacyLevel): Synthesizer privacy level (defaults to high fidelity)\n        datatype (Optional[Union[DataSourceType, str]]): (optional) Dataset datatype - required if `X` is a [`pandas.DataFrame`][pandas.DataFrame]\n        sortbykey (Union[str, List[str]]): (optional) column(s) to use to sort timeseries datasets\n        entities (Union[str, List[str]]): (optional) columns representing entities ID\n        generate_cols (List[str]): (optional) columns that should be synthesized\n        exclude_cols (List[str]): (optional) columns that should not be synthesized\n        dtypes (Dict[str, Union[str, DataType]]): (optional) datatype mapping that will overwrite the datasource metadata column datatypes\n        target (Optional[str]): (optional) Target for the dataset\n        name (Optional[str]): (optional) Synthesizer instance name\n        anonymize (Optional[str]): (optional) fields to anonymize and the anonymization strategy\n        condition_on: (Optional[List[str]]): (optional) list of features to condition upon\n    \"\"\"\nif self._is_initialized():\nraise AlreadyFittedError()\n_datatype = DataSourceType(datatype) if isinstance(\nX, pdDataFrame) else DataSourceType(X.datatype)\ndataset_attrs = self._init_datasource_attributes(\nsortbykey, entities, generate_cols, exclude_cols, dtypes)\nself._validate_datasource_attributes(X, dataset_attrs, _datatype, target)\n# If the training data is a pandas dataframe, we first need to create a data source and then the instance\nif isinstance(X, pdDataFrame):\nif X.empty:\nraise EmptyDataError(\"The DataFrame is empty\")\n_X = LocalDataSource(source=X, datatype=_datatype, client=self._client)\nelse:\nif datatype != _datatype:\nwarn(\"When the training data is a DataSource, the argument `datatype` is ignored.\",\nDataSourceTypeWarning)\n_X = X\nif _X.status != dsStatus.AVAILABLE:\nraise DataSourceNotAvailableError(\nf\"The datasource '{_X.uid}' is not available (status = {_X.status.value})\")\nif isinstance(dataset_attrs, dict):\ndataset_attrs = DataSourceAttrs(**dataset_attrs)\nself._fit_from_datasource(\nX=_X, dataset_attrs=dataset_attrs, target=target, name=name,\nanonymize=anonymize, privacy_level=privacy_level, condition_on=condition_on)\n</code></pre>"},{"location":"sdk/reference/api/synthesizers/base/#ydata.sdk.synthesizers.synthesizer.BaseSynthesizer.get","title":"<code>get(uid, client=None)</code>  <code>staticmethod</code>","text":"<p>List the synthesizer instances.</p> <p>Parameters:</p> Name Type Description Default <code>uid</code> <code>str</code> <p>synthesizer instance uid</p> required <code>client</code> <code>Client</code> <p>(optional) Client to connect to the backend</p> <code>None</code> <p>Returns:</p> Type Description <code>BaseSynthesizer</code> <p>Synthesizer instance</p> Source code in <code>ydata/sdk/synthesizers/synthesizer.py</code> <pre><code>@staticmethod\n@init_client\ndef get(uid: str, client: Optional[Client] = None) -&gt; \"BaseSynthesizer\":\n\"\"\"List the synthesizer instances.\n    Arguments:\n        uid (str): synthesizer instance uid\n        client (Client): (optional) Client to connect to the backend\n    Returns:\n        Synthesizer instance\n    \"\"\"\nresponse = client.get(f'/synthesizer/{uid}')\ndata: list = response.json()\nmodel, synth_cls = BaseSynthesizer._model_from_api(\ndata['dataSource']['dataType'], data)\nreturn ModelFactoryMixin._init_from_model_data(synth_cls, model)\n</code></pre>"},{"location":"sdk/reference/api/synthesizers/base/#ydata.sdk.synthesizers.synthesizer.BaseSynthesizer.list","title":"<code>list(client=None)</code>  <code>staticmethod</code>","text":"<p>List the synthesizer instances.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>(optional) Client to connect to the backend</p> <code>None</code> <p>Returns:</p> Type Description <code>SynthesizersList</code> <p>List of synthesizers</p> Source code in <code>ydata/sdk/synthesizers/synthesizer.py</code> <pre><code>@staticmethod\n@init_client\ndef list(client: Optional[Client] = None) -&gt; SynthesizersList:\n\"\"\"List the synthesizer instances.\n    Arguments:\n        client (Client): (optional) Client to connect to the backend\n    Returns:\n        List of synthesizers\n    \"\"\"\ndef __process_data(data: list) -&gt; list:\nto_del = ['metadata', 'report', 'mode']\nfor e in data:\nfor k in to_del:\ne.pop(k, None)\nreturn data\nresponse = client.get('/synthesizer')\ndata: list = response.json()\ndata = __process_data(data)\nreturn SynthesizersList(data)\n</code></pre>"},{"location":"sdk/reference/api/synthesizers/base/#ydata.sdk.synthesizers.synthesizer.BaseSynthesizer.sample","title":"<code>sample()</code>  <code>abstractmethod</code>","text":"<p>Abstract method to sample from a synthesizer.</p> Source code in <code>ydata/sdk/synthesizers/synthesizer.py</code> <pre><code>@abstractmethod\ndef sample(self) -&gt; pdDataFrame:\n\"\"\"Abstract method to sample from a synthesizer.\"\"\"\n</code></pre>"},{"location":"sdk/reference/api/synthesizers/base/#privacylevel","title":"PrivacyLevel","text":"<p>             Bases: <code>StringEnum</code></p> <p>Privacy level exposed to the end-user.</p>"},{"location":"sdk/reference/api/synthesizers/base/#ydata.sdk.synthesizers.PrivacyLevel.BALANCED_PRIVACY_FIDELITY","title":"<code>BALANCED_PRIVACY_FIDELITY = 'BALANCED_PRIVACY_FIDELITY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Balanced privacy/fidelity</p>"},{"location":"sdk/reference/api/synthesizers/base/#ydata.sdk.synthesizers.PrivacyLevel.HIGH_FIDELITY","title":"<code>HIGH_FIDELITY = 'HIGH_FIDELITY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>High fidelity</p>"},{"location":"sdk/reference/api/synthesizers/base/#ydata.sdk.synthesizers.PrivacyLevel.HIGH_PRIVACY","title":"<code>HIGH_PRIVACY = 'HIGH_PRIVACY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>High privacy</p>"},{"location":"sdk/reference/api/synthesizers/regular/","title":"Regular","text":"<p>             Bases: <code>BaseSynthesizer</code></p> Source code in <code>ydata/sdk/synthesizers/regular.py</code> <pre><code>class RegularSynthesizer(BaseSynthesizer):\ndef sample(self, n_samples: int = 1, condition_on: Optional[dict] = None) -&gt; pdDataFrame:\n\"\"\"Sample from a [`RegularSynthesizer`][ydata.sdk.synthesizers.RegularSynthesizer]\n        instance.\n        Arguments:\n            n_samples (int): number of rows in the sample\n            condition_on: (Optional[dict]): (optional) conditional sampling parameters\n        Returns:\n            synthetic data\n        \"\"\"\nif n_samples &lt; 1:\nraise InputError(\"Parameter 'n_samples' must be greater than 0\")\npayload = {\"numberOfRecords\": n_samples}\nif condition_on is not None:\npayload[\"extraData\"] = {\n\"condition_on\": condition_on\n}\nreturn self._sample(payload=payload)\ndef fit(self, X: Union[DataSource, pdDataFrame],\nprivacy_level: PrivacyLevel = PrivacyLevel.HIGH_FIDELITY,\nentities: Optional[Union[str, List[str]]] = None,\ngenerate_cols: Optional[List[str]] = None,\nexclude_cols: Optional[List[str]] = None,\ndtypes: Optional[Dict[str, Union[str, DataType]]] = None,\ntarget: Optional[str] = None,\nname: Optional[str] = None,\nanonymize: Optional[dict] = None,\ncondition_on: Optional[List[str]] = None) -&gt; None:\n\"\"\"Fit the synthesizer.\n        The synthesizer accepts as training dataset either a pandas [`DataFrame`][pandas.DataFrame] directly or a YData [`DataSource`][ydata.sdk.datasources.DataSource].\n        Arguments:\n            X (Union[DataSource, pandas.DataFrame]): Training dataset\n            privacy_level (PrivacyLevel): Synthesizer privacy level (defaults to high fidelity)\n            entities (Union[str, List[str]]): (optional) columns representing entities ID\n            generate_cols (List[str]): (optional) columns that should be synthesized\n            exclude_cols (List[str]): (optional) columns that should not be synthesized\n            dtypes (Dict[str, Union[str, DataType]]): (optional) datatype mapping that will overwrite the datasource metadata column datatypes\n            target (Optional[str]): (optional) Target column\n            name (Optional[str]): (optional) Synthesizer instance name\n            anonymize (Optional[str]): (optional) fields to anonymize and the anonymization strategy\n            condition_on: (Optional[List[str]]): (optional) list of features to condition upon\n        \"\"\"\nBaseSynthesizer.fit(self, X=X, datatype=DataSourceType.TABULAR, entities=entities,\ngenerate_cols=generate_cols, exclude_cols=exclude_cols, dtypes=dtypes,\ntarget=target, name=name, anonymize=anonymize, privacy_level=privacy_level,\ncondition_on=condition_on)\ndef __repr__(self):\nif self._model is not None:\nreturn self._model.__repr__()\nelse:\nreturn \"RegularSynthesizer(Not Initialized)\"\n</code></pre>"},{"location":"sdk/reference/api/synthesizers/regular/#ydata.sdk.synthesizers.regular.RegularSynthesizer.fit","title":"<code>fit(X, privacy_level=PrivacyLevel.HIGH_FIDELITY, entities=None, generate_cols=None, exclude_cols=None, dtypes=None, target=None, name=None, anonymize=None, condition_on=None)</code>","text":"<p>Fit the synthesizer.</p> <p>The synthesizer accepts as training dataset either a pandas <code>DataFrame</code> directly or a YData <code>DataSource</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[DataSource, DataFrame]</code> <p>Training dataset</p> required <code>privacy_level</code> <code>PrivacyLevel</code> <p>Synthesizer privacy level (defaults to high fidelity)</p> <code>HIGH_FIDELITY</code> <code>entities</code> <code>Union[str, List[str]]</code> <p>(optional) columns representing entities ID</p> <code>None</code> <code>generate_cols</code> <code>List[str]</code> <p>(optional) columns that should be synthesized</p> <code>None</code> <code>exclude_cols</code> <code>List[str]</code> <p>(optional) columns that should not be synthesized</p> <code>None</code> <code>dtypes</code> <code>Dict[str, Union[str, DataType]]</code> <p>(optional) datatype mapping that will overwrite the datasource metadata column datatypes</p> <code>None</code> <code>target</code> <code>Optional[str]</code> <p>(optional) Target column</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>(optional) Synthesizer instance name</p> <code>None</code> <code>anonymize</code> <code>Optional[str]</code> <p>(optional) fields to anonymize and the anonymization strategy</p> <code>None</code> <code>condition_on</code> <code>Optional[List[str]]</code> <p>(Optional[List[str]]): (optional) list of features to condition upon</p> <code>None</code> Source code in <code>ydata/sdk/synthesizers/regular.py</code> <pre><code>def fit(self, X: Union[DataSource, pdDataFrame],\nprivacy_level: PrivacyLevel = PrivacyLevel.HIGH_FIDELITY,\nentities: Optional[Union[str, List[str]]] = None,\ngenerate_cols: Optional[List[str]] = None,\nexclude_cols: Optional[List[str]] = None,\ndtypes: Optional[Dict[str, Union[str, DataType]]] = None,\ntarget: Optional[str] = None,\nname: Optional[str] = None,\nanonymize: Optional[dict] = None,\ncondition_on: Optional[List[str]] = None) -&gt; None:\n\"\"\"Fit the synthesizer.\n    The synthesizer accepts as training dataset either a pandas [`DataFrame`][pandas.DataFrame] directly or a YData [`DataSource`][ydata.sdk.datasources.DataSource].\n    Arguments:\n        X (Union[DataSource, pandas.DataFrame]): Training dataset\n        privacy_level (PrivacyLevel): Synthesizer privacy level (defaults to high fidelity)\n        entities (Union[str, List[str]]): (optional) columns representing entities ID\n        generate_cols (List[str]): (optional) columns that should be synthesized\n        exclude_cols (List[str]): (optional) columns that should not be synthesized\n        dtypes (Dict[str, Union[str, DataType]]): (optional) datatype mapping that will overwrite the datasource metadata column datatypes\n        target (Optional[str]): (optional) Target column\n        name (Optional[str]): (optional) Synthesizer instance name\n        anonymize (Optional[str]): (optional) fields to anonymize and the anonymization strategy\n        condition_on: (Optional[List[str]]): (optional) list of features to condition upon\n    \"\"\"\nBaseSynthesizer.fit(self, X=X, datatype=DataSourceType.TABULAR, entities=entities,\ngenerate_cols=generate_cols, exclude_cols=exclude_cols, dtypes=dtypes,\ntarget=target, name=name, anonymize=anonymize, privacy_level=privacy_level,\ncondition_on=condition_on)\n</code></pre>"},{"location":"sdk/reference/api/synthesizers/regular/#ydata.sdk.synthesizers.regular.RegularSynthesizer.sample","title":"<code>sample(n_samples=1, condition_on=None)</code>","text":"<p>Sample from a <code>RegularSynthesizer</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>number of rows in the sample</p> <code>1</code> <code>condition_on</code> <code>Optional[dict]</code> <p>(Optional[dict]): (optional) conditional sampling parameters</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>synthetic data</p> Source code in <code>ydata/sdk/synthesizers/regular.py</code> <pre><code>def sample(self, n_samples: int = 1, condition_on: Optional[dict] = None) -&gt; pdDataFrame:\n\"\"\"Sample from a [`RegularSynthesizer`][ydata.sdk.synthesizers.RegularSynthesizer]\n    instance.\n    Arguments:\n        n_samples (int): number of rows in the sample\n        condition_on: (Optional[dict]): (optional) conditional sampling parameters\n    Returns:\n        synthetic data\n    \"\"\"\nif n_samples &lt; 1:\nraise InputError(\"Parameter 'n_samples' must be greater than 0\")\npayload = {\"numberOfRecords\": n_samples}\nif condition_on is not None:\npayload[\"extraData\"] = {\n\"condition_on\": condition_on\n}\nreturn self._sample(payload=payload)\n</code></pre>"},{"location":"sdk/reference/api/synthesizers/regular/#privacylevel","title":"PrivacyLevel","text":"<p>             Bases: <code>StringEnum</code></p> <p>Privacy level exposed to the end-user.</p>"},{"location":"sdk/reference/api/synthesizers/regular/#ydata.sdk.synthesizers.PrivacyLevel.BALANCED_PRIVACY_FIDELITY","title":"<code>BALANCED_PRIVACY_FIDELITY = 'BALANCED_PRIVACY_FIDELITY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Balanced privacy/fidelity</p>"},{"location":"sdk/reference/api/synthesizers/regular/#ydata.sdk.synthesizers.PrivacyLevel.HIGH_FIDELITY","title":"<code>HIGH_FIDELITY = 'HIGH_FIDELITY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>High fidelity</p>"},{"location":"sdk/reference/api/synthesizers/regular/#ydata.sdk.synthesizers.PrivacyLevel.HIGH_PRIVACY","title":"<code>HIGH_PRIVACY = 'HIGH_PRIVACY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>High privacy</p>"},{"location":"sdk/reference/api/synthesizers/timeseries/","title":"TimeSeries","text":"<p>             Bases: <code>BaseSynthesizer</code></p> Source code in <code>ydata/sdk/synthesizers/timeseries.py</code> <pre><code>class TimeSeriesSynthesizer(BaseSynthesizer):\ndef sample(self, n_entities: int, condition_on: Optional[dict] = None) -&gt; pdDataFrame:\n\"\"\"Sample from a [`TimeSeriesSynthesizer`][ydata.sdk.synthesizers.TimeSeriesSynthesizer] instance.\n        If a training dataset was not using any `entity` column, the Synthesizer assumes a single entity.\n        A [`TimeSeriesSynthesizer`][ydata.sdk.synthesizers.TimeSeriesSynthesizer] always sample the full trajectory of its entities.\n        Arguments:\n            n_entities (int): number of entities to sample\n            condition_on: (Optional[dict]): (optional) conditional sampling parameters\n        Returns:\n            synthetic data\n        \"\"\"\nif n_entities is not None and n_entities &lt; 1:\nraise InputError(\"Parameter 'n_entities' must be greater than 0\")\npayload = {\"numberOfRecords\": n_entities}\nif condition_on is not None:\npayload[\"extraData\"] = {\n\"condition_on\": condition_on\n}\nreturn self._sample(payload=payload)\ndef fit(self, X: Union[DataSource, pdDataFrame],\nsortbykey: Optional[Union[str, List[str]]],\nprivacy_level: PrivacyLevel = PrivacyLevel.HIGH_FIDELITY,\nentities: Optional[Union[str, List[str]]] = None,\ngenerate_cols: Optional[List[str]] = None,\nexclude_cols: Optional[List[str]] = None,\ndtypes: Optional[Dict[str, Union[str, DataType]]] = None,\ntarget: Optional[str] = None,\nname: Optional[str] = None,\nanonymize: Optional[dict] = None,\ncondition_on: Optional[List[str]] = None) -&gt; None:\n\"\"\"Fit the synthesizer.\n        The synthesizer accepts as training dataset either a pandas [`DataFrame`][pandas.DataFrame] directly or a YData [`DataSource`][ydata.sdk.datasources.DataSource].\n        Arguments:\n            X (Union[DataSource, pandas.DataFrame]): Training dataset\n            sortbykey (Union[str, List[str]]): column(s) to use to sort timeseries datasets\n            privacy_level (PrivacyLevel): Synthesizer privacy level (defaults to high fidelity)\n            entities (Union[str, List[str]]): (optional) columns representing entities ID\n            generate_cols (List[str]): (optional) columns that should be synthesized\n            exclude_cols (List[str]): (optional) columns that should not be synthesized\n            dtypes (Dict[str, Union[str, DataType]]): (optional) datatype mapping that will overwrite the datasource metadata column datatypes\n            target (Optional[str]): (optional) Metadata associated to the datasource\n            name (Optional[str]): (optional) Synthesizer instance name\n            anonymize (Optional[str]): (optional) fields to anonymize and the anonymization strategy\n            condition_on: (Optional[List[str]]): (optional) list of features to condition upon\n        \"\"\"\nBaseSynthesizer.fit(self, X=X, datatype=DataSourceType.TIMESERIES, sortbykey=sortbykey,\nentities=entities, generate_cols=generate_cols, exclude_cols=exclude_cols,\ndtypes=dtypes, target=target, name=name, anonymize=anonymize, privacy_level=privacy_level,\ncondition_on=condition_on)\ndef __repr__(self):\nif self._model is not None:\nreturn self._model.__repr__()\nelse:\nreturn \"TimeSeriesSynthesizer(Not Initialized)\"\n</code></pre>"},{"location":"sdk/reference/api/synthesizers/timeseries/#ydata.sdk.synthesizers.timeseries.TimeSeriesSynthesizer.fit","title":"<code>fit(X, sortbykey, privacy_level=PrivacyLevel.HIGH_FIDELITY, entities=None, generate_cols=None, exclude_cols=None, dtypes=None, target=None, name=None, anonymize=None, condition_on=None)</code>","text":"<p>Fit the synthesizer.</p> <p>The synthesizer accepts as training dataset either a pandas <code>DataFrame</code> directly or a YData <code>DataSource</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[DataSource, DataFrame]</code> <p>Training dataset</p> required <code>sortbykey</code> <code>Union[str, List[str]]</code> <p>column(s) to use to sort timeseries datasets</p> required <code>privacy_level</code> <code>PrivacyLevel</code> <p>Synthesizer privacy level (defaults to high fidelity)</p> <code>HIGH_FIDELITY</code> <code>entities</code> <code>Union[str, List[str]]</code> <p>(optional) columns representing entities ID</p> <code>None</code> <code>generate_cols</code> <code>List[str]</code> <p>(optional) columns that should be synthesized</p> <code>None</code> <code>exclude_cols</code> <code>List[str]</code> <p>(optional) columns that should not be synthesized</p> <code>None</code> <code>dtypes</code> <code>Dict[str, Union[str, DataType]]</code> <p>(optional) datatype mapping that will overwrite the datasource metadata column datatypes</p> <code>None</code> <code>target</code> <code>Optional[str]</code> <p>(optional) Metadata associated to the datasource</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>(optional) Synthesizer instance name</p> <code>None</code> <code>anonymize</code> <code>Optional[str]</code> <p>(optional) fields to anonymize and the anonymization strategy</p> <code>None</code> <code>condition_on</code> <code>Optional[List[str]]</code> <p>(Optional[List[str]]): (optional) list of features to condition upon</p> <code>None</code> Source code in <code>ydata/sdk/synthesizers/timeseries.py</code> <pre><code>def fit(self, X: Union[DataSource, pdDataFrame],\nsortbykey: Optional[Union[str, List[str]]],\nprivacy_level: PrivacyLevel = PrivacyLevel.HIGH_FIDELITY,\nentities: Optional[Union[str, List[str]]] = None,\ngenerate_cols: Optional[List[str]] = None,\nexclude_cols: Optional[List[str]] = None,\ndtypes: Optional[Dict[str, Union[str, DataType]]] = None,\ntarget: Optional[str] = None,\nname: Optional[str] = None,\nanonymize: Optional[dict] = None,\ncondition_on: Optional[List[str]] = None) -&gt; None:\n\"\"\"Fit the synthesizer.\n    The synthesizer accepts as training dataset either a pandas [`DataFrame`][pandas.DataFrame] directly or a YData [`DataSource`][ydata.sdk.datasources.DataSource].\n    Arguments:\n        X (Union[DataSource, pandas.DataFrame]): Training dataset\n        sortbykey (Union[str, List[str]]): column(s) to use to sort timeseries datasets\n        privacy_level (PrivacyLevel): Synthesizer privacy level (defaults to high fidelity)\n        entities (Union[str, List[str]]): (optional) columns representing entities ID\n        generate_cols (List[str]): (optional) columns that should be synthesized\n        exclude_cols (List[str]): (optional) columns that should not be synthesized\n        dtypes (Dict[str, Union[str, DataType]]): (optional) datatype mapping that will overwrite the datasource metadata column datatypes\n        target (Optional[str]): (optional) Metadata associated to the datasource\n        name (Optional[str]): (optional) Synthesizer instance name\n        anonymize (Optional[str]): (optional) fields to anonymize and the anonymization strategy\n        condition_on: (Optional[List[str]]): (optional) list of features to condition upon\n    \"\"\"\nBaseSynthesizer.fit(self, X=X, datatype=DataSourceType.TIMESERIES, sortbykey=sortbykey,\nentities=entities, generate_cols=generate_cols, exclude_cols=exclude_cols,\ndtypes=dtypes, target=target, name=name, anonymize=anonymize, privacy_level=privacy_level,\ncondition_on=condition_on)\n</code></pre>"},{"location":"sdk/reference/api/synthesizers/timeseries/#ydata.sdk.synthesizers.timeseries.TimeSeriesSynthesizer.sample","title":"<code>sample(n_entities, condition_on=None)</code>","text":"<p>Sample from a <code>TimeSeriesSynthesizer</code> instance.</p> <p>If a training dataset was not using any <code>entity</code> column, the Synthesizer assumes a single entity. A <code>TimeSeriesSynthesizer</code> always sample the full trajectory of its entities.</p> <p>Parameters:</p> Name Type Description Default <code>n_entities</code> <code>int</code> <p>number of entities to sample</p> required <code>condition_on</code> <code>Optional[dict]</code> <p>(Optional[dict]): (optional) conditional sampling parameters</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>synthetic data</p> Source code in <code>ydata/sdk/synthesizers/timeseries.py</code> <pre><code>def sample(self, n_entities: int, condition_on: Optional[dict] = None) -&gt; pdDataFrame:\n\"\"\"Sample from a [`TimeSeriesSynthesizer`][ydata.sdk.synthesizers.TimeSeriesSynthesizer] instance.\n    If a training dataset was not using any `entity` column, the Synthesizer assumes a single entity.\n    A [`TimeSeriesSynthesizer`][ydata.sdk.synthesizers.TimeSeriesSynthesizer] always sample the full trajectory of its entities.\n    Arguments:\n        n_entities (int): number of entities to sample\n        condition_on: (Optional[dict]): (optional) conditional sampling parameters\n    Returns:\n        synthetic data\n    \"\"\"\nif n_entities is not None and n_entities &lt; 1:\nraise InputError(\"Parameter 'n_entities' must be greater than 0\")\npayload = {\"numberOfRecords\": n_entities}\nif condition_on is not None:\npayload[\"extraData\"] = {\n\"condition_on\": condition_on\n}\nreturn self._sample(payload=payload)\n</code></pre>"},{"location":"sdk/reference/api/synthesizers/timeseries/#privacylevel","title":"PrivacyLevel","text":"<p>             Bases: <code>StringEnum</code></p> <p>Privacy level exposed to the end-user.</p>"},{"location":"sdk/reference/api/synthesizers/timeseries/#ydata.sdk.synthesizers.PrivacyLevel.BALANCED_PRIVACY_FIDELITY","title":"<code>BALANCED_PRIVACY_FIDELITY = 'BALANCED_PRIVACY_FIDELITY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Balanced privacy/fidelity</p>"},{"location":"sdk/reference/api/synthesizers/timeseries/#ydata.sdk.synthesizers.PrivacyLevel.HIGH_FIDELITY","title":"<code>HIGH_FIDELITY = 'HIGH_FIDELITY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>High fidelity</p>"},{"location":"sdk/reference/api/synthesizers/timeseries/#ydata.sdk.synthesizers.PrivacyLevel.HIGH_PRIVACY","title":"<code>HIGH_PRIVACY = 'HIGH_PRIVACY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>High privacy</p>"},{"location":"support/help-troubleshooting/","title":"Help &amp; Troubleshooting","text":""}]}
{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>YData Fabric is a Data-Centric AI development platform that accelerates AI development by helping data practitioners achieve production-quality data.</p> <p>Much like for software engineering the quality of code is a must for the success of software development, Fabric accounts for the data quality requirements for data-driven applications. It introduces standards, processes, and acceleration to empower data science, analytics, and data engineering teams.</p> <p></p>"},{"location":"#try-fabric","title":"Try Fabric","text":"<ul> <li>Get started with Fabric Community</li> </ul>"},{"location":"#why-adopt-ydata-fabric","title":"Why adopt YData Fabric?","text":"<p>With Fabric, you can standardize the understanding of your data, quickly identify data quality issues, streamline and version your data preparation workflows and finally leverage synthetic data for privacy-compliance or as a tool to boost ML performance. Fabric is a development environment that supports a faster and easier process of preparing data for AI development. Data practitioners are using Fabric to:</p> <ul> <li>Establish a centralized and collaborative repository for data projects.</li> <li>Create and share comprehensive documentation of data, encompassing data schema, structure, and personally identifiable information (PII).</li> <li>Prevent data quality issues with standardized data quality profiling, providing visual understanding and warnings on potential issues.</li> <li>Accelerate data preparation with customizable recipes.</li> <li>Improve machine learning performance with optimal data preparation through solutions such as synthetic data.</li> <li>Shorten access to data with privacy-compliant synthetic data generatio.</li> <li>Build and streamline data preparation workflows effortlessly through a user-friendly drag-and-drop interface.</li> <li>Efficiently manage business rules, conduct comparisons, and implement version control for data workflows using pipelines.</li> </ul>"},{"location":"#key-features","title":"\ud83d\udcdd Key features","text":""},{"location":"#data-catalog","title":"Data Catalog","text":"<p>Fabric Data Catalog provides a centralized perspective on datasets within a project-basis, optimizing data management through seamless integration with the organization's existing data architectures via scalable connectors (e.g., MySQL, Google Cloud Storage, AWS S3). It standardizes data quality profiling, streamlining the processes of efficient data cleaning and preparation, while also automating the identification of Personally Identifiable Information (PII) to facilitate compliance with privacy regulations.</p> <p>Explore how a Data Catalog through a centralized repository of your datasets, schema validation, and automated data profiling.</p>"},{"location":"#labs","title":"Labs","text":"<p>Fabric's Labs environments provide collaborative, scalable, and secure workspaces layered on a flexible infrastructure, enabling users to seamlessly switch between CPUs and GPUs based on their computational needs. Labs are familiar environments that empower data developers with powerful IDEs (Jupyter Notebooks, Visual Code or H2O flow) and a seamless experience with the tools they already love combined with YData's cutting-edge SDK for data preparation.</p> <p>Learn how to use the Labs to generate synthetic data in a familiar Python interface.</p>"},{"location":"#synthetic-data","title":"Synthetic data","text":"<p>Synthetic data, enabled by YData Fabric, provides data developers with a user-friendly interfaces (UI and code) for generating artificial datasets, offering a versatile solution across formats like tabular, time-series and multi-table datasets. The generated synthetic data holds the same value of the original and aligns intricately with specific business rules, contributing to machine learning models enhancement, mitigation of privacy concerns and more robustness for data developments. Fabric offers synthetic data that is ease to adapt and configure, allows customization in what concerns privacy-utility trade-offs.</p> <p>Learn how you to create high-quality synthetic data within a user-friendly UI using Fabric\u2019s data synthesis flow.</p>"},{"location":"#pipelines","title":"Pipelines","text":"<p>Fabric Pipelines streamlines data preparation workflows by automating, orchestrating, and optimizing data pipelines, providing benefits such as flexibility, scalability, monitoring, and reproducibility for efficient and reliable data processing. The intuitive drag-and-drop interface, leveraging Jupyter notebooks or Python scripts, expedites the pipeline setup process, providing data developers with a quick and user-friendly experience.</p> <p>Explore how you can leverage Fabric Pipelines to build versionable and reproducible data preparation workflows for ML development.</p>"},{"location":"#tutorials","title":"Tutorials","text":"<p>To understand how to best apply Fabric to your use cases, start by exploring the following tutorials:</p> <ul> <li> <p>Handling Imbalanced Data for Improved Fraud DetectionLearn how to implement high-performant fraud detection models by incorporating synthetic data to balance your datasets.</p> </li> <li> <p>Prediction with Quality Inspection Learn how to develop data preparation workflows with automated data quality checks and Pipelines.</p> </li> <li> <p>Generating Synthetic Data for Financial TransactionsLearn how to use synthetic data generation to replicate your existing relational databases while ensuring referential integrity.</p> </li> </ul> <p>You can find additional examples and use cases at YData Academy GitHub Repository.</p>"},{"location":"#support","title":"\ud83d\ude4b Support","text":"<p>Facing an issue? We\u2019re committed to providing all the support you need to ensure a smooth experience using Fabric:</p> <ul> <li>Create a support ticket: our team will help you move forward!</li> <li>Contact a Fabric specialist: for personalized guidance or full access to the platform</li> </ul>"},{"location":"data_catalog/","title":"Data Catalog","text":"<p>In the realm of data management and analysis, the ability to efficiently discover, understand, and access data is crucial. Fabric's Data Catalog emerges as a pivotal solution in this context, designed to facilitate an organized, searchable, and accessible repository of metadata. This chapter introduces the concept, functionality, and advantages of the Data Catalog within Fabric's ecosystem, offering developers a comprehensive overview of its significance and utility.</p> <p>To ensure that large volumes of data can be processed through the entire data pipeline, Fabric is equipped with integrated connectors for various types of storages (from RDBMS to cloud object storage), guaranteeing the data never leaves your premises. Furthermore Fabric's Catalog ensures a timely and scalable data analysis as it runs on top of a distributed architecture powered by Kubernetes and Dask.</p> <p>The benefits of Fabric's Data Catalog for data teams are manifold, enhancing not only the efficiency but also the effectiveness of data understanding operations:</p> <ul> <li>Improved Data Accessibility: With the Data Catalog, developers can consume the data they need for a certain project through a user-friendly interface, significantly reducing the time spent searching for data across disparate sources. This enhanced discoverability makes it easier to initiate data analysis, machine learning projects,</li> <li> <p>or any other data-driven tasks.</p> </li> <li> <p>Enhanced Data Governance and Quality: Fabric's Data Catalog provides comprehensive tools for data-drive projects governance in terms of data assets, including data quality profiling and metadata management. These tools help maintain high-data quality and compliance with regulatory standards, ensuring that developers work with reliable and standardized information throughout the project.</p> </li> <li> <p>Knowledge and Insight Sharing: Through detailed metadata, data quality warnings and detailed profiling, Fabric's Data Catalog enhances the understanding of data's context and behaviour. This shared knowledge base supports better decision-making and innovation in a data-driven project.</p> </li> </ul>"},{"location":"data_catalog/#related-materials","title":"Related Materials","text":"<ul> <li>\ud83d\udcd6 Data Catalogs in the modern data stack</li> <li> How to create your first Datasource from a CSV file?</li> <li> How to create a Database in the Data Catalog?</li> <li> How to automate data quality profiling?</li> </ul>"},{"location":"data_catalog/connectors/","title":"Connectors","text":"<p>Fabric connectors play an important role in the landscape of data-driven projects, acting as essential components that facilitate the movement and integration of data across different systems, platforms, and applications. Fabric connectors where designe to offer a seamless and easy connectivity for data exchange between disparate data sources (such as databases, cloud storage systems, etc).</p>"},{"location":"data_catalog/connectors/#benefits","title":"Benefits","text":"<ul> <li>Data Integration: Fabric Connectors are primarily used to consume and integrate data a variety of different sources in a single project, ensuring that data can be easily combined, transformed, and made ready for analysis or operational use.</li> <li>Automation of data flows: They automate the process of data extraction, transformation and loading (ETL), which is crucial for maintaining up-to-date and accurate the data that is being used for a certain project.</li> <li>Simplification of data access: Fabric connectors experience simplify the process of accessing and using data from specialized or complex systems, making it easier for users without deep technical expertise to leverage data for insights.</li> <li>Enhancement of Data Security: Designed to manage in a secure way the credentials and access to your different storage.</li> </ul>"},{"location":"data_catalog/connectors/#get-started-with-fabric-connectors","title":"Get started with Fabric Connectors","text":"<ul> <li> How to create a connector in Fabric?</li> <li> How to use Object Storage Connectors through Labs?</li> <li> How to use RDBMS connectors through Labs?</li> </ul>"},{"location":"data_catalog/connectors/create_connector/","title":"How to create a connector in Fabric's Data Catalog?","text":"<p> How to create a connector to an RDBMS in Fabric?</p> <p>To create a connector in YData Fabric, select the \"Connectors\" page from the left side menu, as illustrated in the image below.</p> <p></p> <p>Click in \"Add Connector\" and a list of connector types to choose from will be shown to you.</p> <p></p> <p>For the purpose of this example we will be creating a connector to our AWS S3 storage. The credentials/secrets to your storage will be requested. After adding them, you can \"Test connection\" to ensure that all the details are correct. A confirmation message, similar to the one shown in the image below, should appear in our screen, letting you know that you can now save your connector successfully!</p> <p></p> <p>Congrats! \ud83d\ude80 You have now created your first Connector! You can now create different Datasources in your project's Data Catalog. Get ready for your journey of improved quality data for AI.</p>"},{"location":"data_catalog/connectors/supported_connections/","title":"Supported connections","text":"<p>Fabric can read and write data from a variety of data sources.</p>"},{"location":"data_catalog/connectors/supported_connections/#connectors","title":"Connectors","text":"<p>Here is the list of the available connectors in Fabric.</p> Connector Name Type Supported file types Notes AWS S3 Object Storage <code>Parquet</code> <code>CSV</code> Azure Blog Storage Object Storage <code>Parquet</code> <code>CSV</code> Azure Data Lake Object Storage <code>Parquet</code> <code>CSV</code> Google Cloud storage Object Storage <code>Parquet</code> <code>CSV</code> Upload file File <code>Parquet</code> <code>CSV</code> Maximum file size is 700MB. Bigger files should be uploaded and read from remote object storages Google BigQuery Big Table <code>Not applicable</code> MySQL RDBMS <code>Not applicable</code> Supports reading whole schemas or specifying a query Azure SQL Server RDBMS <code>Not applicable</code> Supports reading whole schemas or specifying a query PostGreSQL RDBMS <code>Not applicable</code> Supports reading whole schemas or specifying a query Snowflake RDBMS <code>Not applicable</code> Supports reading whole schemas or specifying a query Oracle DB RDBMS <code>Not applicable</code> Supports reading whole schemas or specifying a query Databricks Unity Catalog Catalog <code>Not applicable</code> Supports reading a table Databricks Delta Lake Lakehouse <code>Not applicable</code> Supports reading a table"},{"location":"data_catalog/connectors/supported_connections/#havent-found-your-storage","title":"Haven't found your storage?","text":"<p>To understand our development roadmap or to request prioritization of new data connector, reach out to us at ydata.ai/contact-us.</p>"},{"location":"data_catalog/connectors/use_in_labs/","title":"Use connectors in Lab","text":""},{"location":"data_catalog/connectors/use_in_labs/#create-a-lab-environment","title":"Create a lab environment","text":""},{"location":"data_catalog/datasources/","title":"Overview","text":"<p>YData Fabric Datasources are entities that represent specific data sets such as tables, file sets, or other structured formats within the YData Fabric platform. They offer a centralized framework for managing, cataloging, and profiling data, enhancing data management and quality.</p>"},{"location":"data_catalog/datasources/#benefits","title":"Benefits","text":"<ul> <li> <p>Summarized metadata information: Fabric Datasources provide comprehensive metadata management, offering detailed information about each datasource, including schema details, descriptions, tags, and data lineage. This metadata helps users understand the structure and context of their data.</p> </li> <li> <p>Data Quality Management: Users can find data quality warnings, validation results, cleansing suggestions, and quality scores. These features help in identifying and addressing data quality issues automatically, ensuring reliable data for analysis and decision-making.</p> </li> <li> <p>Data Profiling: Data profiling tools analyze the content and structure of datasources, providing statistical summaries, detecting patterns, assessing completeness, and evaluating data uniqueness. These insights help in understanding and improving data quality.</p> </li> <li> <p>PII Identification and Management: Fabric detects and manages Personally Identifiable Information (PII) within datasources. It includes automatic PII detection, masking tools, and compliance reporting to protect sensitive data and ensure regulatory compliance.</p> </li> <li> <p>Centralized Repository: Fabric Datasources serve as a centralized repository for data quality discovery and management. They provide a single point of access for all data assets, simplifying discovery, monitoring, and governance, and improving overall data management efficiency.</p> </li> </ul>"},{"location":"data_catalog/datasources/pii/","title":"PII identification","text":"<p>To overcome the concerns around data privacy and enable secure data sharing, Fabric incorporates an automated Personal Identifiable Information (PII) identification engine to help detect and handle potential PII.</p> What can be considered Personal Identifiable Information (PII)? <p>PII is information that, when used alone or with other relevant data, can uniquely identify an individual. PII may contain direct indentifiers (e.g., ID, VAT, Credit Card Number) and/or quasi-identifiers (e.g., age, gender, race, occupation). Correctly classifying these is crucial to reduce the risk of re-identification. Learn more about how Fabric mitigates the risk of re-identification using synthetic data.</p> <p>Fabric offers a standardized classification of PII that automatically highlights and tags potential PII. The automatic detection of PII can be enabled during the loading process of your datasets and can be leveraged to generate privacy-preserving synthetic data.</p> <p></p> <p>After the detection, the PII information will be available through the Metadata &gt; PII Types, where each column that may represent potential PII is associated to one or several tags that identify the type of information it might be leaking.</p> <p></p> <p>You can review the automatic PII classification and add additional PII tags of your own by editing the metadata and select additional tags available in a pre-defined list of values, containing the most common types of potential PII information: email, phone, VAT, zip code, among others.</p> <p></p> Need a solution to enable data sharing and comply with GDPR and CCPA regulations? <p>Using synthetic data has proven to foster a culture of data-sharing within organizations, overcoming the limitations of traditional privacy methods and maximizing data value. Try Fabric Community Version to enable secure data sharing.</p>"},{"location":"data_catalog/datasources/warnings/","title":"Warnings","text":"<p>The first technical step in any data science project is to examine the data and understand its quality, value and fitness for purpose. For this reason,  Fabric\u2019s Data Catalog includes an Overview and Warnings module  for a better understanding of the available datasets.</p>"},{"location":"data_catalog/datasources/warnings/#overview","title":"Overview","text":"<p>When clicking on a Dataset available from the Data Catalog, it will show its details page, revealing an Overview and Warnings section.</p> <p></p> <p>In the Overview, you\u2019ll get an overall perspective of your dataset\u2019s characteristics, where descriptive statistics will be presented, including:</p> <ul> <li>Basic description and tags/concepts associated to the dataset</li> <li>Memory consumption</li> <li>Number of rows</li> <li>Duplicate rows (percentage / number of records)</li> <li>Number of columns</li> <li>Total data types (numeric, categorical, string, long text, ID, date)</li> <li>Missing data (percentage / number of cells)</li> <li>Main data quality warnings</li> </ul>"},{"location":"data_catalog/datasources/warnings/#data-quality-warnings","title":"Data Quality Warnings","text":"<p>To enable data-centric development, Fabric automatically detects and signals potential data quality warnings. Warnings highlight certain peculiarities of data that might require further investigation prior to model development and deployment. However, the validity of each issued warning and whether follow-up mitigation work is needed will depend on the specific use case and on domain knowledge.</p> <p></p> <p>Fabric currently supports the following warnings:</p> <ul> <li>Constant: the column presents the same value for all observations</li> <li>Zeros:  the column presents the value \u201c0\u201d for several observations</li> <li>Unique: the column contains only unique/distinct values</li> <li>Cardinality: the columns (categorical) has a large number of distinct values</li> <li>Infinity: the column presents infinite (\\(\\inf\\)) values</li> <li>Constant_length: the column (text) has constant length</li> <li>Correlation: the columns is highly correlated with other(s)</li> <li>Skeweness: the column distribution (numerical) is skewed</li> <li>Missings: the column presents several missing values</li> <li>Non-stationarity: the column (time series) presents statistical properties that change through time</li> <li>Seasonal: the column (time series) exhibits a seasonal pattern</li> <li>Uniform: the column (numerical) follows a uniform distribution</li> <li>Imbalance: the column (categorical) presents a high imbalance ratio between existing categories</li> </ul> <p>Fabric further enables the interactive exploration of warnings, filtering over specific warnings and severity types (i.e., Moderate and High):</p> <p></p>"},{"location":"deployment_and_security/deployment/aws/bastion_host/","title":"Bastion host","text":"<p>During the installation, the user will be prompt with the possibility of allowing the creation of a bastion host. This bastion host is used by YData to give a closer support to the users. If you allow the creation of this bastion host, an EC2 will be created during installation with NO ingress rules on his security group.</p> <p>In case is needed, you will need to send the bastion host Elastic IP to YData Fabric and add an ingress rule to the security group as explained below. In the CloudFormation outputs you can find the relevant information of the EC2 bastion host, such as, elastic IP, the EC2 instance ID and the security group ID:</p> <p></p>"},{"location":"deployment_and_security/deployment/aws/bastion_host/#setting-the-sg-ingress-rule","title":"Setting the SG ingress rule","text":"<ul> <li>To give access to the bastion host, please go to the EC2 service \u2192 Security Groups.</li> <li>You can search for the security group ID provided on the template outputs:</li> </ul> <ul> <li>Go to the \"Inbound rules\" tab and click \"Edit\" inbound rules.</li> <li>You can then, add an inbound rule to allow the access to the bastion host and click Save rules, as per the image below.</li> </ul> <ul> <li>For single IP source, an IP will be given to you on the support time via email.</li> </ul>"},{"location":"deployment_and_security/deployment/aws/bastion_host/#removing-the-sg-ingress-rule","title":"Removing the SG ingress rule","text":"<ul> <li>As soon the support for the specific case ends, you must remove the SG ingress rule and click Save rules.</li> </ul>"},{"location":"deployment_and_security/deployment/aws/clean/","title":"Clean","text":"<p>The following procedure explains how to delete the platform. The full procedure takes around 45m to 1h to be completed. To clean up YData Fabric, you will need to delete the CloudFormation stack and remove the subscription.</p> <p>Please take in consideration that this will delete everything associated with the installation.</p>"},{"location":"deployment_and_security/deployment/aws/clean/#deleting-the-stacks","title":"Deleting the stacks","text":"<ul> <li>Go to the regions where the product is installed</li> <li>Go to the CloudFormation service</li> <li>Select the ydata stack</li> <li>Click in the Delete button</li> </ul> <ul> <li>Select the Extension stack and click in the Delete button.</li> </ul> <p>Note</p> <p>This will disable the extension. If you are using this extension for any other project, please do not delete this stack.</p> <p></p>"},{"location":"deployment_and_security/deployment/aws/clean/#deleting-the-subscription","title":"Deleting the subscription","text":"<ul> <li>Go to the **AWS Marketplace Subscriptions** \u2192 Manage subscriptions</li> <li>Click the YData product</li> </ul> <ul> <li>Actions \u2192 Cancel subscription</li> <li>Click the checkbox and click Yes, cancel subscription</li> </ul> <p>Following the above steps completes the process of deleting YData Fabric from your AWS Cloud instance.</p>"},{"location":"deployment_and_security/deployment/aws/deploy/","title":"Deploy","text":""},{"location":"deployment_and_security/deployment/aws/deploy/#installation-process","title":"Installation process","text":"<p>The following procedure explains how to install the platform using the CloudFormation template and how to connect to the platform after the installation. The full procedure takes around 45m to 1h to be completed. In order to install the platform in your account, the user must have basic knowledge with the used tools, such as CloudFormation, Route53 and Cognito.</p>"},{"location":"deployment_and_security/deployment/aws/deploy/#1-configure-the-product","title":"1. Configure the product","text":"<p>Make sure that you comply with the pre-flight checks</p> <p>You can check the prerequisites and pre-deploy checks.</p> <p>Start with the basic configuration for the app installation:</p> <ul> <li>Ensure you are in the right region.</li> <li>Choose the stack name \"ydata-platform\" is the default name </li> </ul>"},{"location":"deployment_and_security/deployment/aws/deploy/#network","title":"Network","text":"<p>Define your network configurations to access the platform. Using the <code>ACM Certificate ARN</code> OR the <code>Hosted Zone ID</code> and the <code>Domain</code> chosen from the preflight checklist, fill up the following parameters: </p>"},{"location":"deployment_and_security/deployment/aws/deploy/#oauth","title":"OAuth","text":"<p>Define how your users will authenticate in the platform (you can use multiple providers). </p>"},{"location":"deployment_and_security/deployment/aws/deploy/#analytics","title":"Analytics","text":"<p>You can opt for allowing or not the collection of metrics in order to help us understand how users interact with the product. No user data is collected at any point. You can find our privacy policy at ydata.ai/privacy. </p>"},{"location":"deployment_and_security/deployment/aws/deploy/#bastion-host","title":"Bastion host","text":"<p>A bastion host is created and used to give closer support to the users. The bastion host is only accessible on user demand, giving us access to EC2 setting an SG ingress rule. Set it to \"Allow\" to have it available. More information here.</p>"},{"location":"deployment_and_security/deployment/aws/deploy/#create","title":"Create","text":"<ul> <li>Check the \u201cI acknowledge that AWS CloudFormation might create IAM resources with custom names.\u201d</li> <li>Click Create Stack</li> </ul>"},{"location":"deployment_and_security/deployment/aws/deploy/#2-following-the-installation-process","title":"2. Following the installation process","text":"<p>Now we can follow the step-by-step for the installation of YData Fabric.</p> <ul> <li>Click the \u201cCreate\u201d button, the installation of the platform will start:</li> </ul> <p></p> <p>The process will take approximately 45-60 minutes.</p> <ul> <li>If the installation process occurs without any issues, you will see the CREATE_COMPLETE status in the stack:</li> </ul> <p></p> <ul> <li>If any error occur during installation, please open a support case at support.ydata.ai.</li> </ul>"},{"location":"deployment_and_security/deployment/aws/deploy/#3-post-installation-configuration","title":"3. Post installation configuration","text":""},{"location":"deployment_and_security/deployment/aws/deploy/#dns-configuration","title":"DNS Configuration","text":"<p>If you have your domain registered in Route53, you can check the CF Outputs, and click the domain name to access the platform:</p> <p></p> <p>If you are using another DNS provider or a Route53 in another account, you will need to create a CNAME record pointing to the ALB endpoint (ALBDNSName). As an example: <code>CNAME \u2192 ydata-alb-xxxxxxxxx.eu-west-1.elb.amazonaws.com</code></p>"},{"location":"deployment_and_security/deployment/aws/deploy/#4-connecting-to-the-platform","title":"4. Connecting to the platform","text":"<p>To connect the platform, please allow 20-30m so the platform is completed initialised and access using the URL displayed in the CF Outputs. For the login process, if you choose a customer custom login provider, you need to ensure that the users are created.</p> <p>Otherwise, you will need to create the users in the Cognito generated by the CloudFormation stack.</p> <p>More information under can be found at Login providers.</p> <p>\ud83d\ude80 Congratulations you are now ready to start exploring your data with YData Fabric!</p>"},{"location":"deployment_and_security/deployment/aws/instance_types/","title":"Instance types","text":"Name ID Supported System Pool CPU MIcro Pool CPU Small Pool CPU Medium Pool CPU Large Pool CPU Compute Micro Pool GPU MIcro Pool GPU Compute Micro Pool Bastion Host N. Virginia us-east-1 \u2705 t3a.2xlarge t3a.large t3a.xlarge t3a.2xlarge m5a.4xlarge r5a.4xlarge g4dn.xlarge g3.4xlarge t3a.nano Ohio us-east-2 \u2705 t3a.2xlarge t3a.large t3a.xlarge t3a.2xlarge m5a.4xlarge r5a.4xlarge g4dn.xlarge g3.4xlarge t3a.nano N. California us-west-1 \u2705 t3a.2xlarge t3a.large t3a.xlarge t3a.2xlarge m5a.4xlarge r5a.4xlarge g4dn.xlarge g3.4xlarge t3a.nano Oregon us-west-2 \u2705 t3a.2xlarge t3a.large t3a.xlarge t3a.2xlarge m5a.4xlarge r5a.4xlarge g4dn.xlarge g3.4xlarge t3a.nano Cape Town af-south-1 \u2705 t3.2xlarge t3.large t3.xlarge t3.2xlarge m5.4xlarge r5.4xlarge g4dn.xlarge g4dn.2xlarge t3.nano Melbourne ap-southeast-4 \ud83d\udd34 - - - - - - - - - Hong Kong ap-east-1 \u2705 t3.2xlarge t3.large t3.xlarge t3.2xlarge m5.4xlarge r5.4xlarge g4dn.xlarge g4dn.2xlarge t3.nano Hyderabad ap-south-2 \ud83d\udd34 - - - - - - - - - Jakarta ap-southeast-3 \ud83d\udd34 - - - - - - - - - Mumbai ap-south-1 \u2705 t3a.2xlarge t3a.large t3a.xlarge t3a.2xlarge m5a.4xlarge r5a.4xlarge g4dn.xlarge g4dn.2xlarge t3a.nano Osaka ap-northeast-3 \u2705 t3.2xlarge t3.large t3.xlarge t3.2xlarge m5.4xlarge r5.4xlarge g4dn.xlarge g4dn.2xlarge t3.nano Seoul ap-northeast-2 \u2705 t3a.2xlarge t3a.large t3a.xlarge t3a.2xlarge m5a.4xlarge r5a.4xlarge g4dn.xlarge g3.4xlarge t3a.nano Singapore ap-southeast-1 \u2705 t3a.2xlarge t3a.large t3a.xlarge t3a.2xlarge m5a.4xlarge r5a.4xlarge g4dn.xlarge g3.4xlarge t3a.nano Sydney ap-southeast-2 \u2705 t3a.2xlarge t3a.large t3a.xlarge t3a.2xlarge m5a.4xlarge r5a.4xlarge g4dn.xlarge g3.4xlarge t3a.nano Tokyo ap-northeast-1 \u2705 t3a.2xlarge t3a.large t3a.xlarge t3a.2xlarge m5a.4xlarge r5a.4xlarge g4dn.xlarge g3.4xlarge t3a.nano Canada Central ca-central-1 \u2705 t3a.2xlarge t3a.large t3a.xlarge t3a.2xlarge m5a.4xlarge r5a.4xlarge g4dn.xlarge g3.4xlarge t3a.nano Frankfurt eu-central-1 \u2705 t3a.2xlarge t3a.large t3a.xlarge t3a.2xlarge m5a.4xlarge r5a.4xlarge g4dn.xlarge g3.4xlarge t3a.nano Ireland eu-west-1 \u2705 t3a.2xlarge t3a.large t3a.xlarge t3a.2xlarge m5a.4xlarge r5a.4xlarge g4dn.xlarge g3.4xlarge t3a.nano London eu-west-2 \u2705 t3a.2xlarge t3a.large t3a.xlarge t3a.2xlarge m5a.4xlarge r5a.4xlarge g4dn.xlarge g3.4xlarge t3a.nano Milan eu-south-1 \u2705 t3a.2xlarge t3a.large t3a.xlarge t3a.2xlarge m5a.4xlarge r5a.4xlarge g4dn.xlarge g4dn.2xlarge t3a.nano Paris eu-west-3 \u2705 t3a.2xlarge t3a.large t3a.xlarge t3a.2xlarge m5a.4xlarge r5a.4xlarge g4dn.xlarge g4dn.2xlarge t3a.nano Spain eu-south-2 \ud83d\udd34 - - - - - - - - - Stockholm eu-north-1 \u2705 t3.2xlarge t3.large t3.xlarge t3.2xlarge m5.4xlarge r5.4xlarge g4dn.xlarge g4dn.2xlarge t3.nano Zurich eu-central-2 \ud83d\udd34 - - - - - - - - - Bahrain me-south-1 \u2705 t3.2xlarge t3.large t3.xlarge t3.2xlarge m5.4xlarge r5.4xlarge g4dn.xlarge g4dn.2xlarge t3.nano UAE me-central-1 \ud83d\udd34 - - - - - - - - - Tel Aviv il-central-1 \ud83d\udd34 - - - - - - - - - S\u00e3o Paulo sa-east-1 \u2705 t3a.2xlarge t3a.large t3a.xlarge t3a.2xlarge m5a.4xlarge r5a.4xlarge g4dn.xlarge g4dn.2xlarge t3a.nano"},{"location":"deployment_and_security/deployment/aws/pre_deploy_checklist/","title":"Checklist and Prerequisites","text":"<p>Deploying YData Fabric in the AWS cloud offers a scalable and efficient solution for managing and generating synthetic data. AWS provides a robust infrastructure that ensures high availability, security, and performance, making it an ideal platform for YData Fabric.</p> <p>This cloud deployment allows for rapid scaling of resources to meet varying workloads, ensuring optimal performance and cost-efficiency.</p> <p>With AWS's comprehensive security features, including data encryption, network firewalls, and identity management, your synthetic data and models are protected against unauthorized access and threats. Additionally, AWS's global infrastructure allows for the deployment of YData Fabric in multiple regions, ensuring low latency and high availability for users worldwide.</p> <p>Prerequisites</p> <p>If you don't have an AWS account, create a free account before you begin.</p>"},{"location":"deployment_and_security/deployment/aws/pre_deploy_checklist/#basic-configuration","title":"Basic Configuration","text":"<ul> <li>Stack name: The name of the CloudFormation stack</li> <li>Location: where to install the platform and create the resources. You can check the available supported regions here:</li> <li>**Available regions: ** You can find the aws regions where YData Fabric is available here.</li> </ul>"},{"location":"deployment_and_security/deployment/aws/pre_deploy_checklist/#permissions","title":"Permissions","text":"<p>Check and add (if needed) the necessary permissions to the account and region where the platform will be installed.</p> <ul> <li>Go to Identity and Access Management (IAM)</li> <li>Select your user or role used for deployment</li> <li>Under the permissions tab, check if you have the following permissions:<ul> <li>AdministratorAccess</li> </ul> </li> </ul> <p>*this will be updated in the future with only the necessary permissions to create and access the application.</p> <p>You can find AWS official documentation here.</p>"},{"location":"deployment_and_security/deployment/aws/pre_deploy_checklist/#service-linked-roles","title":"Service Linked Roles","text":"<p>During the deployment all the required Service-Linked Roles are created by AWS by default with the exception of the EKS Service-Linked Role.</p> <p>Please go to IAM \u2192 Roles Verify that the following Service-Linked role exists in IAM:</p> <ul> <li><code>AWSServiceRoleForAmazonEKS</code> </li> </ul> <p>Otherwise, please create the missing service linked role:</p> <ul> <li>Click \u201cCreate role\u201d</li> <li>Choose AWS service and EKS:</li> </ul> <p></p> <ul> <li>Click \u201cNext\u201d \u2192 \u201cNext\u201d</li> <li>Click \u201cCreate role\u201d</li> </ul> <p>*You can find AWS official documentation for service-linked roles.*</p>"},{"location":"deployment_and_security/deployment/aws/pre_deploy_checklist/#quotas","title":"Quotas","text":"<p>Check and set (if needed) new quotas for the region where the application will be installed.</p> <ul> <li>Go to Service Quotas (ensure that you are in the right region).</li> <li>Select AWS Services \u2192 Amazon Elastic Compute Cloud (Amazon EC2)</li> <li>Check for the following quota limits:</li> </ul> Quota Minimum Recommended Running On-Demand Standard (A, C, D, H, I, M, R, T, Z) instances 50\u00b9 100\u00b2 Running On-Demand G and VT instances 0\u00b9 20\u00b2 <p><sub><sup> 1. These limits are the required only for the installation of the platform. Usage is limited. 2. Each limit will depend on the platform usage and each client requirements. </sup> <p>If needed, request for a new limit to the AWS support team. More on available instance types can be found here.</p>"},{"location":"deployment_and_security/deployment/aws/pre_deploy_checklist/#network-configuration","title":"Network configuration","text":"<p>Choose how you want to connect to the platform.</p> <p>The parameters below will be used during the deployment process.</p>"},{"location":"deployment_and_security/deployment/aws/pre_deploy_checklist/#dns-configuration","title":"DNS Configuration:","text":"<p>In AWS, you will connect the platform providing your own DNS custom domain, for example: <code>platform.ydata.ai</code>. For that, a registered domain is necessary.</p>"},{"location":"deployment_and_security/deployment/aws/pre_deploy_checklist/#domain-name-and-route53-hosted-zone-id","title":"Domain Name and Route53 Hosted Zone ID","text":"<p>If you have your domain registered in Route53, you can pass the Route53 Hosted Zone ID and the Domain Name, and the CloudFormation template will create an ACM certificate and a Route53 record pointing to the ALB used to connect the platform. So no steps are required before or after the installation.</p>"},{"location":"deployment_and_security/deployment/aws/pre_deploy_checklist/#domain-name-and-acm-certificate-arn","title":"Domain Name and ACM Certificate ARN","text":"<p>Otherwise, if you have your domain registered in another provider or in a route53 in another account, you will need to do one of the following steps:</p> Create the certificate on ACM and validate it Request public certificate Certificate granted <p>After the certificate is requested, copy the CNAME value and name, and create the record in your DNS provider so the certificate can be validated.</p> Import the certificate to ACM Request public certificate Certificate granted <p>After the certificate is imported, ensure the certificate is validated.</p> <p>After the installation, you will need to create another CNAME record pointing to the ALB endpoint, available in the CF Outputs.</p> <p>For example: <code>CNAME \u2192 ydata-alb-xxxxxxxxx.eu-west-1.elb.amazonaws.com</code> </p>"},{"location":"deployment_and_security/deployment/aws/pre_deploy_checklist/#login-provider","title":"Login Provider","text":"<p>In AWS you can use multiple providers to connect to the platform. During the parameter section you can choose to create a Cognito or to use one on your own: </p> <p>Setting this to True, unless you want to use a custom one, you don\u2019t need to specify any other parameters under the OAuth Configuration.</p> You can only have one Cognito <p>You can only choose one Cognito:</p> <ul> <li>The created during the platform installation.</li> <li>One created by you, where you need to pass the credentials parameters.</li> </ul> <p>If both are set, the provided parameters will be ignored and the one created during installation will be used.</p> Some regions do not support Cognito <p>This is not currently supported for some regions! For this regions you will need to use the region specific template and pass your own custom oauth configuration!</p> <p>Check regions information here.</p> <p>You can log in to our app currently using the following providers - at least one is required, but you can choose multiple ones:</p> <ul> <li>Google</li> <li>Microsoft</li> <li>Cognito (you own or the default created during installation)</li> <li>GitHub</li> </ul> <p>More detailed instructions for each login provider can be found here. If you required another authentication method, please fill up a support case at support.ydata.ai</p> <p>After configuring your login provider, please save the values. This values will be used during the deployment process.</p>"},{"location":"deployment_and_security/deployment/aws/pre_deploy_checklist/#awsqsekscluster","title":"AWSQS::EKS::Cluster","text":"<p>YData Fabric relies on an extension to configure the EKS cluster, so you will need to activate this extension before proceeding to the installation.</p> <p>To activate the extension, you can use our *CloudFormation template*. This will open the CloudFormation template ready to create. Choose the correct region on the top. And create the stack!</p> <p>To check that the extension in properly installed, go to CloudFormation \u2192 Registry \u2192 Activated extensions - filter by \u201cPrivately registered\u201d as depicted in the image below: </p> <p>If the resource is listed, the extension in activated.</p> <p>Note</p> <p>This is extension is mandatory for the create, update and delete of the cluster. After activating this extension, please do not make changes or delete the stack.</p> <p>As soon as the above steps are all completed, you are ready to start the deployment.</p>"},{"location":"deployment_and_security/deployment/aws/regions/","title":"\ud83c\udf10 Regions","text":"<p>*Use the Cognito Unsupported Regions template.</p> Name ID Supported Notes N. Virginia us-east-1 \u2705 \u2796 Ohio us-east-2 \u2705 \u2796 N. California us-west-1 \u2705 \u2796 Oregon us-west-2 \u2705 \u2796 Cape Town af-south-1 \u2705 Cognito is not supported at the moment* Melbourne ap-southeast-4 \ud83d\udd34 No GPU machine types available at the moment Hong Kong ap-east-1 \u2705 Cognito is not supported at the moment* Hyderabad ap-south-2 \ud83d\udd34 No GPU machine types available at the moment Jakarta ap-southeast-3 \ud83d\udd34 No GPU machine types available at the moment Mumbai ap-south-1 \u2705 \u2796 Osaka ap-northeast-3 \u2705 \u2796 Seoul ap-northeast-2 \u2705 \u2796 Singapore ap-southeast-1 \u2705 \u2796 Sydney ap-southeast-2 \u2705 \u2796 Tokyo ap-northeast-1 \u2705 \u2796 Canada Central ca-central-1 \u2705 \u2796 Frankfurt eu-central-1 \u2705 \u2796 Ireland eu-west-1 \u2705 \u2796 London eu-west-2 \u2705 \u2796 Milan eu-south-1 \u2705 \u2796 Paris eu-west-3 \u2705 \u2796 Spain eu-south-2 \ud83d\udd34 No GPU machine types available at the moment Stockholm eu-north-1 \u2705 \u2796 Zurich eu-central-2 \ud83d\udd34 No GPU machine types available at the moment Bahrain me-south-1 \u2705 \u2796 UAE me-central-1 \ud83d\udd34 No GPU machine types available at the moment Tel Aviv il-central-1 \ud83d\udd34 No GPU machine types available at the moment S\u00e3o Paulo sa-east-1 \u2705 \u2796"},{"location":"deployment_and_security/deployment/aws/update/","title":"Update Fabric","text":"<p>YData is committed to providing our users with cutting-edge tools and features to enhance their data management and synthetic data generation capabilities. Our solution updates policy is designed to ensure that YData Fabric remains at the forefront of technological advancements while maintaining the highest standards of reliability, security, and user satisfaction.</p> <p>Key Aspects of Our Update Policy</p> <ul> <li>Regular Updates: We release regular updates that include new features, performance improvements, and bug fixes. These updates are aimed at enhancing the overall functionality and user experience of YData Fabric.</li> <li>User Feedback Integration: We actively seek and incorporate feedback from our user community. This ensures that our updates address real-world challenges and meet the evolving needs of our users.</li> <li>Seamless Deployment: Updates are designed to be deployed seamlessly with minimal disruption to ongoing operations. Our team provides detailed documentation and support to facilitate smooth transitions.</li> <li>Security Enhancements: We prioritize the security of our platform. Each update undergoes rigorous testing to ensure that it enhances the security posture of YData Fabric without introducing vulnerabilities.</li> <li>Compatibility and Compliance: Updates are developed to ensure compatibility with existing systems and compliance with industry standards and regulations, safeguarding the integrity and continuity of user operations.</li> </ul> <p>By adhering to this policy, YData ensures that users consistently benefit from the latest advancements in data technology, reinforcing our commitment to innovation and excellence in the field of data science and synthetic data generation.</p> <p>All updates to Fabric are user/organization triggered and by following the next steps to update your CloudFormation stack.</p>"},{"location":"deployment_and_security/deployment/aws/update/#1-get-the-most-recent-version","title":"1. Get the most recent version","text":"<ul> <li>Go to the **AWS Marketplace Subscriptions** \u2192 Manage subscriptions</li> <li>Click the YData Fabric subscription</li> </ul> <ul> <li>Click Launch more software.</li> </ul> <ul> <li>Check for new versions and click Continue to Launch. At this stage you will find the link for the new version.</li> </ul> <p>Click the deployment template associated with your installation.</p> <ul> <li>Here you will have the new template URL. Copy the link as per the image below:</li> </ul> <p></p> <ul> <li>Go to the deployed CloudFormation stack and clink in \"Update\" button.</li> <li>Choose \u201cReplace current template\u201d and provide the new stack URL.</li> </ul> <p></p> <ul> <li>For the parameters, use the same parameters or change if needed. Click Next \u2192 Next \u2192 Submit</li> </ul> <p></p> <ol> <li>Following the installation process</li> </ol> <p>Now you can follow the installation process. Different from the initial deploy, the update process will only take approximately 15-60 minutes depending on the update complexity.</p> <p>\ud83d\ude80 Congratulations you have now the latest version of YData Fabric!</p>"},{"location":"deployment_and_security/deployment/login_support/login_providers/","title":"Login Providers","text":"<p>YData Fabric offers a flexible and secure authentication system, allowing users to log in using a variety of trusted identity providers. This technical documentation provides a comprehensive guide to configuring and managing login providers for YData Fabric, including Google, Microsoft, and Amazon Cognito. By leveraging these providers, users can benefit from seamless and secure access to YData Fabric, ensuring a smooth and efficient user experience.</p>"},{"location":"deployment_and_security/deployment/login_support/login_providers/#google","title":"Google","text":"<ol> <li>Open the Google Cloud Console.</li> <li>At the top-left, click Menu&gt;APIs &amp; Services&gt;Credentials.</li> <li>Click Create Credentials&gt;OAuth client ID.</li> <li>Click Application type&gt;Web application.</li> <li>In the \"Name\" field, type a name for the credential. This name is only shown in the Cloud Console.</li> <li> <p>Leave the \u201cAuthorized JavaScript origins\u201d empty.     Add a new \u201cAuthorized redirect URIs\u201d with the platform endpoint with a suffix <code>*/dex/callback*</code>     For the provided example:</p> If you are using the DNS Public EndpointOr, if you are using the DNS  Custom Domain <p></p> <p></p> </li> <li> <p>Click \u201cCreate\u201d</p> </li> <li> <p>Save the following credentials:</p> <ul> <li> <p>a. Client ID</p> <p>The Client ID for the Web Application     - b. Client Secret</p> <p>The Client Secret for the Web Application     - c. APP Hosted domain</p> <p><code>Google supports whitelisting allowed domains when using G Suite</code> For example, for one company with the emails like person@example.com, the APP Hosted domain is example.com</p> </li> </ul> </li> <li> <p>Use the credentials as inputs for YData Fabric.</p> </li> </ol> <p>You can find more details in Google's official documentation.</p>"},{"location":"deployment_and_security/deployment/login_support/login_providers/#microsoft","title":"Microsoft","text":"<ol> <li>Open the Azure Portal</li> <li>Go to \u201cEntra ID\u201d</li> <li>Click \u201cApp registrations\u201d</li> <li>Click \u201cNew registration\u201d</li> <li>Choose a name</li> <li>For the supported account types, choose the most appropriated choice for you.</li> <li> <p>For the Redirect URI, choose \u201cWeb\u201d, and fill with the platform endpoint with a suffix <code>*/dex/callback*</code>. For the provided example:</p> If you are using the DNS Public EndpointOr, if you are using the DNS  Custom Domain <p></p> <p></p> </li> <li> <p>Click \u201cRegister\u201d</p> </li> <li>Go to \u201cCertificates &amp; Secrets\u201d, generate a new secret and save the value (not the secret id). Please choose a large expiration date. This value cannot be changed after the installation of the platform.</li> <li> <p>Go to \u201cOverview\u201d and save the following credentials:</p> <ul> <li> <p>a. Client ID</p> <p>The Application (client) ID</p> </li> <li> <p>b. Client Secret</p> <p>The secret generated in step 9 (not the secret id).</p> </li> <li> <p>c. Tenant ID</p> <p>The Directory (tenant) ID</p> </li> </ul> </li> <li> <p>Use the credentials as inputs for YData Fabric.</p> </li> </ol>"},{"location":"deployment_and_security/deployment/login_support/login_providers/#consent-workflow","title":"Consent workflow","text":"<p>The admin consent workflow is necessary to configure, so you can access the platform using the app registered above.</p> <ol> <li>Open the Azure Portal</li> <li>Go to \u201cAzure Active Directory\u201d</li> <li>Click \"Enterprise applications\u201d</li> <li>Open the \u201cConsent and permissions\u201d page \u2192 \u201cUser consent settings\u201d</li> <li>Check with the AD administrator if an administrator is required to login to the app, or if all users can consent for the apps. </li> </ol>"},{"location":"deployment_and_security/deployment/login_support/login_providers/#give-access-only-to-a-set-of-users-andor-groups","title":"Give access only to a set of users and/or groups","text":"<ol> <li>In order to give access only to a set of users or groups, open your app and click the link \u201cManaged application in local directory\u201d on the right side: </li> <li>Then, click in \u201cProperties\u201d and enable the \u201cAssignment required\u201d </li> <li>To add users and/or groups, go to \u201cUsers and Groups\u201d and click \u201cAdd user/group\u201d.</li> </ol> <p>With the above steps, only the users and groups listed here can access YData Fabric. For more information check Microsoft's official documentation for Microsoft identy platform and Microsoft Entra.</p>"},{"location":"deployment_and_security/deployment/login_support/login_providers/#aws-cognito","title":"AWS Cognito","text":"<ol> <li>Go to the Amazon Cognito console. If prompted, enter your AWS credentials.</li> <li>Choose User Pools. Create a new User Pool.</li> <li>For the \u201cConfigure security requirements\u201d, \u201cConfigure sign-up experience\u201d and \u201cConfigure message delivery\u201d tabs are up to your choices or leave as the default.</li> <li> <p>In the \u201cIntegrate your app\u201d please set the attributes as the following:</p> <ol> <li>\u201cUser Pool Name\u201d - a name of your choice</li> <li>Tick the \u201cUse the Cognito Hosted UI\u201d check box.</li> <li>\u201cDomain type\u201d, you can use a cognito or a custom domain.</li> <li>\u201cInitial app client\u201d choose \u201cPublic client\u201d and set a \u201cApp client name\u201d</li> <li>For \u201cClient secret\u201d, choose \u201cGenerate a client secret\u201d</li> <li>In the \u201cAllowed callback URLs\u201d, set your callback URL with the platform endpoint with a suffix <code>*/dex/callback*</code>     For the provided example:</li> </ol> If you are using the DNS Public EndpointOr, if you are using the DNS Custom Domain <p></p> <p></p> <ol> <li>In the \u201cAdvanced app client settings\u201d \u2192  \u201cAuthentication flows\u201d step, choose \u201cALLOW_USER_PASSWORD_AUTH\u201d</li> <li>For the \u201cOpenID Connect scopes\u201d choose: \u201cEmail\u201d, \u201cOpenID\u201d and \u201cProfile\u201d.</li> <li>Review your settings, and \u201cCreate User Pool\u201d.</li> <li>Click your new user pool, go to the \u201cApp integration\u201d tab and \u201cApp clients and analytics\u201d.</li> <li>Copy and save the Client ID and Client secret.</li> <li>For the \u201cIssuer URL\u201d, get your URL by going to https://cognito-idp.[region].amazonaws.com/[user_pool_id]/.well-known/openid-configuration And copy and save the \"issuer URL.</li> <li>Use these credentials as inputs for YData Fabric.</li> </ol> </li> </ol>"},{"location":"deployment_and_security/deployment/login_support/login_providers/#adding-new-users","title":"Adding new users","text":"<ol> <li>Go to the Cognito service.</li> <li>Click the YData platform Cognito user pool.</li> <li>Go to the Users tab</li> <li>Click Create user</li> <li>Create the users: </li> <li>The user will receive an e-mail with the temporary credentials.</li> </ol> <p>For more information check Amazon's Cognito official documentation on user pools^ and ^^user pool app client.</p>"},{"location":"deployment_and_security/deployment/login_support/login_providers/#github","title":"Github","text":"<ol> <li>Go to the GitHub OAuth Application page. If prompted, enter your GitHub credentials.</li> <li>For the \u201cApplication Name\u201d, choose anything.</li> <li>For the \u201cHomepage URL\u201d and \u201cAuthorization callback URL\u201d, fill with the platform endpoint and  platform endpoint with a suffix <code>*/dex/callback</code>* correspondingly. For the provided example:</li> </ol> If you are using the DNS Public EndpointOr, if you are using the DNS Custom Domain <ol> <li>Open your new APP and generate a new secret</li> <li>Save the Client ID and Client secret</li> <li>For the org, use your GitHub organization name.</li> </ol> <p>Finally, use these credentials as inputs for to login YData Fabric. For more information check GitHub's official login documentation.</p>"},{"location":"deployment_and_security/security/","title":"Security","text":"<p>This section describes YData\u2019s security measures to provide a best-in-class experience for its customers, ensuring not only a good product and service but also risk management and compliance.</p> <p>Visit YData's Trust page to check all the Policies, Controls and Monitoring in place.</p>"},{"location":"deployment_and_security/security/#hosting-security","title":"Hosting security","text":"<p>YData is not a cloud service provider, however, we use providers which are hosted on their data centers, such as Google, Microsoft and Amazon Web Services, when the setup is not made on the customer premises. They are leading cloud infrastructure providers with top-class safety standards. They are able to respond quickly to both operational and security, including well-defined change management policies and procedures to determine when and how change occurs.</p>"},{"location":"deployment_and_security/security/#clouds-compliance-standards","title":"Clouds compliance standards","text":"GoogleAWSMicrosoft Azure <ul> <li>CSA</li> <li>ISO 27018</li> <li>SOC 3</li> <li>ISO 27001</li> <li>SOC 1</li> <li>ISO 27017</li> <li>SOC 2</li> </ul> <ul> <li>CSA</li> <li>ISO 27017</li> <li>SOC 2</li> <li>ISO 9001</li> <li>ISO 27018</li> <li>SOC 3</li> <li>ISO 27001</li> <li>SOC 1</li> </ul> <ul> <li>CSA</li> <li>ISO 27017</li> <li>ISO 22301</li> <li>SOC</li> <li>ISO 9001</li> <li>ISO 27018</li> <li>ISO 20000-1</li> <li>ISO 27001</li> <li>ISO 27701</li> <li>WCAG</li> </ul> <p>Both physical access perimeters and entry points are strictly controlled by professional security personnel. Authorized personnel must pass a minimum of two-step verification to gain access to the authorized center floors.</p>"},{"location":"deployment_and_security/security/#corporate-security","title":"Corporate security","text":"<p>YData has applied internal security policies that are in line with the industry's ISO 27001 and SOC 2. We are regularly training our employees in safety and privacy awareness, which protects technical and non-technical roles. Training materials are developed for individual roles so that employees can fulfill their responsibilities appropriately.</p> <ul> <li>Two-step verification for all services is enforced</li> <li>Encrypted hard drives of our devices is enforced</li> <li>Hard password requirements and rotation is enforced</li> </ul>"},{"location":"deployment_and_security/security/#verification-and-access-management","title":"Verification and Access Management","text":"<p>Users can log in via a secured Authentication provider, such as Security Assurance Markup Language, Microsoft Active Directory, Google Sign In or OpenID services. All requests to any of YData\u2019s APIs must be approved. Data writing requests require at least reporting access as well as an API key. Data reading requests require full user access as well as application keys. These keys act as carrier tokens to allow access to the YData service functionality. We also use Auth0 in user identification. Auth0 can never save a password because the password is encrypted when the user logs in, and compares with AuthO's encrypted password to see if they are using the correct password.</p> <p>The user can change and save the password as they wish. The user can use all types of characters to strengthen his password.</p>"},{"location":"deployment_and_security/security/#certificate-management-communications","title":"Certificate Management &amp; Communications","text":"<p>All certificates are generated and used inside the Kubernetes cluster, using cert-manager. Exceptions for cloud providers for specific certificates and described below. Every component inside the cluster uses its own certificate, sharing the same issuer so all the components exchange encrypted communication between them.</p> AWSMicrosoft Azure <p>\"During the deployment, a certificate is requested and provisioned by Let\u2019s Encrypt to the specified domain.\"</p> <p>\"The public certificate is generated using the AWS Certificate Manager service.\"</p>"},{"location":"deployment_and_security/security/#protection-of-customer-data","title":"Protection of Customer Data","text":"<p>User uploaded information or data will be considered confidential, which is stored in encrypted form, separate from other networks, including the public network if available. Data for a limited time without user request, not allowed to come out. All data transmitted layer protection (TSL) and HTTP sent by users protected using Strike Transport Security (HSTS). The application is usable if encrypted communication is compromised. User uploaded data is not transferred from one data center to another. Encryption is used in many places to protect customer information, such as: IS-266 with encryption at rest, incomplete encryption (PGP) for system backups, KMS-based protection for privacy protection, and GPG encryption. Users can use the data stored for business or administrative purposes, but they have to go through many security levels, including multifactor authentication (MFA).</p>"},{"location":"deployment_and_security/security/#secure-build-materials-sbom","title":"Secure Build Materials (SBOM)","text":"<p>To enhance transparency and facilitate security assessments, we provide access to Secure Build Materials (SBOM) for our products and services. SBOM files offer detailed insights into the components, dependencies, and associated vulnerabilities within our software stack. These files enable stakeholders, including customers, auditors, and security researchers, to evaluate the security posture of our offerings comprehensively. For access to SBOM files and additional security-related information, please visit our Security Resources page at: Find more information here.</p>"},{"location":"deployment_and_security/security/#certification-attestation-and-framework","title":"Certification, Attestation and Framework","text":"<p>YData uses a frontend framework React (originally maintained by Facebook) which combines the use of unique user tokens to protect your users against common threats such as cross-site scripting (CSS / XSS) and cross-site request fraud (CSRF / XSRF). This makes it impossible for the user to access data from another user's account.</p>"},{"location":"deployment_and_security/security/#laws-and-regulations","title":"Laws and Regulations","text":"<p>The cloud service providers used by YData are compatible with the General Data Protection Resolution (GDPR). GDPR is working to expand its products, methods and processes to fulfill its responsibilities as a data processor. YData's security and privacy teams have established a vendor management program that determines the need for YData to be approved when it involves third parties or external vendors. Our security team recognizes that the company\u2019s information resources and vendor reliance are critical to our continued activities and service delivery. These spaces are designed to evaluate technical, physical and administrative controls and ensure that it meets the expectations of it and its customers. It is a monitoring service for infrastructure and applications. Our CCPA compliance process may provide additions so that our customers can fulfill their obligations under the CCPA if there is access to personal data, while we make no plans to transfer, process, use or store personal information.</p>"},{"location":"deployment_and_security/security/#data-security","title":"Data Security","text":"<ul> <li>No data ever leaves the costumer client cloud.</li> <li>All the data is stored using cloud specific services to ensure security, privacy and compliance with YData\u2019s customers requirements.</li> </ul>"},{"location":"deployment_and_security/security/#data-encryption","title":"Data Encryption","text":"<p>The way YData\u2019s customers communicate with the servers is through SSL / TLS connections, which are encrypted. YData protects the servers where YData Fabric is deployed from DDOS, SQL injection and other fraudulent activities. If one wants to interrupt the data transfer, one can only see a mixture of some characters, which is not possible to decrypt. All data in databases is encrypted with industry standard AES-256.</p>"},{"location":"deployment_and_security/security/#api-security","title":"API Security","text":"<p>To use the API the user needs to have a JWT token that is automatically generated by Fabric for a specific user. The token is signed and encrypted using a random key created during the deployment and only known by the service responsible for its provisioning.</p>"},{"location":"deployment_and_security/security/#availability-and-disaster-recovery","title":"Availability and disaster recovery","text":"<p>When using one of the cloud providers, the data stored in the bucket and database is distributed and copied to different servers. If a bucket or database fails, it is usually recovered from a different server without targeting other users.Databases are backed up on a daily basis and can be restored if the software or server fails significantly. Backups are stored in various European and North American data centers (depending on the customer location) for extra protection. It is not possible for YData to recover individual customer information - if you delete something in your account, it will be permanently deleted, and we will not be able to recover it.</p>"},{"location":"deployment_and_security/security/#monitoring","title":"Monitoring","text":"<p>The functionality of our applications and databases is monitored 24/7 through in-built monitoring tools provided by Google, Azure and Amazon Web Services. Internal errors or failures of our various integrations trigger logins and notifications. This usually helps us to identify the problem very quickly and remedy the situation.</p>"},{"location":"deployment_and_security/security/#full-disclosure-policy","title":"Full disclosure policy","text":"<p>If something serious happens and your data is damaged as required by GDPR, we will disclose in full (such as a data breach). Transparency is important to us and we will provide you with all the necessary information to properly assess the situation and potential impact. So far no customer data has been compromised and we aim to keep it that way.</p>"},{"location":"deployment_and_security/security/security_building_materials/","title":"Secure Build Materials (SBOM)","text":"<p>To enhance transparency and facilitate security assessments, we provide access to Secure Build Materials (SBOM) for our products and services.</p> <p>SBOM files offer detailed insights into the components, dependencies, and associated vulnerabilities within our software stack. These files enable stakeholders, including customers, auditors, and security researchers, to evaluate the security posture of our offerings comprehensively.</p>"},{"location":"deployment_and_security/security/security_building_materials/#all-files","title":"All files","text":"<p>https://s3.console.aws.amazon.com/s3/buckets/repos-sboms?region=eu-west-1&amp;bucketType=general&amp;tab=objects</p>"},{"location":"deployment_and_security/security/security_building_materials/#individual-raw-files","title":"Individual raw files","text":"<ul> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/api-gateway/docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/api-gateway/package-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/authentication-service/docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/authentication-service/package-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/aws-adapter/metering-docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/aws-adapter/package-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/aws-adapter/quota-docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/aws-asg-tags-lambda/command-docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/aws-asg-tags-lambda/lambda-docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/aws-asg-tags-lambda/package-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/azure-adapter/metering-docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/azure-adapter/package-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/azure-adapter/quota-docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/backoffice-console/command-docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/backoffice-console/package-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/backoffice/api-docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/backoffice/package-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/dashboard-app/docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/dashboard-app/package-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/datasource-controller/api-docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/datasource-controller/manager-docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/datasource-controller/package-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/dex-theme/docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/dex-theme/package-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/dockerfiles/dask-gateway-scheduler/docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/dockerfiles/dask-gateway-worker/docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/dockerfiles/h2oflow/cpu-docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/dockerfiles/h2oflow/gpu-docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/dockerfiles/jupyterlab_python/cpu-docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/dockerfiles/jupyterlab_python_community/cpu-docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/dockerfiles/jupyterlab_python_tensorflow/cpu-docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/dockerfiles/jupyterlab_python_torch/cpu-docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/dockerfiles/jupyterlab_r/cpu-docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/dockerfiles/jupyterlab_r/gpu-docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/dockerfiles/pipelines_python_tensorflow/cpu-docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/dockerfiles/pipelines_python_torch/cpu-docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/dockerfiles/pipelines_python_ydata/cpu-docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/dockerfiles/pipelines_ydata/cpu-docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/dockerfiles/visualcode/cpu-docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/dockerfiles/visualcode/gpu-docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/dockerfiles/visualcode_tensorflow/cpu-docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/dockerfiles/visualcode_torch/cpu-docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/dockerfiles/visualcode_ydata/cpu-docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/dockerfiles/ydata/cpu-docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/gcp-adapter/docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/gcp-adapter/package-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/laboratory-controller/api-docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/laboratory-controller/manager-docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/laboratory-controller/package-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/metering-service/docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/metering-service/package-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/profile-controller/api-docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/profile-controller/manager-docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/profile-controller/package-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/quota-manager/docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/quota-manager/package-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/static-content-server/docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/static-content-server/package-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/synthesizer-controller/api-docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/synthesizer-controller/manager-docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/synthesizer-controller/package-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/uploader-service/docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/uploader-service/package-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/ydata-lib-platform-integration-tool/cpu-docker-sbom.cyclonedx.json</li> <li>https://repos-sboms.s3.eu-west-1.amazonaws.com/ydata-lib-platform-integration-tool/package-sbom.cyclonedx.json</li> </ul>"},{"location":"get-started/","title":"Get started with Fabric","text":"<p>The get started is here to help you if you are not yet familiar with YData Fabric or if you just want to learn more about data quality, data preparation workflows and how you can start leveraging synthetic data. Mention to YData Fabric Community</p>"},{"location":"get-started/#create-your-first-dataset-with-the-data-catalog","title":"\ud83d\udcda Create your first Dataset with the Data Catalog","text":""},{"location":"get-started/#create-your-multi-table-dataset-with-the-data-catalog","title":"\ud83d\udcbe Create your Multi-Table Dataset with the Data Catalog","text":""},{"location":"get-started/#create-your-first-synthetic-data-generator","title":"\u2699\ufe0f Create your first Synthetic Data generator","text":""},{"location":"get-started/#create-a-relational-database-synthetic-data-generator","title":"\ud83d\uddc4\ufe0f Create a Relational Database Synthetic Data generator","text":""},{"location":"get-started/#create-your-first-lab","title":"\ud83e\uddea Create your first Lab","text":""},{"location":"get-started/#create-your-first-data-pipeline","title":"\ud83c\udf00 Create your first data Pipeline","text":""},{"location":"get-started/create_database_sd_generator/","title":"How to create your first Relational Database Synthetic Data generator","text":"<p> Check this quickstart video on how to create your first Relational Database Synthetic Data generator.</p> <p>To generate your first synthetic relational database, you need to have a Multi-Dataset already available in your Data Catalog. Check this tutorial to see how you can add your first dataset to Fabric\u2019s Data Catalog.</p> <p>With your database created as a Datasource, you are now able to start configure your Synthetic Data (SD) generator to create a replicate of your database. You can either select \"Synthetic Data\" from your left side menu, or you can select \"Create Synthetic Data\" in your project Home as shown in the image below.</p> <p></p> <p>You'll be asked to select the dataset you wish to generate synthetic data from and verify the tables you'd like to include in the synthesis process, validating their data types - Time-series or Tabular.</p> <p>Table data types are relevant for synthetic data quality</p> <p>In case some of your tables hold time-series information (meaning there is a time relation between records) it is very important that during the process of configuring your synthetic data generator you do change update your tables data types accordingly. This will not only ensure the quality of that particular table, but also the overall database quality and relations.</p> <p></p> <p>All the PK and FK identified based on the database schema definition, have an automatically created anonymization setting defined. Aa standard and incremental integer will be used as the anonymization configuration, but user can change to other pre-defined generation options or regex base (user can provide the expected pattern of generation).</p> <p></p> <p>Finally, as the last step of our process it comes the Synthetic Data generator specific configurations, for this particular case we need to define both Display Name and the Destination connector. The Destination connector it is mandatory and allow to select the database where the generated synthetic database is expected to be written. After providing both inputs we can finish the process by clicking in the \"Save\" button as per the image below.</p> <p></p> <p>Your Synthetic Data generator is now training and listed under \"Synthetic Data\". While the model is being trained, the Status will be \ud83d\udfe1, as soon as the training is completed successfully it will transition to \ud83d\udfe2. Once the Synthetic Data generator has finished training, you're ready to start generating your first synthetic dataset. You can start by exploring an overview of the model configurations and even validate the quality of the synthetic data generator from a referential integrity point of view.</p> <p></p> <p>Next, you can generate synthetic data samples by accessing the Generation tab or click on \"Go to Generation\". In this section, you are able to generate as many synthetic samples as you want. For that you need to define the size of your database in comparison to the real one. This ratio is provided as a percentage. In the example below, we have asked a sample with 100% size, meaning, a synthetic database with the same size as the original.</p> <p></p> <p>A new line in your \"Sample History\" will be shown and as soon as the sample generation is completed you will be able to check the quality the synthetic data already available in your destination database.</p> <p>Congrats! \ud83d\ude80 You have now successfully created your first Relation Synthetic Database with Fabric. Get ready for your journey of improved quality data for AI.</p>"},{"location":"get-started/create_lab/","title":"How to create your first Lab environment","text":"<p>Labs are code environments for a more flexible development of data-driven solutions while leveraging Fabric capabilities combined with already loved tools such as scikit-learn, numpy and pandas. To create your first Lab, you can use the \u201cCreate Lab\u201d from Fabric\u2019s home, or you can access it from the Labs module by selecting it on the left side menu, and clicking the \u201cCreate Lab\u201d button.</p> <p></p> <p>Next, a menu with different IDEs will be shown. As a quickstart select Jupyter Lab. As labs are development environments you will be also asked what language you would prefer your environment to support: R or Python. Select Python.</p> Select IDE Select language <p>Bundles are environments with pre-installed packages. Select YData bundle, so we can leverage some other Fabric features such as Data Profiling, Synthetic Data and Pipelines.</p> <p></p> <p>As a last step, you will be asked to configure the infrastructure resources for this new environment as well as giving it a Display Name. We will keep the defaults, but you have flexibility to select GPU acceleration or whether you need more computational resources for your developments.</p> <p></p> <p>Finally, your Lab will be created and added to the \"Labs\" list, as per the image below. The status of the lab will be \ud83d\udfe1 while preparing, and this process takes a few minutes, as the infrastructure is being allocated to your development environment. As soon as the status changes to \ud83d\udfe2, you can open your lab by clicking in the button as shown below:</p> <p></p> <p>Create a new notebook in the JupyterLab and give it a name. You are now ready to start your developments!</p> Create a new notebook Notebook created <p>Congrats! \ud83d\ude80 You have now successfully created your first Lab a code environment, so you can benefit from the most advanced Fabric features as well as compose complex data workflows. Get ready for your journey of improved quality data for AI.</p>"},{"location":"get-started/create_multitable_dataset/","title":"How to create your first Relational database in Fabric's Catalog","text":"<p>To create your first multi-table dataset in the Data Catalog, you can start by clicking on \"Add Dataset\" from the Home section. Or click to Data Catalog (on the left side menu) and click \u201cAdd Dataset\u201d.</p> <p></p> <p>After that the below modal will be shown. You will need to select a connector. To create a multi-table dataset, we need to choose an RDBMS connector like Azure SQL, Snowflake or MySQL. In this case let's select MySQL.</p> <p></p> <p>Once you've selected the \u201cMySQL\u201d connector, a new screen will appear, enabling you to introduce the connection details such as database username, host, password as well as the database name.</p> <p></p> <p>With the Connector created, you'll be able to add a dataset and specify its properties:</p> <ul> <li>Name: The name of your dataset;</li> <li>Table: You can create a dataset with all the tables from the schema or select the tables that you need in your project.</li> <li>Query: Create a single table dataset by providing a query</li> </ul> <p></p> <p>Now both the Connector to the MySQL Berka database and Berka dataset will be added to our Catalog. As soon as the status is green, you can navigate your Dataset. Click in Open dataset as per the image below.</p> <p></p> <p>Within the Dataset details, you can gain valuable insights like your database schema.</p> <p></p> <p>For each an every table you can explore the both an overview on the structure (number of columns, number of rows, etc.) but also a useful summary of the quality and warnings regarding your dataset behaviour.</p> <p></p> <p>Congrats! \ud83d\ude80 You have now successfully created your first Connector and Multi-table Dataset in Fabric\u2019s Data Catalog. To get the both the ID of your database and project you can decompose the URL from the Database schema overview page. The structure is as follows:</p> <pre><code>    https://fabric.ydata.ai/rdbms/{your-dataset-id}?ns={your-project-id}\n</code></pre> <p>Get ready for your journey of improved quality data for AI.</p>"},{"location":"get-started/create_pipeline/","title":"How to create your first Pipeline","text":"<p> Check this quickstart video on how to create your first Pipeline.</p> <p>The best way to get started with Pipelines is to use the interactive Pipeline editor available in the Labs with Jupyter Lab set as IDE. If you don't have a Lab yet, or you don't know how to create one, check our quickstart guide on how to create your first lab.</p> <p>Open an already existing lab.</p> <p>A Pipeline comprises one or more nodes that are connected (or not!) with each other to define execution dependencies. Each pipeline node is and should be implemented as a component that is expected to manage a single task, such as read the data, profiling the data, training a model, or even publishing a model to production environments.</p> <p>In this tutorial we will build a simple and generic pipeline that use a Dataset from Fabric's Data Catalog and profile to check it's quality. We have the notebooks template already available. For that you need to access the \"Academy\" folder as per the image below.</p> <p></p> <p>Make sure to copy all the files in the folder \"3 - Pipelines/quickstart\" to the root folder of your lab, as per the image below.</p> <p></p> <p>Now that we have our notebooks we need to make a small change in the notebook \"1. Read dataset\". Go back to your Data Catalog, from one of the datasets in your Catalog list, select the three vertical dots and click in \"Explore in Labs\" as shown in the image below.</p> <p></p> <p>The following screen will be shown. Click in copy.</p> <p></p> <p>Now that we have copied the code, let's get back to our \"1. Read data.ipynb\" notebook, and replace the first code cell by with the new code. This will allow us to use a dataset from the Data Catalog in our pipeline.</p> Placeholder code Replaced with code snippet <p>With our notebooks ready, we can now configure our Pipeline. For this quickstart we will be leveraging an already existing pipeline - double-click the file my_first_pipeline.pipeline. You should see a pipeline as depicted in the images below. To create a new Pipeline, you can open the lab launcher tab and select \"Pipeline Editor\".</p> Open Pipeline My first pipeline <p>Before running the pipeline, we need to check each component/step properties and configurations. Right-click each one of the steps, select \"Open Properties\", and a menu will be depicted in your right side. Make sure that you have \"YData - CPU\" selected as the Runtime Image as show below.</p> Open properties Runtime image <p>We are now ready to create and run our first pipeline. In the top left corner of the pipeline editor, the run button will be available for you to click.</p> <p></p> <p>Accept the default values shown in the run dialog and start the run</p> <p></p> <p>If the following message is shown, it means that you have create a run of your first pipeline.</p> <p></p> <p>Now that you have created your first pipeline, you can select the Pipeline from Fabric's left side menu.</p> <p></p> <p>Your most recent pipeline will be listed, as shown in below image.</p> <p></p> <p>To check the run of your pipeline, jump into the \"Run\" tab. You will be able to see your first pipeline running!</p> <p></p> <p>By clicking on top of the record you will be able to see the progress of the run step-by-step, and visualize the outputs of each and every step by clicking on each step and selecting the Visualizations tab.</p> <p></p> <p>Congrats! \ud83d\ude80 You have now successfully created your first Pipeline a code environment, so you can benefit from Fabric's orchestration engine to crate scalable, versionable and comparable data workflows. Get ready for your journey of improved quality data for AI.</p>"},{"location":"get-started/create_syntheticdata_generator/","title":"How to create your first Synthetic Data generator","text":"<p> Check this quickstart video on how to create your first Synthetic Data generator.</p> <p>To generate your first synthetic data, you need to have a Dataset already available in your Data Catalog. Check this tutorial to see how you can add your first dataset to Fabric\u2019s Data Catalog.</p> <p>With your first dataset created, you are now able to start the creation of your Synthetic Data generator. You can either select \"Synthetic Data\" from your left side menu, or you can select \"Create Synthetic Data\" in your project Home as shown in the image below.</p> <p></p> <p>You'll be asked to select the dataset you wish to generate synthetic data from and verify the columns you'd like to include in the synthesis process, validating their Variable and Data Types.</p> <p>Data types are relevant for synthetic data quality</p> <p>Data Types are important to be revisited and aligned with the objectives for the synthetic data as they can highly impact the quality of the generated data. For example, let's say we have a column that is a \"Name\", while is some situations it would make sense to consider it a String, under the light of a dataset where \"Name\" refers to the name of the product purchases, it might be more beneficial to set it as a Category.</p> <p></p> <p>Finally, as the last step of our process it comes the Synthetic Data specific configurations, for this particular case we only need to define a Display Name, and we can finish the process by clicking in the \"Save\" button as per the image below.</p> <p></p> <p>Your Synthetic Data generator is now training and listed under \"Synthetic Data\". While the model is being trained, the Status will be \ud83d\udfe1, as soon as the training is completed successfully it will transition to \ud83d\udfe2 as per the image below.</p> <p></p> <p>Once the Synthetic Data generator has finished training, you're ready to start generating your first synthetic dataset. You can start by exploring an overview of the model configurations and even download a PDF report with a comprehensive overview of your Synthetic Data Quality Metrics. Next, you can generate synthetic data samples by accessing the Generation tab or click on \"Go to Generation\".</p> <p></p> <p>In this section, you are able to generate as many synthetic samples as you want. For that you need to define the number rows to generate and click \"Generate\", as depicted in the image below.</p> <p></p> <p>A new line in your \"Sample History\" will be shown and as soon as the sample generation is completed you will be able to \"Compare\" your synthetic data with the original data, add as a Dataset with \"Add to Data Catalog\" and last but not the least download it as a file with \"Download csv\".</p> <p></p> <p>Congrats! \ud83d\ude80 You have now successfully created your first Synthetic Data generator with Fabric. Get ready for your journey of improved quality data for AI.</p>"},{"location":"get-started/fabric_community/","title":"Get started with Fabric Community","text":"<p>Fabric Community is a SaaS version that allows you to explore all the functionalities of Fabric first-hand: free, forever, for everyone. You\u2019ll be able to validate your data quality with automated profiling, unlock data sharing and improve your ML models with synthetic data, and increase your productivity with seamless integration:</p> <ul> <li>Build 1 personal project;</li> <li>Create your first Data Catalog and benefit from automated data profiling;</li> <li>Train and generate synthetic data up to 2 models and datasets with 50 columns and 100K rows;</li> <li>Optimize synthetic data quality for your use cases with an evaluation PDF report;</li> <li>Create 1 development environment (Labs) and integrate it with your familiar ML packages and workflows.</li> </ul>"},{"location":"get-started/fabric_community/#register","title":"Register","text":"<p>To register for Fabric Community:</p> <ul> <li>Access the Fabric Community Try Now and create your YData account by submitting the form</li> <li>Check your email for your login credentials</li> <li>Login into fabric.ydata.ai and enjoy!</li> </ul> <p></p> <p>Once you login, you'll access the Home page and get started with your data preparation!</p> <p></p>"},{"location":"get-started/upload_csv/","title":"How to create your first Dataset from a CSV file","text":"<p> Check this quickstart video on how to create your first Dataset from a CSV file.</p> <p>To create your first dataset in the Data Catalog, you can start by clicking on \"Add Dataset\" from the Home section. Or click to Data Catalog (on the left side menu) and click \u201cAdd Dataset\u201d.</p> <p></p> <p>After that the below modal will be shown. You will need to select a connector. To upload a CSV file, we need to select \u201cUpload CSV\u201d.</p> <p></p> <p>Once you've selected the \u201cUpload CSV\u201d connector, a new screen will appear, enabling you to upload your file and designate a name for your connector. This file upload connector will subsequently empower you to create one or more datasets from the same file at a later stage.</p> Loading area Upload csv file <p>With the Connector created, you'll be able to add a dataset and specify its properties:</p> <ul> <li>Name: The name of your dataset;</li> <li>Separator: This is an important parameter to make sure that we can parse your CSV correctly. The default value is \u201c,\u201d.</li> <li>Data Type: Whether your dataset contains tabular or time-series (i.e., containing temporal dependency) data.</li> </ul> <p></p> <p>Your created Connector (\u201cCensus File\u201d) and Dataset (\u201cCensus\u201d) will be added to the Data Catalog. As soon as the status is green, you can navigate your Dataset. Click in Open Dataset as per the image below.</p> <p></p> <p>Within the Dataset details, you can gain valuable insights through our automated data quality profiling. This includes comprehensive metadata and an overview of your data, encompassing details like row count, identification of duplicates, and insights into the overall quality of your dataset.</p> <p></p> <p>Or perhaps, you want to further explore through visualization, the profile of your data with both univariate and multivariate of your data.</p> <p></p> <p>Congrats! \ud83d\ude80 You have now successfully created your first Connector and Dataset in Fabric\u2019s Data Catalog. Get ready for your journey of improved quality data for AI.</p>"},{"location":"integrations/","title":"Integrations","text":"<p>Recognizing the modern enterprise data stack comprises a vast array of services and tools, YData Fabric is augmented by a growing ecosystem of partners and integrations, acting both upstream and downstream in the lifecycle of an AI project.</p> <p>The list below is a non-exhaustive compilation of MLOps, Data and Cloud Providers which smoothly integrate with Fabric:</p> <ul> <li>DVC: Enhancing data versioning</li> <li> <p>Databricks: Enhancing feature/data engineering before improving with YData</p> <ul> <li>\ud83d\udcda Follow Databricks step-by-step tutorials</li> <li>\ud83d\udc68\u200d\ud83d\udcbb Check code example in YData Academy</li> </ul> </li> <li> <p>Snowflake: Enhancing feature/data engineering before improving with YData</p> <ul> <li>\ud83d\udcda Follow Snowflake step-by-step tutorials</li> <li>\ud83d\udc68\u200d\ud83d\udcbb Check code example in YData Academy</li> </ul> </li> <li> <p>H2O: Framework available through code and Fabric Labs (H2O Flow)</p> </li> <li> <p>Algorithmia: Integration for easy model deployment</p> <ul> <li>\ud83d\udc68\u200d\ud83d\udcbb Check code example in YData Academy</li> </ul> </li> <li> <p>UbiOps: Integration for easy model deployment</p> <ul> <li>\ud83d\udc68\u200d\ud83d\udcbb Check code example in YData Academy</li> </ul> </li> <li> <p>Great Expectations: Data profiling is integrated with Great Expectations</p> </li> <li> <p>Azure ML: Integration for easy model deployment</p> <ul> <li>\ud83d\udc68\u200d\ud83d\udcbb Check code example in YData Academy</li> </ul> </li> <li> <p>AWS SageMaker: Integration for easy model deployment</p> <ul> <li>\ud83d\udc68\u200d\ud83d\udcbb Check code example in YData Academy</li> </ul> </li> <li> <p>Google Vertex AI: Integration for easy model deployment</p> </li> </ul> <p>Up-to-date examples</p> <p>\ud83d\udc49 For the most up-to-date examples and ready-to-use recipes of how to integrate with YData Fabric with some services above, check out the Integrations section of YData\u2019s Academy.</p>"},{"location":"integrations/databricks/integration_connectors_catalog/","title":"Connectors &amp; Catalog","text":"<p>YData Fabric provides a seamless integration with Databricks, allowing you to connect, query, and manage your data in Databricks Unity Catalog and Delta Lake with ease. This section will guide you through the benefits, setup, and usage of the Databricks' available connector in Fabric.</p> <p>Prerequisites</p> <p>Before using the YData SDK in Databricks notebooks, ensure the following prerequisites are met:</p> <ul> <li>Access to a Databricks workspace</li> <li>A valid YData Fabric account and API key</li> <li>Credentials for Databricks (tokens, Databricks host, warehouse, database, schema, etc.).</li> </ul>"},{"location":"integrations/databricks/integration_connectors_catalog/#delta-lake","title":"Delta Lake","text":"<p>Databricks Delta Lake is an open-source storage layer that brings reliability to data lakes. Built on top of Apache Spark, Delta Lake provides ACID (Atomicity, Consistency, Isolation, Durability) transaction guarantees, scalable metadata handling, and unifies streaming and batch data processing.</p> <p>In this tutorial it will be covered how you can leverage YData Fabric connectors to integrate with Databricks Delta Lake.</p>"},{"location":"integrations/databricks/integration_connectors_catalog/#setting-up-the-delta-lake-connector","title":"Setting Up the Delta Lake Connector","text":"<p>To create a Delta Lake connector in YData Fabric Ui you need to meet the following pre-requisites.</p>"},{"location":"integrations/databricks/integration_connectors_catalog/#step-by-step-creation-through-the-ui","title":"Step-by-step creation through the UI","text":"<p>To create a connector in YData Fabric, select the \"Connectors\" page from the left side menu, as illustrated in the image below.</p> <p></p> <p>Now, click in the \"Create Connector\" button and the following menu with the available connectors will be shown.</p> <p></p> <p>Depending on the cloud vendor that you have your Databricks' instance deployed, select the Delta Lake connector for AWS or Azure. After selecting the connector type \"Databricks Delta Lake\" the below menu will be shown. This is where you can configure the connection to your Delta Lake. For that you will need the following information:</p> <p></p> <ul> <li>Databricks Host: The URL of your Databricks cluster</li> <li>Access token: Your Databricks' user token</li> <li>Catalog: The name of a Catalog that you want to connect to</li> <li>Schema: The name of the schema that you want to connect to</li> </ul> <p>Depending on the cloud selected, you will be asked for the credentials to your staging storage (AWS S3 or Azure Blob Storage). In this example we are using AWS and for that reason the below inputs refer to AWS S3.</p> <ul> <li>Key ID: The Snowflake database to connect to.</li> <li>Key Secret: The schema within the database.</li> </ul> <p>And finally, the name for your connector: - Display name: A unique name for your connector.  Test your connection and that's it! \ud83d\ude80</p> <p>You are now ready to create different Datasources using this connector - read the data from a table, evaluate the quality of the data or even read a full database and generate a synthetic replica of your data! Read more about Fabric Datasources in here.</p>"},{"location":"integrations/databricks/integration_connectors_catalog/#use-it-inside-the-labs","title":"Use it inside the Labs","text":"<p>\ud83d\udc68\u200d\ud83d\udcbb Full code example and recipe can be found here.</p> <p>In case you prefer a Python interface, we also have connectors available through Fabric SDK inside the labs. For a seamless integration between the UI and the Labs environment, Fabric offers an SDK that allows you to re-use connectors, datasources and even synthesizers.</p> <p>Start by creating your code environment through the Labs. In case you need to get started with the Labs, check this step-by-step guide.</p> <pre><code>    # Importing YData's packages\n    from ydata.labs import Connectors\n    # Getting a previously created Connector\n    connector = Connectors.get(uid='insert-connector-id',\n                               namespace='indert-namespace-id')\n    print(connector)\n</code></pre>"},{"location":"integrations/databricks/integration_connectors_catalog/#read-from-your-delta-lake","title":"Read from your Delta Lake","text":"<p>Using the Delta Lake connector it is possible to:</p> <ul> <li>Get the data from a Delta Lake table</li> <li>Get a sample from a Delta Lake table</li> <li>Get the data from a query to a Delta Lake instance</li> </ul>"},{"location":"integrations/databricks/integration_connectors_catalog/#unity-catalog","title":"Unity Catalog","text":"<p>Databricks Unity Catalog is a unified governance solution for all data and AI assets within the Databricks Lakehouse Platform.</p> <p>Databricks Unity Catalog leverages the concept of Delta Sharing, meaning this is a great way not only to ensure alignment between Catalogs but also to limit the access to data. This means that byt leveraging the Unity Catalog connector, users can only access a set of data assets that were authorized for a given Share.</p>"},{"location":"integrations/databricks/integration_connectors_catalog/#step-by-step-creation-through-the-ui_1","title":"Step-by-step creation through the UI","text":"<p> How to create a connector to Databricks Unity Catalog in Fabric?</p> <p>The process to create a new connector is similar to what we have covered before to create a new Databricks Unity Catalog connector in YData Fabric.</p> <p>After selecting the connector \"Databricks Unity Catalog\", you will be requested to upload your Delta Sharing token as depicted in the image below.</p> <p></p> <p>Test your connection and that's it! \ud83d\ude80</p>"},{"location":"integrations/databricks/integration_connectors_catalog/#use-it-inside-the-labs_1","title":"Use it inside the Labs","text":"<p>\ud83d\udc68\u200d\ud83d\udcbb Full code example and recipe can be found here.</p> <p>In case you prefer a Python interface, we also have connectors available through Fabric inside the labs. Start by creating your code environment through the Labs. In case you need to get started with the Labs, check this step-by-step guide.</p>"},{"location":"integrations/databricks/integration_connectors_catalog/#navigate-your-delta-share","title":"Navigate your Delta Share","text":"<p>With your connector created you are now able to explore the schemas and tables available in a Delta share.</p> List available shares<pre><code>    #List the available shares for the provided authentication\n    connector.list_shares()\n</code></pre> List available schemas<pre><code>    #List the available schemas for a given share\n    connector.list_schemas(share_name='teste')\n</code></pre> List available tables<pre><code>    #List the available tables for a given schema in a share\n    connector.list_tables(schema_name='berka',\n                           share_name='teste')\n\n    #List all the tables regardless of share and schema\n    connector.list_all_tables()\n</code></pre>"},{"location":"integrations/databricks/integration_connectors_catalog/#read-from-your-delta-share","title":"Read from your Delta Share","text":"<p>Using the Delta Lake connector it is possible to:</p> <ul> <li>Get the data from a Delta Lake table</li> <li>Get a sample from a Delta Lake table</li> </ul> Read the data from a table<pre><code>    #This method reads all the data records in the table\n    table = connector.read_table(table_name='insert-table-name',\n                                 schema_name='insert-schema-name',\n                                 share_name='insert-share-name')\n    print(table)\n</code></pre> Read a data sample from a table<pre><code>    #This method reads all the data records in the table\n    table = connector.read_table(table_name='insert-table-name',\n                                 schema_name='insert-schema-name',\n                                 share_name='insert-share-name',\n                                 sample_size=100)\n    print(table)\n</code></pre> <p>I hope you enjoyed this quick tutorial on seamlessly integrating Databricks with your data preparation workflows. \ud83d\ude80</p>"},{"location":"integrations/databricks/integration_with_sdk/","title":"YData SDK in Databricks Notebooks","text":"<p>The YData Fabric SDK provides a powerful set of tools for integrating and enhancing data within Databricks notebooks. This guide covers the installation, basic usage, and advanced features of the Fabric SDK, helping users maximize the potential of their data for AI and machine learning applications.</p> <p>\ud83d\udc68\u200d\ud83d\udcbb Full code example and recipe can be found here.</p> <p>Prerequisites</p> <p>Before using the YData Fabric SDK in Databricks notebooks, ensure the following prerequisites are met:</p> <ul> <li>Access to a Databricks workspace</li> <li>A valid YData Fabric account and API key</li> <li>Basic knowledge of Python and Databricks notebooks</li> <li>A safe connection between your Databricks cluster and Fabric</li> </ul> <p>Best Practices</p> <ul> <li>Data Security: Ensure API keys and sensitive data are securely managed.</li> <li>Efficient Coding: Use vectorized operations for data manipulation where possible.</li> <li>Resource Management: Monitor and manage the resources used by your clusters (Databricks and Fabric) Databricks cluster to optimize performance.</li> </ul>"},{"location":"integrations/databricks/integration_with_sdk/#installation","title":"Installation","text":"<p>To install the YData SDK in a Databricks notebook, use the following command: <pre><code>%pip install ydata-sdk\ndbutils.library.restartPython()\n</code></pre> Ensure the installation is successful before proceeding to the next steps.</p>"},{"location":"integrations/databricks/integration_with_sdk/#basic-usage-data-integration","title":"Basic Usage - data integration","text":"<p>This section provides step-by-step instructions on connecting to YData Fabric and performing essential data operations using the YData SDK within Databricks notebooks. This includes establishing a secure connection to YData Fabric and accessing datasets.</p>"},{"location":"integrations/databricks/integration_with_sdk/#connecting-to-ydata-fabric","title":"Connecting to YData Fabric","text":"<p>First, establish a connection to YData Fabric using your API key:</p> <pre><code>import os\n\n# Add your Fabric token as part of your environment variables for authentication\nos.environ[\"YDATA_TOKEN\"] = '&lt;TOKEN&gt;'\n</code></pre>"},{"location":"integrations/databricks/integration_with_sdk/#data-access-manipulation","title":"Data access &amp; manipulation","text":"<p>Once connected, you can access and manipulate data within YData Fabric. For example, to list available datasets:</p> <pre><code>from ydata.sdk.datasources import DataSource\n\n#return the list of available DataSources\nDataSource.list()\n</code></pre> <p>To load a specific dataset into a Pandas DataFrame:</p> <pre><code>#get the data from an existing datasource\ndataset = DataSource.get('&lt;DATASOURCE-ID&gt;')\n</code></pre>"},{"location":"integrations/databricks/integration_with_sdk/#advanced-usage-synthetic-data-generation","title":"Advanced Usage - Synthetic data generation","text":"<p>This section explores one of the most powerful features of the Fabric SDK for enhancing and refining data within Databricks notebooks. This includes as generating synthetic data to augment datasets or to generate privacy-preserving data. By leveraging these advanced capabilities, users can significantly enhance the robustness and performance of their AI and machine learning models, unlocking the full potential of their data.</p>"},{"location":"integrations/databricks/integration_with_sdk/#privacy-preserving","title":"Privacy-preserving","text":"<p>Leveraging synthetic data allows to create privacy-preserving datasets that maintain real-world value, enabling users to work with sensitive information securely while accessing utility of real data.</p> <p>Check the SDK documentation for more information regarding privacy-controls and anonymization.</p>"},{"location":"integrations/databricks/integration_with_sdk/#from-a-datasource-in-ydata-fabric","title":"From a datasource in YData Fabric","text":"<p>Users can generate synthetic data from datasource's existing in Fabric:</p> Train a synthetic data generator<pre><code># From an existing Fabric datasource\nfrom ydata.sdk.synthesizers import RegularSynthesizer\n\nsynth = RegularSynthesizer(name='&lt;NAME-YOUR-MODEL&gt;')\nsynth.fit(X=dataset)\n</code></pre> <p>Sample from a Synthetic data generator<pre><code># From an existing Fabric datasource\nfrom ydata.sdk.synthesizers import RegularSynthesizer\n\nsynth = RegularSynthesizer(name='&lt;NAME-YOUR-MODEL&gt;')\nsynth.fit(X=dataset)\n</code></pre> After your synthetic data generator have been trained successfully you can generate as many synthetic datasets as needed Sampling from the model that we have just trained<pre><code>from ydata.sdk.synthesizers import RegularSynthesizer\nsample = synth.sample(100)\nsample.head()\n</code></pre></p> <p>It is also possible to generate data from other synthetic data generation models previously trained:</p> Generating synthetic data from a previously trained model<pre><code>from ydata.sdk.synthesizers import RegularSynthesizer\n\nexisting_synth = RegularSynthesizer('&lt;INSERT-SYNTHETIC-DATA-GENERATOR-ID&gt;').get()\nsample = existing_synth.sample(100)\n</code></pre>"},{"location":"integrations/databricks/integration_with_sdk/#from-a-datasource-in-databricks","title":"From a datasource in Databricks","text":"<p>Another important integration is to train a synthetic data generator from a dataset that you are currently exploring in your notebook environment. In order to do so, we recommend that you create your dataset using YData Fabric integration connector to your Delta Lake and follow the flow for the creation of a synthetic data generation models from Fabric existing dasources.</p> <p>For a small dataset you can also follow this tutorial.</p>"},{"location":"integrations/databricks/integration_with_sdk/#data-augmentation","title":"Data augmentation","text":"<p>Another key focus is on generating synthetic data to augment existing datasets. This technique, particularly through conditional synthetic data generation, allows users to create targeted, realistic datasets. By addressing data imbalances and enriching the training data, conditional synthetic data generation significantly enhances the robustness and performance of machine learning (ML) models, leading to more accurate and reliable outcomes.</p> Read data from a delta table<pre><code># Read data from the catalog\ndf = spark.sql(\"SELECT * FROM ydata.default.credit_scoring_labeled\")\n\n# Display the dataframe\ndisplay(df)\n</code></pre> <p>After reading the data we need to convert it to pandas dataframe in order to create our synthetic data generation model. For the augmentation use-case we will be leveraging Conditional Synthetic data generation.</p> Training a conditional synthetic data generator<pre><code>from ydata.sdk.synthesizers import RegularSynthesizer\n\n# Convert Spark dataframe to pandas dataframe\npandas_df = df.toPandas()\npandas_df = pandas_df.drop('ID', axis=1)\n\n# Train a synthetic data generator using ydata-sdk\nsynth = RegularSynthesizer(name='Synth credit scoring | Conditional')\nsynth.fit(pandas_df, condition_on='Label')\n\n# Display the synthetic dataframe\ndisplay(synth)\n</code></pre> <p>Now that we have a trained conditional synthetic data generator we are able to generate a few samples controlling the population behaviour based on the columns that we have conditioned the process to.</p> Generating a synthetic sample conditioned to column 'Label'<pre><code>#generate synthetic samples condition to Label\nsynthetic_sample = synth.sample(n_samples=len(pandas_df), condition_on={\n            \"Label\": {\n                        \"categories\": [{\n                            \"category\": 1,\n                            \"percentage\": 0.7\n                        }]\n        }\n    }\n)\n</code></pre> <p>After generating the synthetic data we can combine it with our dataset.</p> Convert the dataframe to Spark dataframe<pre><code># Enable Arrow-based columnar data transfers\nspark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n\n#Create a spark dataframe from the synthetic dataframe\nsynthetic_df = spark.createDataFrame(synthetic_sample)\n\ndisplay(synthetic_df)\n</code></pre> Combining the datasets<pre><code># Concatenate the original dataframe with the synthetic dataframe\n#removing the column ID as it is not used\ndf = df.drop('ID')\nconcatenated_df = df.union(synthetic_df)\n\n# Display the concatenated dataframe\ndisplay(concatenated_df)\n</code></pre> <p>Afterwards you can use your augmented dataset to train a Machine Learning model using MLFlow.</p>"},{"location":"integrations/databricks/overview/","title":"Overview","text":"<p>This sections provides a detailed guide on integrating YData Fabric with Databricks. By combining Databricks and YData Fabric, users gain a comprehensive AI solution. Fabric enables access to previously siloed data, enhances understanding, and improves data quality. Meanwhile, Databricks provides the scalability needed to deliver robust AI capabilities.</p>"},{"location":"integrations/databricks/overview/#integration-benefits","title":"Integration benefits","text":"<ul> <li>Enhanced Data Accessibility: Seamlessly access and integrate previously siloed data.</li> <li>Improved Data Quality: Use YData Fabric's tools to enhance the quality of your data through data preparation and augmentation.</li> <li>Scalability: Leverage Databricks' robust infrastructure to scale data processing and AI workloads.</li> <li>Streamlined Workflows: Simplify data workflows with connectors and SDKs, reducing manual effort and potential errors.</li> <li>Comprehensive Support: Benefit from extensive documentation and support for both platforms, ensuring smooth integration and operation.</li> </ul>"},{"location":"integrations/databricks/overview/#integration-methods","title":"Integration methods","text":""},{"location":"integrations/databricks/overview/#data-catalog-connectors","title":"Data Catalog - Connectors","text":"<p>YData Fabric provides a range of connectors that enable direct integration with Databricks' Unity Catalog and Delta Lake. These connectors streamline data transfer and ensure seamless interoperability between the two platforms.</p> <p>Key Features:</p> <ul> <li>Easy configuration</li> <li>Secure data transfer</li> <li>Data synchronization</li> </ul>"},{"location":"integrations/databricks/overview/#sdk","title":"SDK","text":"<p>The YData Fabric SDK offers a programmatic approach to integrating with Databricks. It provides developers with the tools and libraries needed to automate and customize data workflows between YData Fabric and Databricks.</p> <p>Key Features:</p> <ul> <li>Python based interface</li> <li>Flexible and customizable</li> <li>Comprehensive documentation and support</li> </ul> <p>Find a comprehensive guideline on using YData Fabric SDK in Databricks Notebooks.</p>"},{"location":"integrations/databricks/overview/#api","title":"API","text":"<p>The YData Fabric API allows for integration via RESTful services, providing a versatile method to interact with Databricks. This approach is ideal for applications requiring direct API calls and custom integrations.</p> <p>Key Features:</p> <ul> <li>RESTful architecture</li> <li>Language-agnostic integration</li> <li>Detailed API documentation</li> <li>Support for a wide range of operations</li> </ul>"},{"location":"integrations/databricks/overview/#integration-diagram","title":"Integration diagram","text":"<p>The integration diagram below illustrates the interaction between YData Fabric and Databricks, highlighting the data flow and key components involved in the integration process.</p> <p></p>"},{"location":"integrations/snowflake/integration_snowflake/","title":"\u2744\ufe0f Integrate Fabric with Snowflake - from Analytics to Machine Learning","text":"<p>YData Fabric provides a seamless integration with Snowflake, allowing you to connect, query, and manage your data in Snowflake with ease. This section will guide you through the benefits, setup, and usage of the Snowflake connector within YData Fabric.</p>"},{"location":"integrations/snowflake/integration_snowflake/#benefits-of-integration","title":"Benefits of Integration","text":"<p>Integrating YData Fabric with Snowflake offers several key benefits:</p> <ul> <li>Scalability: Snowflake's architecture scales effortlessly with your data needs, while YData Fabric's tools ensure efficient data integration and management.</li> <li>Performance: Leveraging Snowflake's high performance for data querying and YData Fabric's optimization techniques enhances overall data processing speed.</li> <li>Security: Snowflake's robust security features, combined with YData Fabric's data governance capabilities, ensure your data remains secure and compliant.</li> <li>Interoperability: YData Fabric simplifies the process of connecting to Snowflake, allowing you to quickly set up and start using the data without extensive configuration. Benefit from the unique Fabric functionalities like data preparation with Python, synthetic data generation and data profiling.</li> </ul>"},{"location":"integrations/snowflake/integration_snowflake/#setting-up-the-snowflake-connector","title":"Setting Up the Snowflake Connector","text":"<p> How to create a connector to Snowflake in Fabric?</p> <p>To create a Snowflake connector in YData Fabric Ui you need to meet the following pre-requisites and steps:</p> <p>Prerequisites</p> <p>Before setting up the connector, ensure you have the following:</p> <ul> <li>A Snowflake account with appropriate access permissions.</li> <li>YData Fabric installed and running in your environment.</li> <li>Credentials for Snowflake (username, password, account identifier, warehouse, database, schema).</li> </ul>"},{"location":"integrations/snowflake/integration_snowflake/#step-by-step-creation-through-the-ui","title":"Step-by-step creation through the UI","text":"<p>To create a connector in YData Fabric, select the \"Connectors\" page from the left side menu, as illustrated in the image below.</p> <p></p> <p>Now, click in the \"Create Connector\" button and the following menu with the available connectors will be shown.</p> <p></p> <p>After selecting the connector type \"Snowflake\" the below menu will be shown. This is where you can configure the connection to your Snowflake instance. For that you will need the following information:</p> <p></p> <ul> <li>Username: Your Snowflake username.</li> <li>Password: Your Snowflake password.</li> <li>Host/Account Identifier: Your Snowflake account identifier (e.g., xy12345.us-east-1).</li> <li>Port: The Snowflake port number.</li> <li>Database: The Snowflake database to connect to.</li> <li>Schema: The schema within the database.</li> <li>Warehouse: The Snowflake warehouse to use.</li> <li>Display Name: A unique name for your connector. </li> </ul> <p>Test your connection and that's it! \ud83d\ude80</p> <p>You are now ready to create different Datasources using this connector - read the data from a query, evaluate the quality of the data from a table or even read a full database and generate a synthetic replica of your data! Read more about Fabric Datasources in here.</p>"},{"location":"integrations/snowflake/integration_snowflake/#use-it-inside-the-labs","title":"Use it inside the Labs","text":"<p>\ud83d\udc68\u200d\ud83d\udcbb Full code example and recipe can be found here.</p> <p>In case you prefer a Python interface, we also have connectors available through Fabric SDK inside the labs. For a seamless integration between the UI and the Labs environment, Fabric offers an SDK that allows you to re-use connectors, datasources and even synthesizers.</p> <p>Start by creating your code environment through the Labs. In case you need to get started with the Labs, check this step-by-step guide.</p> <pre><code>    # Importing YData's packages\n    from ydata.labs import Connectors\n    # Getting a previously created Connector\n    connector = Connectors.get(uid='insert-connector-id',\n                               namespace='indert-namespace-id')\n    print(connector)\n</code></pre>"},{"location":"integrations/snowflake/integration_snowflake/#navigate-your-database","title":"Navigate your database","text":"<p>With your connector created you are now able to explore your database and available datasets.</p> List available schemas and get the metadata of a given schema<pre><code>    # returns a list of schemas\n    schemas = connector.list_schemas()\n\n    # get the metadata of a database schema, including columns and relations between tables (PK and FK)\n    schema = connector.get_database_schema('PATIENTS')\n</code></pre>"},{"location":"integrations/snowflake/integration_snowflake/#read-from-a-snowflake-instance","title":"Read from a Snowflake instance","text":"<p>Using the Snowflake connector it is possible to:</p> <ul> <li>Get the data from a Snowflake table</li> <li>Get a sample from a Snowflake table</li> <li>Get the data from a query to a Snowflake instance</li> <li>Get the full data from a selected database</li> </ul> Read full and a sample from a table<pre><code>    # returns the whole data from a given table\n    table = connector.get_table('cardio_test')\n    print(table)\n\n    # Get a sample with n rows from a given table\n    table_sample = connector.get_table_sample(table='cardio_test', sample_size=50)\n    print(table_sample)\n</code></pre> Get the data from a query<pre><code>    # returns the whole data from a given table\n    query_output = connector.query('SELECT * FROM patients.cardio_test;')\n    print(query_output)\n</code></pre>"},{"location":"integrations/snowflake/integration_snowflake/#write-to-a-snowflake-instance","title":"Write to a Snowflake instance","text":"<p>If you need to write your data into a Snowflake instance you can also leverage your Snowflake connector for the following actions:</p> <ul> <li>Write the data into a table</li> <li>Write a new database schema</li> </ul> <p>The if_exists parameter allow you to decide whether you want to append, replace or fail in case a table with the same name already exists in the schema.</p> Writing a dataset to a table in a Snowflake schema<pre><code>    connector.write_table(data=tables['cardio_test'],\n                          name='cardio',\n                          if_exists='fail')\n</code></pre> <p>table_names allow you to define a new name for the table in the database. If not provided it will be assumed the table names from your dataset. Writing a full database to a Snowflake schema<pre><code>    connector.write_database(data=database,\n                         schema_name='new_cardio',\n                         table_names={'cardio_test': 'cardio'})\n</code></pre></p> <p>I hope you enjoyed this quick tutorial on seamlessly integrating Snowflake with your data preparation workflows. \u2744\ufe0f\ud83d\ude80</p>"},{"location":"labs/","title":"Fabric coding environment","text":"<p>YData Fabric Labs are on-demand, cloud-based data development environments with automatically provisioned hardware (multiple infrastructure configurations, including GPUs, are possible) and full platform integration via a Python interface (allowing access to Data Sources, Synthesizers, and the Workspace\u2019s shared files).</p> <p>Wit Labs, you can create environment with the support to familiar IDEs like Visual Studio Code, **Jupyter Lab** and H20 Flow, with support for both Python and R are included.</p> <p>For Python specifically, pre-configured bundles including TensorFlow, PyTorch and/or the main popular data science libraries are also available, jumpstarting data development. Additional libraries can be easily installed leveraging a simple !pip install</p> <p></p>"},{"location":"labs/#get-started-with-your-first-lab","title":"Get started with your first lab","text":"<p>\ud83e\uddea Follow this step-by-step guided tutorial to create your first Lab.</p>"},{"location":"labs/#tutorials-recipes","title":"Tutorials &amp; recipes","text":"<p>Leverage YData extensive collection of tutorials and recipes that you can find in YData Academy. Quickstart or accelerate your data developments with recipes and tutorial use-cases.</p>"},{"location":"labs/overview/","title":"Overview","text":"<p>Labs exist for Data practitioners to tackle more complex use cases through a familiar environment supercharged with infrastructure, integration with other Fabric modules and access to advanced synthesis and profiling technology via a familiar python interface.</p> <p>It is the preferred environment for Data practitioners to express their domain expertise with all the required tools, technology and computational power at their fingertips. It is thus the natural continuation of the data understanding works which started in Data Sources.</p>"},{"location":"labs/overview/#supported-ides-and-images","title":"Supported IDE's and images","text":""},{"location":"labs/overview/#ides","title":"IDEs","text":"<p>YData Fabric supports integration with various Integrated Development Environments (IDEs) to enhance productivity and streamline workflows. The supported IDEs include:</p> <ul> <li>Visual Studio Code (VS Code): A highly versatile and widely-used code editor that offers robust support for numerous programming languages and frameworks. Its integration with Git and extensions like GitLens makes it ideal for version control and collaborative development.</li> <li>Jupyter Lab: An interactive development environment that allows for notebook-based data science and machine learning workflows. It supports seamless Git integration through extensions and offers a user-friendly interface for managing code, data, and visualizations.</li> <li>H2O Flow: A web-based interface specifically designed for machine learning and data analysis with the H2O platform. It provides a flow-based, interactive environment for building and deploying machine learning models.</li> </ul>"},{"location":"labs/overview/#labs-images","title":"Labs images","text":"<p>In the Labs environment, users have access to the following default images, tailored to different computational needs:</p>"},{"location":"labs/overview/#python","title":"Python","text":"<p>All the below images support Python as the programming language. Current Python version is x</p> <ul> <li>YData CPU: Optimized for general-purpose computing and data analysis tasks that do not require GPU acceleration. This image includes access to YData Fabric unique capabilities for data processing (profiling, constraints engine, synthetic data generation, etc).</li> <li>YData GPU: Designed for tasks that benefit from GPU acceleration, providing enhanced performance for large-scale data processing and machine learning operations. Also includes access to YData Fabric unique capabilities for data processing.</li> <li>YData GPU TensorFlow: Specifically configured for TensorFlow-based machine learning and deep learning applications, leveraging GPU capabilities to accelerate training and inference processes. These images ensure that users have the necessary resources and configurations to efficiently conduct their data science and machine learning projects within the Labs environment.</li> <li>YData GPU Torch: Specifically configured for Torch-based machine learning and deep learning applications, leveraging GPU capabilities to accelerate training and inference processes. These images ensure that users have the necessary resources and configurations to efficiently conduct their data science and machine learning projects within the Labs environment.</li> </ul>"},{"location":"labs/overview/#r","title":"R","text":"<p>An image for R, that allows you to leverage the latest version of the language as well as the most user libraries.</p>"},{"location":"labs/overview/#existing-labs","title":"Existing Labs","text":"<p>Existing Labs appear in the Labs pane of the web application. Besides information about its settings and status, three buttons exist:</p> <ul> <li>Open: Open the Lab\u2019s IDE in a new browser tab</li> <li>Pause: Pause the Lab. When resumed, all data will be available.</li> <li>Delete: Lab will be deleted. Data not saved in the workspace\u2019s shared folder (see below) will be deleted.</li> </ul> <p></p> <p>The details list of a Lab, with the status and its main actions.</p> <p>The Status column indicates the Labs\u2019 status. A Lab can have 4 statuses:</p> <ul> <li>\ud83d\udfe2 Lab is running</li> <li>\ud83d\udfe1 Lab is being created (hardware is being provisioned) or is either pausing or starting</li> <li>\ud83d\udd34 Lab was shutdown due to an error. A common error is the Lab going out-of-memory. Additional details are offered in the web application.</li> <li>\u26ab Lab is paused</li> </ul>"},{"location":"labs/overview/#git-integration","title":"Git integration","text":"<p>Integrating Git with Jupyter Notebooks and Visual Studio Code (VS Code) streamlines version control and collaborative workflows for data developers. This integration allows you to track changes, manage project versions, and collaborate effectively within familiar interfaces.</p>"},{"location":"labs/overview/#jupyter-lab","title":"Jupyter Lab","text":"<p>Inside of Labs that use Jupyter Lab as IDE, you will find the jupyterlab-git extension installed in the environment.</p> <p>To create or clone a new repository you need to perform the following steps:</p> Select Jupyter Lab Git extension Cloning a repository to your local env <p>For more complex actions like forking and merging branches, see the gif below: </p>"},{"location":"labs/overview/#visual-code-vs-code","title":"Visual Code (VS Code)","text":"<p>To clone or create a new git repository you can click in \"Clone Git Repository...\" and paste it in the text box in the top center area of screen as depicted in the image below.</p> Clone Git repository Cloning a repository to your local env"},{"location":"labs/overview/#building-pipelines","title":"Building Pipelines","text":"<p>Building data pipelines and breaking them down into modular components can be challenging. For instance, a typical machine learning or deep learning pipeline starts with a series of preprocessing steps, followed by experimentation and optimization, and finally deployment. Each of these stages presents unique challenges within the development lifecycle.</p> <p>Fabric Jupyter Labs simplifies this process by incorporating Elyra as the Pipeline Visual Editor. The visual editor enables users to build data pipelines from notebooks, Python scripts, and R scripts, making it easier to convert multiple notebooks or script files into batch jobs or workflows.</p> <p>Currently, these pipelines can be executed either locally in JupyterLab or on Kubeflow Pipelines, offering flexibility and scalability for various project needs. Read more about pipelines.</p>"},{"location":"pipelines/","title":"Pipelines","text":"<p>The Pipelines module of YData Fabric is a general-purpose job orchestrator with built-in scalability and modularity plus reporting and experiment tracking capabilities. With automatic hardware provisioning, on-demand or scheduled execution, run fingerprinting and a UI interface for review and configuration, Pipelines equip the Fabric with operational capabilities for interfacing with up/downstream systems (for instance to automate data ingestion, synthesis and transfer workflows) and with the ability to experiment at scale (crucial during the iterative development process required to discover the data improvement pipeline yielding the highest quality datasets).</p> <p>YData Fabric's Pipelines are based on Kubeflow Pipelines and can be created via an interactive interface in Labs with Jupyter Lab as the IDE (recommended) or via Kubeflow Pipeline\u2019s Python SDK.</p> <p>With its full integration with Fabric's scalable architecture and the ability to leverage Fabric\u2019s Python interface, Pipelines are the recommended tool to scale up notebook work to experiment at scale or move from experimentation to production.</p>"},{"location":"pipelines/#benefits","title":"Benefits","text":"<p>Using Pipelines for data preparation offers several benefits, particularly in the context of data engineering, machine learning, and data science workflows. Here are some key advantages:</p> <ul> <li>Modularity: they allow to break down data preparation into discrete, reusable steps. Each step can be independently developed, tested, and maintained, enhancing code modularity and readability.</li> <li>Automation: they automate the data preparation process, reducing the need for manual intervention and ensuring that data is consistently processed. This leads to more efficient workflows and saves time.</li> <li>Scalability: Fabric's distributed infrastructure combined with kubernetes based pipelines allows to handle large volumes of data efficiently, making them suitable for big data environments.</li> <li>Reproducibility: By defining a series of steps that transform raw data into a ready-to-use format, pipelines ensure that the same transformations are applied every time. This reproducibility is crucial for maintaining data integrity and for validating results. Maintainability:</li> <li>Versioning: support versioning of the data preparation steps. This versioning is crucial for tracking changes, auditing processes, and rolling back to previous versions if needed.</li> <li>Flexibility: and above all they can be customized to fit specific requirements of different projects. They can be adapted to include various preprocessing techniques, feature engineering steps, and data validation processes.</li> </ul>"},{"location":"pipelines/#related-materials","title":"Related Materials","text":"<ul> <li>\ud83d\udcd6 How to create your first Pipeline</li> <li> How to build a pipeline with YData Fabric</li> </ul>"},{"location":"pipelines/concepts/","title":"Concepts","text":"<p>An example pipeline (as seen in the Pipelines module of the dashboard), where each single-responsibility block corresponds to a step in a typical machine learning workflow</p> <p>Each Pipeline is a set of connected blocks. A block is a self-contained set of code, packaged as a container, that performs one step in the Pipeline. Usually, each Pipeline block corresponds to a single responsibility task in a workflow. In a machine learning workflow, each step would correspond to one block, i.e, data ingestion, data cleaning, pre-processing, ML model training, ML model evaluation.</p> <p>Each block is parametrized by:</p> <ul> <li>code: it executes (for instance, a Jupyter Notebook, a Python file, an R script)</li> <li>runtime: which specifies the container environment it runs in, allowing modularization and inter-step independence of software requirements (for instance, specific Python versions for different blocks)</li> <li>hardware requirements: depending on the workload, a block may have different needs regarding CPU/GPU/RAM. These requirements are automatically matched with the hardware availability of the cluster the Platform\u2019s running in. This, combined with the modularity of each block, allows cost and efficiency optimizations by up/downscaling hardware according to the workload.</li> <li>file dependencies: local files that need to be copied to the container environment</li> <li>environment variables, useful, for instance to apply specific settings or inject authentication credentials</li> <li>output files: files generated during the block\u2019s workload, which will be made available to all subsequent Pipeline steps</li> </ul> <p>The hierarchy of a Pipeline, in an ascending manner, is as follows:</p> <ul> <li>Run: A single execution of a Pipeline. Usually, Pipelines are run due to changes on the code, on the data sources or on its parameters (as Pipelines can have runtime parameters)</li> <li>Experiment: Groups of runs of the same Pipeline (may have different parameters, code or settings, which are then easily comparable). All runs must have an Experiment. An Experiment can contain Runs from different Pipelines.</li> <li>Pipeline Version: Pipeline definitions can be versioned (for instance, early iterations on the flow of operations; different versions for staging and production environments)</li> <li>Pipeline</li> </ul> <p>\ud83d\udcd6 Get started with the concepts and a step-by-step tutorial</p>"},{"location":"pipelines/concepts/#runs-recurring-runs","title":"Runs &amp; Recurring Runs","text":"<p>A run is a single execution of a pipeline. Runs comprise an immutable log of all experiments that you attempt, and are designed to be self-contained to allow for reproducibility. You can track the progress of a run by looking at its details page on the pipeline's UI, where you can see the runtime graph, output artifacts, and logs for each step in the run.</p> <p>A recurring run, or job in the backend APIs, is a repeatable run of a pipeline. The configuration for a recurring run includes a copy of a pipeline with all parameter values specified and a run trigger. You can start a recurring run inside any experiment, and it will periodically start a new copy of the run configuration. You can enable or disable the recurring run from the pipeline's UI. You can also specify the maximum number of concurrent runs to limit the number of runs launched in parallel. This can be helpful if the pipeline is expected to run for a long period and is triggered to run frequently.</p>"},{"location":"pipelines/concepts/#experiment","title":"Experiment","text":"<p>An experiment is a workspace where you can try different configurations of your pipelines. You can use experiments to organize your runs into logical groups. Experiments can contain arbitrary runs, including recurring runs.</p>"},{"location":"pipelines/concepts/#pipeline-pipeline-version","title":"Pipeline &amp; Pipeline Version","text":"<p>A pipeline is a description of a workflow, which can include machine learning (ML) tasks, data preparation or even the generation of synthetic data. The pipeline outlines all the components involved in the workflow and illustrates how these components interrelate in the form of a graph. The pipeline configuration defines the inputs (parameters) required to run the pipeline and specifies the inputs and outputs of each component.</p> <p>When you run a pipeline, the system launches one or more Kubernetes Pods corresponding to the steps (components) in your workflow. The Pods start Docker containers, and the containers, in turn, start your programs.</p> <p>Pipelines can be easily versioned for reproducibility of results.</p>"},{"location":"pipelines/concepts/#artifacts","title":"Artifacts","text":"<p>For each block/step in a Run, Artifacts can be generated. Artifacts are raw output data which is automatically rendered in the Pipeline\u2019s UI in a rich manner - as formatted tables, text, charts, bar graphs/scatter plots/line graphs, ROC curves, confusion matrices or inline HTML.</p> <p>Artifacts are useful to attach, to each step/block of a data improvement workflow, relevant visualizations, summary tables, data profiling reports or text analyses. They are logged by creating a JSON file with a simple, pre-specified format (according to the output artifact type). Additional types of artifacts are supported (like binary files - models, datasets), yet will not benefit from rich visualizations in the UI.</p> <p>Compare side-by-side</p> <p>\ud83d\udca1 Artifacts and Metrics can be compared side-by-side across runs, which makes them a powerful tool when doing iterative experimentation over data quality improvement pipelines.</p>"},{"location":"pipelines/concepts/#pipelines-examples-in-ydata-academy","title":"Pipelines examples in YData Academy","text":"<p>\ud83d\udc49 Use cases on YData\u2019s Academy contain examples of full use-cases as well as Pipelines interface to log metrics and artifacts.</p>"},{"location":"pipelines/runs/","title":"Creating &amp; managing runs","text":""},{"location":"pipelines/runs/#viewing-run-details","title":"Viewing Run details","text":"<p>To view a specific Run, we need to go into the Experiments list and click on the desired Run. Alternatively, accessing Runs and selecting directly the desired run is possible.</p> <p></p> <p>Acessing Runs through its Experiment</p> <p></p> <p>Viewing the full list of Runs, for all Pipelines and Experiments. Runs can be filtered and sorted based on different fields (including Metrics).</p> <p>Once a Run is selected, its graph can be viewed (and in real-time, if the Run is being executing). The graph shows the execution status of each log. Clicking on each block will reveal the block\u2019s details, including artifacts, various configuration details and logs (useful for troubleshooting).</p> <p></p> <p>The details page of a step, showing a profiling report (as HTML) as an Artifact</p> <p>The Run Output tab includes outputs such as metrics or binary artifacts.</p>"},{"location":"pipelines/runs/#creating-runs","title":"Creating Runs","text":"<p>Besides triggering Execution via the pipeline editor in Jupyter Lab or the Python SDK, the Pipelines management UI can also be used.</p>"},{"location":"pipelines/runs/#one-off","title":"One-off","text":"<p>To create a one-off run of a Pipeline, choose a Pipeline in the Pipelines section (including the specific Pipeline version, in case there are multiple definitions) and click + Create Run.</p> <p></p> <p>Creating a Run of a specific Pipeline</p> <p>To finish creating the Run, additional information is needed:</p> <ul> <li>a Description (optional)</li> <li>the Experiment (mandatory and can be chosen from the list of existing ones)</li> <li>the Run Type (which should be one-off)</li> <li>any eventual runtime parameters of the Pipeline.</li> </ul> <p></p> <p>Clicking Start **will trigger execution. Each Run will have a unique, automatically created ID.</p>  \ud83d\udca1 One-off runs are useful for, for instance, quickly trying out different parameters or for stable data pipelines where the input data has changed (unexpectedly) and the pipelines needs to be ran again."},{"location":"pipelines/runs/#recurring","title":"Recurring","text":"<p>To create a Recurring Run, the procedure shown above should be followed, but instead a Recurring Run Type should be chosen.</p> <p>The main configuration parameters of a Recurring Run are the frequency, start date and end date, as well as the maximum number of concurrent Runs of the Pipeline. The maximum number of concurrent Runs is a particularly relevant parameter for Pipelines whose execution time may stretch into the following\u2019s scheduled Run start time - it should be tweaked to avoid overwhelming the available infrastructure. Recurrency can also be configured via cron-like definitions.</p> <p></p> <p>Configuring a Recurrent Run</p> <p>The recurring run will keep on executing until its end date or until it is manually disabled. Configured Recurrent Runs are listed on the Recurring Runs section.</p>  \ud83d\udca1 Recurring runs are useful in several situations:  - determining the average execution time of a Pipeline (in case there are run-dependent time fluctuations) - when any of the inputs (for instance, input data read from a remote location) changes at a predictable pace"},{"location":"pipelines/runs/#creating-a-pipeline","title":"Creating a Pipeline","text":"<p>The recommended way to create a Pipeline is to use the interactive Pipeline editor available on Labs with Jupyter Lab set as IDE. It allows the:</p> <ul> <li>addition of blocks by dragging and dropping notebooks/Python scripts/R scripts (can be a mixture)</li> <li>connecting blocks in linear and non-linear ways to define the execution sequence</li> <li>configuring the parameters of each block in-line.</li> </ul> <p>Building a simple synthetic data generation pipeline in the interactive editor by dragging and dropping Jupyter Notebooks (Python/R files could also be dragged), leveraging input files for credentials, environment variables for workflow settings, software runtime specification and per-block hardware needs.</p> <p>Building a simple synthetic data generation pipeline in the interactive editor by dragging and dropping Jupyter Notebooks (Python/R files could also be dragged), leveraging input files for credentials, environment variables for workflow settings, software runtime specification and per-block hardware needs.</p> <p>The built Pipeline can be directly ran from the editor. It will then be automatically available in the dashboard\u2019s web UI, where it can be viewed and managed.</p>  \ud83d\udc49 To build Pipelines fully via code (in any Python IDE), refer to the [Kubeflow Pipelines SDK](https://www.kubeflow.org/docs/components/pipelines/sdk/sdk-overview/)."},{"location":"pipelines/runs/#managing-pipelines","title":"Managing Pipelines","text":"<p>The Pipelines management interface is accessible in the platform\u2019s dashboard, via the sidebar item Pipelines.</p> <p></p> <p>The Pipelines management module</p> <p>It has 6 main sub-modules:</p> <ul> <li>Pipelines: list of existing Pipelines, which can be further drilled-down into the versions of each Pipeline, as Pipeline definitions can be versioned.</li> <li>Experiments: a **list of all available Experiments (groups of Runs), regardless of their origin Pipeline.</li> <li>Runs: a **list of all available Runs, regardless of their origin Pipeline/Experiment.</li> <li>Recurring Runs: an interface to view and configure the Runs triggered on a schedule.</li> <li>Artifacts: list of Artifacts generated by all Runs of all Pipelines</li> <li>Executions: a list of all executed blocks/steps across all Runs of all Pipelines</li> </ul>  \ud83d\udca1 Pipelines created via code can be compiled to a `.pipeline` file, which can then be submited via the *+ Upload pipeline* button."},{"location":"pipelines/runs/#creating-a-new-experiment","title":"Creating a new Experiment","text":"<p>An experiment is used to group together the runs of a single or different Pipelines. It is particularly useful for organization and Artifacts/Metrics comparison purposes.</p> <p>To create a new Experiment, access the Experiments section and click + Create Experiment. An Experiment requires a name and an optional description.</p>"},{"location":"pipelines/runs/#comparing-runs","title":"Comparing Runs","text":"<p>Comparing runs is particularly useful in iterative data improvement scenarios, as Artifacts, Metrics and Parameters can be directly compared side-by-side. Runs using different pre-processing techniques, settings, algorithms can be put against each other side-by-side in a visual and intuitive interface.</p> <p>To compare multiple Runs, select the Runs of interest (either from the Experiments or Runs pane) and select Compare runs:</p> <p></p> <p>Selecting Runs to compare from the Experiments list</p> <p></p> <p>In case of this particular data quality improvement Pipeline, the Metrics of each Run are shown side by side.</p> <p>Up to 10 runs can be selected for side-by-side comparison. In case any step of the Run has logged Artifacts, the equivalent Artifacts are shown in a comparative interface.</p> <p></p> <p>Comparing the confusion matrices of three Runs of a Pipeline, which were logged as Artifacts during one of the Pipeline\u2019s steps.</p>"},{"location":"pipelines/runs/#cloning-runs","title":"Cloning Runs","text":"<p>For full reproducibility purposes, it is possible to select a previous run and clone it. Cloned runs will use exactly the same runtime input parameters and settings. However, any time dependent inputs (like the state of a remote data source at a particular point in time) will not be recreated.</p> <p>To clone a Run, click the Clone run button available in a Run\u2019s detail page or in the list of Runs/Experiment (when a single Run is selected). It will be possible to review the settings prior to triggering the execution.</p>"},{"location":"pipelines/runs/#archiving-runs","title":"Archiving Runs","text":"<p>Archiving a Run will move it to the Archived section the Runs and Experiments list. This section can be used to save older executions, to highlight best runs or to record anomalous executions which require further digging into.</p> <p>Archive a Run by clicking the Archive button from the Run\u2019s details page (or from the list of Runs/Experiments when a Run is selected).</p> <p></p> <p>The Archived section, which is in all ways similar to the list of Active buttons. The Restore button (highlighted) moves Runs between the two sections.</p> <p>When a Run is archived, it can be restored through the Restore button.</p>  \ud83d\udca1 **Learn by example**  To understand how to best apply the full capabilities of Pipelines in real world use cases, check out the [use cases section of YData\u2019s Academy](https://github.com/ydataai/academy/tree/master/5%20-%20use-cases).  Most use cases include a pipeline leveraging common and use case specific features of the Pipelines module. These pipelines are offered in  `.pipeline` files which can be interactively explored in Jupyter Lab, inside Labs.   <p></p>"},{"location":"sdk/","title":"Overview","text":"<p>YData Fabric SDK for improved data quality everywhere!</p> <p>To start using create a Fabric community account at ydata.ai/register</p>"},{"location":"sdk/#overview","title":"Overview","text":"<p>The Fabric SDK is an ecosystem of methods that allows users to, through a python interface, adopt data development focused on improving the quality of the data. The solution includes a set of integrated components for data ingestion, standardized data quality evaluation and data improvement, such as synthetic data generation, allowing an iterative improvement of the datasets used in high-impact business applications.</p>"},{"location":"sdk/#benefits","title":"Benefits","text":"<p>Fabric SDK interface enables the ability to integrate data quality tooling with other platforms offering several beneficts in the realm of data science development and data management:</p> <ul> <li>Interoperability: seamless integration with other data platform and systems like Databricks, Snowflake, etc. This ensures that all your software will work cohesively with all the elements from your data architecture.</li> <li>Collaboration: ease of integration with a multitude of tools and services, reducing the need to reinvent the wheel and fostering a collaborative environment for all developers (data scientists, data engineers, software developers, etc.)</li> <li>Improved usage experience: Fabric SDK enables a well-integrated software solution, which allows a seamless transition between different tools or platforms without facing compatibility issues.</li> </ul>"},{"location":"sdk/#current-functionality","title":"Current functionality","text":"<p>Fabric SDK is currently composed by the following main modules:</p> <ul> <li> <p>Datasources</p> <ul> <li>YData\u2019s SDK includes several connectors for easy integration with existing data sources. It supports several storage types, like filesystems and RDBMS. Check the list of connectors.</li> <li>SDK\u2019s Datasources run on top of Dask, which allows it to deal with not only small workloads but also larger volumes of data.</li> </ul> </li> <li> <p>Synthetic data generators</p> <ul> <li>Simplified interface to train a generative model and learn in a data-driven manner the behavior, the patterns and original data distribution. Optimize your model for privacy or utility use-cases.</li> <li>From a trained synthetic data generator, you can generate synthetic samples as needed and parametrise the number of records needed.</li> <li>Anonymization and privacy preserving capabilities to ensure that synthetic datasets does not contain Personal Identifiable Information (PII) and can safely be shared!</li> <li>Conditional sampling can be used to restrict the domain and values of specific features in the sampled data.</li> </ul> </li> <li> <p>Synthetic data quality report Coming soon</p> <ul> <li>An extensive synthetic data quality report that measures 3 dimensions: privacy, utility and fidelity of the generated data. The report can be downloaded in PDF format for ease of sharing and compliance purposes or as a JSON to enable the integration in data flows.</li> </ul> </li> <li> <p>Profiling Coming soon</p> <ul> <li>A set of metrics and algorithms summarizes datasets quality in three main dimensions: warnings, univariate analysis and a multivariate perspective.</li> </ul> </li> </ul>"},{"location":"sdk/#supported-data-formats","title":"Supported data formats","text":"TabularTime-SeriesTransactionalRelational databases <p> The RegularSynthesizer is perfect to synthesize high-dimensional data, that is time-indepentent with high quality results.</p> <p> The TimeSeriesSynthesizer is perfect to synthesize both regularly and not evenly spaced time-series, from smart-sensors to stock.</p> <p> The TimeSeriesSynthesizer supports transactional data, known to have highly irregular time intervals between records and directional relations between entities.</p> <p>Coming soon</p> <p> The MultiTableSynthesizer is perfect to learn how to replicate the data within a relational database schema.</p>"},{"location":"sdk/installation/","title":"Installation","text":"<p>YData SDK is generally available through both Pypi and Conda allowing an easy process of installation. This experience allows combining YData SDK with other packages such as Pandas, Numpy or Scikit-Learn.</p> <p>YData SDK is available for the public through a token-based authentication system. If you don\u2019t have one yet, you can get your free license key during the installation process. You can check what features are available in the free version here.</p>"},{"location":"sdk/installation/#installing-the-package","title":"Installing the package","text":"<p>YData SDK supports python versions bigger than python 3.8, and can be installed in Windows, Linux or MacOS operating systems.</p> <p>Prior to the package installation, it is recommended the creation of a virtual or conda environment:</p> pyenv <pre><code>pyenv virtualenv 3.10 ydatasdk\n</code></pre> <p>And install <code>ydata-sdk</code></p> pypi <pre><code>pip install ydata-sdk\n</code></pre>"},{"location":"sdk/installation/#authentication","title":"Authentication","text":"<p>Once you've installed <code>ydata-sdk</code> package you will need a token to run the functionalities. YData SDK uses a token based authentication system. To get access to your token, you need to create a YData account.</p> <p>YData SDK offers a free-trial and an enterprise version. To access your free-trial token, you need to create a YData account.</p> <p>The token will be available here, after login:</p> <p></p> <p>With your account toke copied, you can set a new environment variable <code>YDATA_TOKEN</code> in the beginning of your development session.</p> <pre><code>    import os\n\n    os.setenv['YDATA_TOKEN'] = '{add-your-token}'\n</code></pre> <p>Once you have set your token, you are good to go to start exploring the incredible world of data-centric AI and smart synthetic data generation!</p> <p>Check out our quickstart guide!</p>"},{"location":"sdk/quickstart/","title":"Quickstart","text":"<p>YData SDK allows you to with an easy and familiar interface, to adopt a Data-Centric AI approach for the development of Machine Learning solutions. YData SDK features were designed to support structure data, including tabular data, time-series and transactional data.</p>"},{"location":"sdk/quickstart/#read-data","title":"Read data","text":"<p>To start leveraging the package features you should consume your data either through the Connectors or pandas.Dataframe. The list of available connectors can be found here [add a link].</p> From pandas dataframeFrom a connector <pre><code>    # Example for a Google Cloud Storage Connector\n    credentials = \"{insert-credentials-file-path}\"\n\n    # We create a new connector for Google Cloud Storage\n    connector = Connector(connector_type='gcs', credentials=credentials)\n\n    # Create a Datasource from the connector\n    # Note that a connector can be re-used for several datasources\n    X = DataSource(connector=connector, path='gs://&lt;my_bucket&gt;.csv')\n</code></pre> <pre><code>    # Load a small dataset\n    X = pd.read_csv('{insert-file-path.csv}')\n\n    # Init a synthesizer\n    synth = RegularSynthesizer()\n\n    # Train the synthesizer with the pandas Dataframe as input\n    # The data is then sent to the cluster for processing\n    synth.fit(X)\n</code></pre> <p>The synthesis process returns a <code>pandas.DataFrame</code> object. Note that if you are using the <code>ydata-sdk</code> free version, all of your data is sent to a remote cluster on YData's infrastructure.</p>"},{"location":"sdk/quickstart/#data-synthesis-flow","title":"Data synthesis flow","text":"<p>The process of data synthesis can be described into the following steps:</p> <pre><code>stateDiagram-v2\n  state read_data\n  read_data --&gt; init_synth\n  init_synth --&gt; train_synth\n  train_synth --&gt; generate_samples\n  generate_samples --&gt; [*]</code></pre> <p>The code snippet below shows how easy can be to start generating new synthetic data. The package includes a set of examples datasets for a quickstart.</p> <pre><code>    from ydata.sdk.dataset import get_dataset\n\n    #read the example data\n    X = get_dataset('census')\n\n    # Init a synthesizer\n    synth = RegularSynthesizer()\n\n    # Fit the synthesizer to the input data\n    synth.fit(X)\n\n    # Sample new synthetic data. The below request ask for new 1000 synthetic rows\n    synth.sample(n_samples=1000)\n</code></pre> <p>Do I need to prepare my data before synthesis?</p> <p>The sdk ensures that the original behaviour is replicated. For that reason, there is no need to preprocess outlier observations or missing data.</p> <p>By default all the missing data is replicated as NaN.</p>"},{"location":"sdk/examples/synthesize_tabular_data/","title":"Synthesize tabular data","text":"<p>Use YData's RegularSynthesizer to generate tabular synthetic data</p> <pre><code>import os\n\nfrom ydata.sdk.dataset import get_dataset\nfrom ydata.sdk.synthesizers import RegularSynthesizer\n\n# Do not forget to add your token as env variables\nos.environ[\"YDATA_TOKEN\"] = '&lt;TOKEN&gt;'  # Remove if already defined\n\n\ndef main():\n    \"\"\"In this example, we demonstrate how to train a synthesizer from a pandas\n    DataFrame.\n\n    After training a Regular Synthesizer, we request a sample.\n    \"\"\"\n    X = get_dataset('census')\n\n    # We initialize a regular synthesizer\n    # As long as the synthesizer does not call `fit`, it exists only locally\n    synth = RegularSynthesizer()\n\n    # We train the synthesizer on our dataset\n    synth.fit(X)\n\n    # We request a synthetic dataset with 50 rows\n    sample = synth.sample(n_samples=50)\n\n    print(sample.shape)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"sdk/examples/synthesize_timeseries_data/","title":"Synthesize time-series data","text":"<p>Use YData's TimeSeriesSynthesizer to generate time-series synthetic data</p> <p>Tabular data is the most common type of data we encounter in data problems.</p> <p>When thinking about tabular data, we assume independence between different records, but this does not happen in reality. Suppose we check events from our day-to-day life, such as room temperature changes, bank account transactions, stock price fluctuations, and air quality measurements in our neighborhood. In that case, we might end up with datasets where measures and records evolve and are related through time. This type of data is known to be sequential or time-series data.</p> <p>Thus, sequential or time-series data refers to any data containing elements ordered into sequences in a structured format. Dissecting any time-series dataset, we see differences in variables' behavior that need to be understood for an effective generation of synthetic data. Typically any time-series dataset is composed of the following:</p> <ul> <li>Variables that define the order of time (these can be simple with one variable or composed)</li> <li>Time-variant variables</li> <li>Variables that refer to entities (single or multiple entities)</li> <li>Variables that are attributes (those that don't depend on time but rather on the entity)</li> </ul> <p>Below find an example:</p> <pre><code>import os\n\nfrom ydata.sdk.dataset import get_dataset\nfrom ydata.sdk.synthesizers import TimeSeriesSynthesizer\n\n# Do not forget to add your token as env variable\nos.environ[\"YDATA_TOKEN\"] = '&lt;TOKEN&gt;'\n\nX = get_dataset('occupancy')\n\n# We initialize a time series synthesizer\n# As long as the synthesizer does not call `fit`, it exists only locally\nsynth = TimeSeriesSynthesizer()\n\n# We train the synthesizer on our dataset\n# sortbykey -&gt; variable that define the time order for the sequence\nsynth.fit(X, sortbykey='date')\n\n# By default it is requested a synthetic sample with the same length as the original data\n# The TimeSeriesSynthesizer is designed to replicate temporal series and therefore the original time-horizon is respected\nsample = synth.sample(n_entities=1)\n</code></pre>"},{"location":"sdk/examples/synthesize_with_anonymization/","title":"Anonymization","text":"<p>YData Synthesizers offers a way to anonymize sensitive information such that the original values are not present in the synthetic data but replaced by fake values.</p> <p>Does the model retain the original values?</p> <p>No! The anonymization is performed before the model training such that it never sees the original values.</p> <p>The anonymization is performed by specifying which columns need to be anonymized and how to perform the anonymization. The anonymization rules are defined as a dictionary with the following format:</p> <p><code>{column_name: anonymization_rule}</code></p> <p>While here are some predefined anonymization rules such as <code>name</code>, <code>email</code>, <code>company</code>, it is also possible to create a rule using a regular expression. The anonymization rules have to be passed to a synthesizer in its <code>fit</code> method using the parameter <code>anonymize</code>.</p> <p>What is the difference between anonymization and privacy?</p> <p>Anonymization makes sure sensitive information are hidden from the data. Privacy makes sure it is not possible to infer the original data points from the synthetic data points via statistical attacks.</p> <p>Therefore, for data sharing anonymization and privacy controls are complementary.</p> <p>The example below demonstrates how to anonymize the column <code>Name</code> by fake names and the column <code>Ticket</code> by a regular expression: <pre><code>import os\n\nfrom ydata.sdk.dataset import get_dataset\nfrom ydata.sdk.synthesizers import RegularSynthesizer\n\n# Do not forget to add your token as env variables\nos.environ[\"YDATA_TOKEN\"] = '&lt;TOKEN&gt;'  # Remove if already defined\n\n\ndef main():\n    \"\"\"In this example, we demonstrate how to train a synthesizer from a pandas\n    DataFrame.\n\n    After training a Regular Synthesizer, we request a sample.\n    \"\"\"\n    X = get_dataset('titanic')\n\n    # We initialize a regular synthesizer\n    # As long as the synthesizer does not call `fit`, it exists only locally\n    synth = RegularSynthesizer()\n\n    # We define anonymization rules, which is a dictionary with format:\n    # {column_name: anonymization_rule, ...}\n    # while here are some predefined anonymization rules like: name, email, company\n    # it is also possible to create a rule using a regular expression\n    rules = {\n        \"Name\": \"name\",\n        \"Ticket\": \"[A-Z]{2}-[A-Z]{4}\"\n    }\n\n    # We train the synthesizer on our dataset\n    synth.fit(\n        X,\n        name=\"titanic_synthesizer\",\n        anonymize=rules\n    )\n\n    # We request a synthetic dataset with 50 rows\n    sample = synth.sample(n_samples=50)\n\n    print(sample[[\"Name\", \"Ticket\"]].head(3))\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre></p>"},{"location":"sdk/examples/synthesize_with_conditional_sampling/","title":"Conditional sampling","text":"<p>YData Synthesizers support conditional sampling. The <code>fit</code> method has an optional parameter named <code>condition_on</code>, which receives a list of features to condition upon. Furthermore, the <code>sample</code> method receives the conditions to be applied through another optional parameter also named <code>condition_on</code>. For now, two types of conditions are supported:</p> <ul> <li>Condition upon a categorical (or string) feature. The parameters are the name of the feature and a list of values (i.e., categories) to be considered. Each category also has its percentage of representativeness. For example, if we want to condition upon two categories, we need to define the percentage of rows each of these categories will have on the synthetic dataset. Naturally, the sum of such percentages needs to be 1. The default percentage is also 1 since it is the required value for a single category.</li> <li>Condition upon a numerical feature. The parameters are the name of the feature and the minimum and maximum of the range to be considered. This feature will present a uniform distribution on the synthetic dataset, limited by the specified range.</li> </ul> <p>The example below demonstrates how to train and sample from a synthesizer using conditional sampling:</p> <pre><code>import os\n\nfrom ydata.sdk.dataset import get_dataset\nfrom ydata.sdk.synthesizers import RegularSynthesizer\n\n# Do not forget to add your token as env variables.\nos.environ[\"YDATA_TOKEN\"] = '&lt;TOKEN&gt;'  # Remove if already defined.\n\n\ndef main():\n    \"\"\"In this example, we demonstrate how to train and\n    sample from a synthesizer using conditional sampling.\"\"\"\n    X = get_dataset('census')\n\n    # We initialize a regular synthesizer.\n    # As long as the synthesizer does not call `fit`, it exists only locally.\n    synth = RegularSynthesizer()\n\n    # We train the synthesizer on our dataset setting\n    # the features to condition upon.\n    synth.fit(\n        X,\n        name=\"census_synthesizer\",\n        condition_on=[\"sex\", \"native-country\", \"age\"]\n    )\n\n    # We request a synthetic dataset with specific condition rules.\n    sample = synth.sample(\n        n_samples=500,\n        condition_on={\n            \"sex\": {\n                \"categories\": [{\n                    \"category\": 'Female',\n                    \"percentage\": 0.7\n                }]\n            },\n            \"native-country\": {\n                \"categories\": [{\n                    \"category\": 'United-States',\n                    \"percentage\": 0.6\n                }, {\n                    \"category\": 'Mexico',\n                    \"percentage\": 0.4\n                }]\n            },\n            \"age\": {\n                \"minimum\": 55,\n                \"maximum\": 60\n            }\n        }\n    )\n    print(sample)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"sdk/examples/synthesize_with_privacy_control/","title":"Privacy control","text":"<p>YData Synthesizers offers 3 different levels of privacy:</p> <ol> <li>high privacy: the model is optimized for privacy purposes,</li> <li>high fidelity (default): the model is optimized for high fidelity,</li> <li>balanced: tradeoff between privacy and fidelity.</li> </ol> <p>The default privacy level is high fidelity. The privacy level can be changed by the user at the moment a synthesizer level is trained by using the parameter <code>privacy_level</code>. The parameter expect a <code>PrivacyLevel</code> value.</p> <p>What is the difference between anonymization and privacy?</p> <p>Anonymization makes sure sensitive information are hidden from the data. Privacy makes sure it is not possible to infer the original data points from the synthetic data points via statistical attacks.</p> <p>Therefore, for data sharing anonymization and privacy controls are complementary.</p> <p>The example below demonstrates how to train a synthesizer configured for high privacy:</p> <pre><code>import os\n\nfrom ydata.sdk.dataset import get_dataset\nfrom ydata.sdk.synthesizers import PrivacyLevel, RegularSynthesizer\n\n# Do not forget to add your token as env variables\nos.environ[\"YDATA_TOKEN\"] = '&lt;TOKEN&gt;'  # Remove if already defined\n\n\ndef main():\n    \"\"\"In this example, we demonstrate how to train a synthesizer\n    with a high-privacy setting from a pandas DataFrame.\n    After training a Regular Synthesizer, we request a sample.\n    \"\"\"\n    X = get_dataset('titanic')\n\n    # We initialize a regular synthesizer\n    # As long as the synthesizer does not call `fit`, it exists only locally\n    synth = RegularSynthesizer()\n\n    # We train the synthesizer on our dataset setting the privacy level to high\n    synth.fit(\n        X,\n        name=\"titanic_synthesizer\",\n        privacy_level=PrivacyLevel.HIGH_PRIVACY\n    )\n\n    # We request a synthetic dataset with 50 rows\n    sample = synth.sample(n_samples=50)\n    print(sample)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"sdk/examples/synthesizer_multitable/","title":"Synthesize Relational databases","text":"<p>Integrate Fabric's MultiTableSynthesizer in your data flows and generate synthetic relational databases or multi-table datasets</p> <p>The capability to generate synthetic data from relational databases is a powerful and innovative approach to streamline the access to data and improve data democratization strategy within the organization. Fabric's SDK makes available an easy-to-use code interface to integrate the process of generating synthetic multi-table databases into your existing data flows.</p> <p>How to get your datasource?</p> <p>Learn how to create your multi-table data in Fabric here before creating your first multi-table synthetic data generator!</p> <p>Get your datasource and connector ID</p> <p>Datasource uid: You can find your datasource ID through Fabric UI. Open your relational dataset and click in the \"Explore in Labs\" button. Copy the uid that you find available in the code snippet.</p> <p>Connector uid: You can find your connector ID through Fabric UI. Open the connector tab from your Data Catalog. Under the connector \"Actions\" select \"Explore in Lab\". Copy the uid available in the code snippet.</p> <p>Quickstart example:</p> <pre><code>import os\n\nfrom ydata.sdk.datasources import DataSource\nfrom ydata.sdk.synthesizers import MultiTableSynthesizer\n\n# Authenticate to Fabric to leverage the SDK - https://docs.sdk.ydata.ai/latest/sdk/installation/\n# Make sure to add your token as env variable.\nos.environ[\"YDATA_TOKEN\"] = '&lt;TOKEN&gt;'  # Remove if already defined\n\n# In this example, we demonstrate how to train a synthesizer from an existing RDBMS Dataset.\n# Make sure to follow the step-by-step guide to create a Dataset in Fabric's catalog: https://docs.sdk.ydata.ai/latest/get-started/create_multitable_dataset/\nX = DataSource.get('&lt;DATASOURCE_UID&gt;')\n\n# Init a multi-table synthesizer. Provide a connector so that the process of data synthesis write the\n# synthetic data into the destination database\n# Provide a connector ID as the write_connector argument. See in this tutorial how to get a connector ID\nsynth = MultiTableSynthesizer(write_connector='&lt;CONNECTOR_UID')\n\n# Start the training of your synthetic data generator\nsynth.fit(X)\n\n# As soon as the training process is completed you are able to sample a synthetic database\n# The input expected is a percentage of the original database size\n# In this case it was requested a synthetic database with the same size as the original\n# Your synthetic sample was written to the database provided in the write_connector\nsynth.sample(frac=1.)\n</code></pre>"},{"location":"sdk/modules/connectors/","title":"Connectors","text":"<p>YData SDK allows users to consume data assets from remote storages through Connectors. YData Connectors support different types of storages, from filesystems to RDBMS'.</p> <p>Below the list of available connectors:</p> Connector Name Type Supported File Types Useful Links Notes AWS S3 Remote object storage CSV, Parquet https://aws.amazon.com/s3/ Google Cloud Storage Remote object storage CSV, Parquet https://cloud.google.com/storage Azure Blob Storage Remote object storage CSV, Parquet https://azure.microsoft.com/en-us/services/storage/blobs/ File Upload Local CSV - Maximum file size is 220MB. Bigger files should be uploaded and read from remote object storages MySQL RDBMS Not applicable https://www.mysql.com/ Supports reading whole schemas or specifying a query Azure SQL Server RDBMS Not applicable https://azure.microsoft.com/en-us/services/sql-database/campaign/ Supports reading whole schemas or specifying a query PostgreSQL RDBMS Not applicable https://www.postgresql.org/ Supports reading whole schemas or specifying a query Snowflake RDBMS Not applicable https://docs.snowflake.com/en/sql-reference-commands Supports reading whole schemas or specifying a query Google BigQuery Data warehouse Not applicable https://cloud.google.com/bigquery Azure Data Lake Data lake CSV, Parquet https://azure.microsoft.com/en-us/services/storage/data-lake-storage/ <p>More details can be found at Connectors APi Reference Docs.</p>"},{"location":"sdk/modules/synthetic_data/","title":"Synthetic data generation","text":""},{"location":"sdk/modules/synthetic_data/#data-formats","title":"Data formats","text":""},{"location":"sdk/modules/synthetic_data/#tabular-data","title":"Tabular data","text":""},{"location":"sdk/modules/synthetic_data/#time-series-data","title":"Time-series data","text":""},{"location":"sdk/modules/synthetic_data/#transactions-data","title":"Transactions data","text":""},{"location":"sdk/modules/synthetic_data/#best-practices","title":"Best practices","text":""},{"location":"sdk/reference/api/common/client/","title":"Get client","text":"<p>Deduce how to initialize or retrieve the client.</p> <p>This is meant to be a zero configuration for the user.</p> Create and set a client globally <pre><code>from ydata.sdk.client import get_client\nget_client(set_as_global=True)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>client_or_creds</code> <code>Optional[Union[Client, dict, str, Path]]</code> <p>Client to forward or credentials for initialization</p> <code>None</code> <code>set_as_global</code> <code>bool</code> <p>If <code>True</code>, set client as global</p> <code>False</code> <code>wait_for_auth</code> <code>bool</code> <p>If <code>True</code>, wait for the user to authenticate</p> <code>True</code> <p>Returns:</p> Type Description <code>Client</code> <p>Client instance</p> Source code in <code>ydata/sdk/common/client/utils.py</code> <pre><code>def get_client(client_or_creds: Optional[Union[Client, Dict, str, Path]] = None, set_as_global: bool = False, wait_for_auth: bool = True) -&gt; Client:\n    \"\"\"Deduce how to initialize or retrieve the client.\n\n    This is meant to be a zero configuration for the user.\n\n    Example: Create and set a client globally\n            ```py\n            from ydata.sdk.client import get_client\n            get_client(set_as_global=True)\n            ```\n\n    Args:\n        client_or_creds (Optional[Union[Client, dict, str, Path]]): Client to forward or credentials for initialization\n        set_as_global (bool): If `True`, set client as global\n        wait_for_auth (bool): If `True`, wait for the user to authenticate\n\n    Returns:\n        Client instance\n    \"\"\"\n    client = None\n    global WAITING_FOR_CLIENT\n    try:\n\n        # If a client instance is set globally, return it\n        if not set_as_global and Client.GLOBAL_CLIENT is not None:\n            return Client.GLOBAL_CLIENT\n\n        # Client exists, forward it\n        if isinstance(client_or_creds, Client):\n            return client_or_creds\n\n        # Explicit credentials\n        ''' # For the first version, we deactivate explicit credentials via string or file for env var only\n        if isinstance(client_or_creds, (dict, str, Path)):\n            if isinstance(client_or_creds, str):  # noqa: SIM102\n                if Path(client_or_creds).is_file():\n                    client_or_creds = Path(client_or_creds)\n\n            if isinstance(client_or_creds, Path):\n                client_or_creds = json.loads(client_or_creds.open().read())\n\n            return Client(credentials=client_or_creds)\n\n        # Last try with environment variables\n        #if client_or_creds is None:\n        client = _client_from_env(wait_for_auth=wait_for_auth)\n        '''\n        credentials = environ.get(TOKEN_VAR)\n        if credentials is not None:\n            client = Client(credentials=credentials)\n\n    except ClientHandshakeError as e:\n        wait_for_auth = False  # For now deactivate wait_for_auth until the backend is ready\n        if wait_for_auth:\n            WAITING_FOR_CLIENT = True\n            start = time()\n            login_message_printed = False\n            while client is None:\n                if not login_message_printed:\n                    print(\n                        f\"The token needs to be refreshed - please validate your token by browsing at the following URL:\\n\\n\\t{e.auth_link}\")\n                    login_message_printed = True\n                with suppress(ClientCreationError):\n                    sleep(BACKOFF)\n                    client = get_client(wait_for_auth=False)\n                now = time()\n                if now - start &gt; CLIENT_INIT_TIMEOUT:\n                    WAITING_FOR_CLIENT = False\n                    break\n\n    if client is None and not WAITING_FOR_CLIENT:\n        sys.tracebacklimit = None\n        raise ClientCreationError\n    return client\n</code></pre> <p>Main Client class used to abstract the connection to the backend.</p> <p>A normal user should not have to instanciate a <code>Client</code> by itself. However, in the future it will be useful for power-users to manage projects and connections.</p> <p>Parameters:</p> Name Type Description Default <code>credentials</code> <code>Optional[dict]</code> <p>(optional) Credentials to connect</p> <code>None</code> <code>project</code> <code>Optional[Project]</code> <p>(optional) Project to connect to. If not specified, the client will connect to the default user's project.</p> <code>None</code> Source code in <code>ydata/sdk/common/client/client.py</code> <pre><code>@typechecked\nclass Client(metaclass=SingletonClient):\n    \"\"\"Main Client class used to abstract the connection to the backend.\n\n    A normal user should not have to instanciate a [`Client`][ydata.sdk.common.client.Client] by itself.\n    However, in the future it will be useful for power-users to manage projects and connections.\n\n    Args:\n        credentials (Optional[dict]): (optional) Credentials to connect\n        project (Optional[Project]): (optional) Project to connect to. If not specified, the client will connect to the default user's project.\n    \"\"\"\n\n    codes = codes\n\n    DEFAULT_PROJECT: Optional[Project] = environ.get(\"DEFAULT_PROJECT\", None)\n\n    def __init__(self, credentials: Optional[Union[str, Dict]] = None, project: Optional[Project] = None, set_as_global: bool = False):\n        self._base_url = environ.get(\"YDATA_BASE_URL\", DEFAULT_URL).removesuffix('/')\n        self._verify_ssl = bool(int(environ.get('YDATA_VERIFY_SSL', 1)))\n        self._headers = {'Authorization': credentials}\n\n        if self._verify_ssl is False:\n            self._http_client = httpClient(\n                headers=self._headers, timeout=Timeout(10, read=None), verify=self._verify_ssl)\n        else:\n            self._http_client = httpClient(\n                headers=self._headers, timeout=Timeout(10, read=None))\n\n        self._handshake()\n\n        self._default_project = project or Client.DEFAULT_PROJECT or self._get_default_project(\n            credentials)\n        if set_as_global:\n            self.__set_global()\n\n    @property\n    def project(self) -&gt; Project:\n        return Client.DEFAULT_PROJECT or self._default_project\n\n    @project.setter\n    def project(self, value: Project):\n        self._default_project = value\n\n    def post(\n        self, endpoint: str, content: Optional[RequestContent] = None, data: Optional[Dict] = None,\n        json: Optional[Dict] = None, project: Optional[Project] = None, files: Optional[Dict] = None,\n        raise_for_status: bool = True\n    ) -&gt; Response:\n        \"\"\"POST request to the backend.\n\n        Args:\n            endpoint (str): POST endpoint\n            content (Optional[RequestContent])\n            data (Optional[dict]): (optional) multipart form data\n            json (Optional[dict]): (optional) json data\n            files (Optional[dict]): (optional) files to be sent\n            raise_for_status (bool): raise an exception on error\n\n        Returns:\n            Response object\n        \"\"\"\n        url_data = self.__build_url(\n            endpoint, data=data, json=json, files=files, project=project)\n        response = self._http_client.post(**url_data)\n\n        if response.status_code != Client.codes.OK and raise_for_status:\n            self.__raise_for_status(response)\n\n        return response\n\n    def patch(\n        self, endpoint: str, content: Optional[RequestContent] = None, data: Optional[Dict] = None,\n        json: Optional[Dict] = None, project: Optional[Project] = None, files: Optional[Dict] = None,\n        raise_for_status: bool = True\n    ) -&gt; Response:\n        \"\"\"PATCH request to the backend.\n\n        Args:\n            endpoint (str): POST endpoint\n            content (Optional[RequestContent])\n            data (Optional[dict]): (optional) multipart form data\n            json (Optional[dict]): (optional) json data\n            files (Optional[dict]): (optional) files to be sent\n            raise_for_status (bool): raise an exception on error\n\n        Returns:\n            Response object\n        \"\"\"\n        url_data = self.__build_url(\n            endpoint, data=data, json=json, files=files, project=project)\n        response = self._http_client.patch(**url_data, content=content)\n\n        if response.status_code != Client.codes.OK and raise_for_status:\n            self.__raise_for_status(response)\n\n        return response\n\n    def get(\n        self, endpoint: str, params: Optional[Dict] = None, project: Optional[Project] = None,\n        cookies: Optional[Dict] = None, raise_for_status: bool = True\n    ) -&gt; Response:\n        \"\"\"GET request to the backend.\n\n        Args:\n            endpoint (str): GET endpoint\n            cookies (Optional[dict]): (optional) cookies data\n            raise_for_status (bool): raise an exception on error\n\n        Returns:\n            Response object\n        \"\"\"\n        url_data = self.__build_url(endpoint, params=params,\n                                    cookies=cookies, project=project)\n        response = self._http_client.get(**url_data)\n\n        if response.status_code != Client.codes.OK and raise_for_status:\n            self.__raise_for_status(response)\n\n        return response\n\n    def get_static_file(\n        self, endpoint: str, project: Optional[Project] = None, raise_for_status: bool = True\n    ) -&gt; Response:\n        \"\"\"Retrieve a static file from the backend.\n\n        Args:\n            endpoint (str): GET endpoint\n            raise_for_status (bool): raise an exception on error\n\n        Returns:\n            Response object\n        \"\"\"\n        from urllib.parse import urlparse\n        url_data = self.__build_url(endpoint, project=project)\n        url_parse = urlparse(self._base_url)\n        url_data['url'] = f'{url_parse.scheme}://{url_parse.netloc}/static-content{endpoint}'\n        response = self._http_client.get(**url_data)\n\n        if response.status_code != Client.codes.OK and raise_for_status:\n            self.__raise_for_status(response)\n\n        return response\n\n    def _handshake(self):\n        \"\"\"Client handshake.\n\n        It is used to determine is the client can connect with its\n        current authorization token.\n        \"\"\"\n        response = self.get('/profiles', params={}, raise_for_status=False)\n        if response.status_code == Client.codes.FOUND:\n            parser = LinkExtractor()\n            parser.feed(response.text)\n            raise ClientHandshakeError(auth_link=parser.link)\n\n    def _get_default_project(self, token: str):\n        response = self.get('/profiles/me', params={}, cookies={'access_token': token})\n        data: Dict = response.json()\n        return data['myWorkspace']\n\n    def __build_url(self, endpoint: str, params: Optional[Dict] = None, data: Optional[Dict] = None,\n                    json: Optional[Dict] = None, project: Optional[Project] = None, files: Optional[Dict] = None,\n                    cookies: Optional[Dict] = None) -&gt; Dict:\n        \"\"\"Build a request for the backend.\n\n        Args:\n            endpoint (str): backend endpoint\n            params (Optional[dict]): URL parameters\n            data (Optional[Project]): (optional) multipart form data\n            json (Optional[dict]): (optional) json data\n            files (Optional[dict]): (optional) files to be sent\n            cookies (Optional[dict]): (optional) cookies data\n\n        Returns:\n            dictionary containing the information to perform a request\n        \"\"\"\n        _params = params if params is not None else {\n            'ns': project or self._default_project\n        }\n\n        url_data = {\n            'url': f'{self._base_url}/{endpoint.removeprefix(\"/\")}',\n            'headers': self._headers,\n            'params': _params,\n        }\n\n        if data is not None:\n            url_data['data'] = data\n\n        if json is not None:\n            url_data['json'] = json\n\n        if files is not None:\n            url_data['files'] = files\n\n        if cookies is not None:\n            url_data['cookies'] = cookies\n\n        return url_data\n\n    def __set_global(self) -&gt; None:\n        \"\"\"Sets a client instance as global.\"\"\"\n        # If the client is stateful, close it gracefully!\n        Client.GLOBAL_CLIENT = self\n\n    def __raise_for_status(self, response: Response) -&gt; None:\n        \"\"\"Raise an exception if the response is not OK.\n\n        When an exception is raised, we try to convert it to a ResponseError which is\n        a wrapper around a backend error. This usually gives enough context and provides\n        nice error message.\n\n        If it cannot be converted to ResponseError, it is re-raised.\n\n        Args:\n            response (Response): response to analyze\n        \"\"\"\n        try:\n            response.raise_for_status()\n        except HTTPStatusError as e:\n            with suppress(Exception):\n                e = ResponseError(**response.json())\n            raise e\n</code></pre>"},{"location":"sdk/reference/api/common/client/#ydata.sdk.common.client.client.Client.__build_url","title":"<code>__build_url(endpoint, params=None, data=None, json=None, project=None, files=None, cookies=None)</code>","text":"<p>Build a request for the backend.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>backend endpoint</p> required <code>params</code> <code>Optional[dict]</code> <p>URL parameters</p> <code>None</code> <code>data</code> <code>Optional[Project]</code> <p>(optional) multipart form data</p> <code>None</code> <code>json</code> <code>Optional[dict]</code> <p>(optional) json data</p> <code>None</code> <code>files</code> <code>Optional[dict]</code> <p>(optional) files to be sent</p> <code>None</code> <code>cookies</code> <code>Optional[dict]</code> <p>(optional) cookies data</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict</code> <p>dictionary containing the information to perform a request</p> Source code in <code>ydata/sdk/common/client/client.py</code> <pre><code>def __build_url(self, endpoint: str, params: Optional[Dict] = None, data: Optional[Dict] = None,\n                json: Optional[Dict] = None, project: Optional[Project] = None, files: Optional[Dict] = None,\n                cookies: Optional[Dict] = None) -&gt; Dict:\n    \"\"\"Build a request for the backend.\n\n    Args:\n        endpoint (str): backend endpoint\n        params (Optional[dict]): URL parameters\n        data (Optional[Project]): (optional) multipart form data\n        json (Optional[dict]): (optional) json data\n        files (Optional[dict]): (optional) files to be sent\n        cookies (Optional[dict]): (optional) cookies data\n\n    Returns:\n        dictionary containing the information to perform a request\n    \"\"\"\n    _params = params if params is not None else {\n        'ns': project or self._default_project\n    }\n\n    url_data = {\n        'url': f'{self._base_url}/{endpoint.removeprefix(\"/\")}',\n        'headers': self._headers,\n        'params': _params,\n    }\n\n    if data is not None:\n        url_data['data'] = data\n\n    if json is not None:\n        url_data['json'] = json\n\n    if files is not None:\n        url_data['files'] = files\n\n    if cookies is not None:\n        url_data['cookies'] = cookies\n\n    return url_data\n</code></pre>"},{"location":"sdk/reference/api/common/client/#ydata.sdk.common.client.client.Client.__raise_for_status","title":"<code>__raise_for_status(response)</code>","text":"<p>Raise an exception if the response is not OK.</p> <p>When an exception is raised, we try to convert it to a ResponseError which is a wrapper around a backend error. This usually gives enough context and provides nice error message.</p> <p>If it cannot be converted to ResponseError, it is re-raised.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>Response</code> <p>response to analyze</p> required Source code in <code>ydata/sdk/common/client/client.py</code> <pre><code>def __raise_for_status(self, response: Response) -&gt; None:\n    \"\"\"Raise an exception if the response is not OK.\n\n    When an exception is raised, we try to convert it to a ResponseError which is\n    a wrapper around a backend error. This usually gives enough context and provides\n    nice error message.\n\n    If it cannot be converted to ResponseError, it is re-raised.\n\n    Args:\n        response (Response): response to analyze\n    \"\"\"\n    try:\n        response.raise_for_status()\n    except HTTPStatusError as e:\n        with suppress(Exception):\n            e = ResponseError(**response.json())\n        raise e\n</code></pre>"},{"location":"sdk/reference/api/common/client/#ydata.sdk.common.client.client.Client.__set_global","title":"<code>__set_global()</code>","text":"<p>Sets a client instance as global.</p> Source code in <code>ydata/sdk/common/client/client.py</code> <pre><code>def __set_global(self) -&gt; None:\n    \"\"\"Sets a client instance as global.\"\"\"\n    # If the client is stateful, close it gracefully!\n    Client.GLOBAL_CLIENT = self\n</code></pre>"},{"location":"sdk/reference/api/common/client/#ydata.sdk.common.client.client.Client.get","title":"<code>get(endpoint, params=None, project=None, cookies=None, raise_for_status=True)</code>","text":"<p>GET request to the backend.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>GET endpoint</p> required <code>cookies</code> <code>Optional[dict]</code> <p>(optional) cookies data</p> <code>None</code> <code>raise_for_status</code> <code>bool</code> <p>raise an exception on error</p> <code>True</code> <p>Returns:</p> Type Description <code>Response</code> <p>Response object</p> Source code in <code>ydata/sdk/common/client/client.py</code> <pre><code>def get(\n    self, endpoint: str, params: Optional[Dict] = None, project: Optional[Project] = None,\n    cookies: Optional[Dict] = None, raise_for_status: bool = True\n) -&gt; Response:\n    \"\"\"GET request to the backend.\n\n    Args:\n        endpoint (str): GET endpoint\n        cookies (Optional[dict]): (optional) cookies data\n        raise_for_status (bool): raise an exception on error\n\n    Returns:\n        Response object\n    \"\"\"\n    url_data = self.__build_url(endpoint, params=params,\n                                cookies=cookies, project=project)\n    response = self._http_client.get(**url_data)\n\n    if response.status_code != Client.codes.OK and raise_for_status:\n        self.__raise_for_status(response)\n\n    return response\n</code></pre>"},{"location":"sdk/reference/api/common/client/#ydata.sdk.common.client.client.Client.get_static_file","title":"<code>get_static_file(endpoint, project=None, raise_for_status=True)</code>","text":"<p>Retrieve a static file from the backend.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>GET endpoint</p> required <code>raise_for_status</code> <code>bool</code> <p>raise an exception on error</p> <code>True</code> <p>Returns:</p> Type Description <code>Response</code> <p>Response object</p> Source code in <code>ydata/sdk/common/client/client.py</code> <pre><code>def get_static_file(\n    self, endpoint: str, project: Optional[Project] = None, raise_for_status: bool = True\n) -&gt; Response:\n    \"\"\"Retrieve a static file from the backend.\n\n    Args:\n        endpoint (str): GET endpoint\n        raise_for_status (bool): raise an exception on error\n\n    Returns:\n        Response object\n    \"\"\"\n    from urllib.parse import urlparse\n    url_data = self.__build_url(endpoint, project=project)\n    url_parse = urlparse(self._base_url)\n    url_data['url'] = f'{url_parse.scheme}://{url_parse.netloc}/static-content{endpoint}'\n    response = self._http_client.get(**url_data)\n\n    if response.status_code != Client.codes.OK and raise_for_status:\n        self.__raise_for_status(response)\n\n    return response\n</code></pre>"},{"location":"sdk/reference/api/common/client/#ydata.sdk.common.client.client.Client.patch","title":"<code>patch(endpoint, content=None, data=None, json=None, project=None, files=None, raise_for_status=True)</code>","text":"<p>PATCH request to the backend.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>POST endpoint</p> required <code>data</code> <code>Optional[dict]</code> <p>(optional) multipart form data</p> <code>None</code> <code>json</code> <code>Optional[dict]</code> <p>(optional) json data</p> <code>None</code> <code>files</code> <code>Optional[dict]</code> <p>(optional) files to be sent</p> <code>None</code> <code>raise_for_status</code> <code>bool</code> <p>raise an exception on error</p> <code>True</code> <p>Returns:</p> Type Description <code>Response</code> <p>Response object</p> Source code in <code>ydata/sdk/common/client/client.py</code> <pre><code>def patch(\n    self, endpoint: str, content: Optional[RequestContent] = None, data: Optional[Dict] = None,\n    json: Optional[Dict] = None, project: Optional[Project] = None, files: Optional[Dict] = None,\n    raise_for_status: bool = True\n) -&gt; Response:\n    \"\"\"PATCH request to the backend.\n\n    Args:\n        endpoint (str): POST endpoint\n        content (Optional[RequestContent])\n        data (Optional[dict]): (optional) multipart form data\n        json (Optional[dict]): (optional) json data\n        files (Optional[dict]): (optional) files to be sent\n        raise_for_status (bool): raise an exception on error\n\n    Returns:\n        Response object\n    \"\"\"\n    url_data = self.__build_url(\n        endpoint, data=data, json=json, files=files, project=project)\n    response = self._http_client.patch(**url_data, content=content)\n\n    if response.status_code != Client.codes.OK and raise_for_status:\n        self.__raise_for_status(response)\n\n    return response\n</code></pre>"},{"location":"sdk/reference/api/common/client/#ydata.sdk.common.client.client.Client.post","title":"<code>post(endpoint, content=None, data=None, json=None, project=None, files=None, raise_for_status=True)</code>","text":"<p>POST request to the backend.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>POST endpoint</p> required <code>data</code> <code>Optional[dict]</code> <p>(optional) multipart form data</p> <code>None</code> <code>json</code> <code>Optional[dict]</code> <p>(optional) json data</p> <code>None</code> <code>files</code> <code>Optional[dict]</code> <p>(optional) files to be sent</p> <code>None</code> <code>raise_for_status</code> <code>bool</code> <p>raise an exception on error</p> <code>True</code> <p>Returns:</p> Type Description <code>Response</code> <p>Response object</p> Source code in <code>ydata/sdk/common/client/client.py</code> <pre><code>def post(\n    self, endpoint: str, content: Optional[RequestContent] = None, data: Optional[Dict] = None,\n    json: Optional[Dict] = None, project: Optional[Project] = None, files: Optional[Dict] = None,\n    raise_for_status: bool = True\n) -&gt; Response:\n    \"\"\"POST request to the backend.\n\n    Args:\n        endpoint (str): POST endpoint\n        content (Optional[RequestContent])\n        data (Optional[dict]): (optional) multipart form data\n        json (Optional[dict]): (optional) json data\n        files (Optional[dict]): (optional) files to be sent\n        raise_for_status (bool): raise an exception on error\n\n    Returns:\n        Response object\n    \"\"\"\n    url_data = self.__build_url(\n        endpoint, data=data, json=json, files=files, project=project)\n    response = self._http_client.post(**url_data)\n\n    if response.status_code != Client.codes.OK and raise_for_status:\n        self.__raise_for_status(response)\n\n    return response\n</code></pre>"},{"location":"sdk/reference/api/common/types/","title":"Types","text":""},{"location":"sdk/reference/api/connectors/connector/","title":"Connector","text":"<p>               Bases: <code>ModelFactoryMixin</code></p> <p>A <code>Connector</code> allows to connect and access data stored in various places. The list of available connectors can be found here.</p> <p>Parameters:</p> Name Type Description Default <code>connector_type</code> <code>Union[ConnectorType, str]</code> <p>Type of the connector to be created</p> <code>None</code> <code>credentials</code> <code>dict</code> <p>Connector credentials</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>(optional) Connector name</p> <code>None</code> <code>project</code> <code>Optional[Project]</code> <p>(optional) Project name for this Connector</p> <code>None</code> <code>client</code> <code>Client</code> <p>(optional) Client to connect to the backend</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>uid</code> <code>UID</code> <p>UID fo the connector instance (creating internally)</p> <code>type</code> <code>ConnectorType</code> <p>Type of the connector</p> Source code in <code>ydata/sdk/connectors/connector.py</code> <pre><code>class Connector(ModelFactoryMixin):\n    \"\"\"A [`Connector`][ydata.sdk.connectors.Connector] allows to connect and\n    access data stored in various places. The list of available connectors can\n    be found [here][ydata.sdk.connectors.ConnectorType].\n\n    Arguments:\n        connector_type (Union[ConnectorType, str]): Type of the connector to be created\n        credentials (dict): Connector credentials\n        name (Optional[str]): (optional) Connector name\n        project (Optional[Project]): (optional) Project name for this Connector\n        client (Client): (optional) Client to connect to the backend\n\n    Attributes:\n        uid (UID): UID fo the connector instance (creating internally)\n        type (ConnectorType): Type of the connector\n    \"\"\"\n\n    _MODEL_CLASS = mConnector\n\n    _model: Optional[mConnector]\n\n    def __init__(\n            self, connector_type: Union[ConnectorType, str, None] = None, credentials: Optional[Dict] = None,\n            name: Optional[str] = None, project: Optional[Project] = None, client: Optional[Client] = None):\n        self._init_common(client=client)\n        self._model = _connector_type_to_model(ConnectorType._init_connector_type(connector_type))._create_model(\n            connector_type, credentials, name, client=client)\n\n        self._project = project\n\n    @init_client\n    def _init_common(self, client: Optional[Client] = None):\n        self._client = client\n        self._logger = create_logger(__name__, level=LOG_LEVEL)\n\n    @property\n    def uid(self) -&gt; UID:\n        return self._model.uid\n\n    @property\n    def name(self) -&gt; str:\n        return self._model.name\n\n    @property\n    def type(self) -&gt; ConnectorType:\n        return ConnectorType(self._model.type)\n\n    @property\n    def project(self) -&gt; Project:\n        return self._project or self._client.project\n\n    @staticmethod\n    @init_client\n    def get(\n        uid: UID, project: Optional[Project] = None, client: Optional[Client] = None\n    ) -&gt; _T:\n        \"\"\"Get an existing connector.\n\n        Arguments:\n            uid (UID): Connector identifier\n            project (Optional[Project]): (optional) Project name from where to get the connector\n            client (Optional[Client]): (optional) Client to connect to the backend\n\n        Returns:\n            Connector\n        \"\"\"\n        response = client.get(f'/connector/{uid}', project=project)\n        data = response.json()\n        data_type = data[\"type\"]\n        connector_class = _connector_type_to_model(\n            ConnectorType._init_connector_type(data_type))\n        connector = connector_class._init_from_model_data(\n            connector_class._MODEL_CLASS(**data))\n        connector._project = project\n\n        return connector\n\n    @staticmethod\n    def _init_credentials(\n        connector_type: ConnectorType, credentials: Union[str, Path, Dict, Credentials]\n    ) -&gt; Credentials:\n        _credentials = None\n\n        if isinstance(credentials, str):\n            credentials = Path(credentials)\n\n        if isinstance(credentials, Path):\n            try:\n                _credentials = json_loads(credentials.open().read())\n            except Exception:\n                raise CredentialTypeError(\n                    'Could not read the credentials. Please, check your path or credentials structure.')\n\n        try:\n            from ydata.sdk.connectors._models.connector_map import TYPE_TO_CLASS\n            credential_cls = TYPE_TO_CLASS.get(connector_type.value)\n            _credentials = credential_cls(**_credentials)\n        except Exception:\n            raise CredentialTypeError(\n                \"Could not create the credentials. Verify the path or the structure your credentials.\")\n\n        return _credentials\n\n    @staticmethod\n    def create(\n        connector_type: Union[ConnectorType, str], credentials: Union[str, Path, Dict, Credentials],\n        name: Optional[str] = None, project: Optional[Project] = None, client: Optional[Client] = None\n    ) -&gt; _T:\n        \"\"\"Create a new connector.\n\n        Arguments:\n            connector_type (Union[ConnectorType, str]): Type of the connector to be created\n            credentials (dict): Connector credentials\n            name (Optional[str]): (optional) Connector name\n            project (Optional[Project]): (optional) Project where to create the connector\n            client (Client): (optional) Client to connect to the backend\n\n        Returns:\n            New connector\n        \"\"\"\n        connector_type = ConnectorType._init_connector_type(connector_type)\n        connector_class = _connector_type_to_model(connector_type)\n\n        payload = {\n            \"type\": connector_type.value,\n            \"credentials\": credentials.dict(by_alias=True)\n        }\n        model = connector_class._create(payload, name, project, client)\n\n        connector = connector_class._init_from_model_data(model)\n        connector._project = project\n        return connector\n\n    @classmethod\n    @init_client\n    def _create(\n        cls, payload: dict, name: Optional[str] = None, project: Optional[Project] = None,\n        client: Optional[Client] = None\n    ) -&gt; _MODEL_CLASS:\n        _name = name if name is not None else str(uuid4())\n        payload[\"name\"] = _name\n        response = client.post('/connector/', project=project, json=payload)\n        data = response.json()\n\n        return cls._MODEL_CLASS(**data)\n\n    @staticmethod\n    @init_client\n    def list(project: Optional[Project] = None, client: Optional[Client] = None) -&gt; ConnectorsList:\n        \"\"\"List the connectors instances.\n\n        Arguments:\n            project (Optional[Project]): (optional) Project name from where to list the connectors\n            client (Client): (optional) Client to connect to the backend\n\n        Returns:\n            List of connectors\n        \"\"\"\n        response = client.get('/connector', project=project)\n        data: list = response.json()\n        return ConnectorsList(data)\n\n    def __repr__(self):\n        return self._model.__repr__()\n</code></pre>"},{"location":"sdk/reference/api/connectors/connector/#ydata.sdk.connectors.connector.Connector.create","title":"<code>create(connector_type, credentials, name=None, project=None, client=None)</code>  <code>staticmethod</code>","text":"<p>Create a new connector.</p> <p>Parameters:</p> Name Type Description Default <code>connector_type</code> <code>Union[ConnectorType, str]</code> <p>Type of the connector to be created</p> required <code>credentials</code> <code>dict</code> <p>Connector credentials</p> required <code>name</code> <code>Optional[str]</code> <p>(optional) Connector name</p> <code>None</code> <code>project</code> <code>Optional[Project]</code> <p>(optional) Project where to create the connector</p> <code>None</code> <code>client</code> <code>Client</code> <p>(optional) Client to connect to the backend</p> <code>None</code> <p>Returns:</p> Type Description <code>_T</code> <p>New connector</p> Source code in <code>ydata/sdk/connectors/connector.py</code> <pre><code>@staticmethod\ndef create(\n    connector_type: Union[ConnectorType, str], credentials: Union[str, Path, Dict, Credentials],\n    name: Optional[str] = None, project: Optional[Project] = None, client: Optional[Client] = None\n) -&gt; _T:\n    \"\"\"Create a new connector.\n\n    Arguments:\n        connector_type (Union[ConnectorType, str]): Type of the connector to be created\n        credentials (dict): Connector credentials\n        name (Optional[str]): (optional) Connector name\n        project (Optional[Project]): (optional) Project where to create the connector\n        client (Client): (optional) Client to connect to the backend\n\n    Returns:\n        New connector\n    \"\"\"\n    connector_type = ConnectorType._init_connector_type(connector_type)\n    connector_class = _connector_type_to_model(connector_type)\n\n    payload = {\n        \"type\": connector_type.value,\n        \"credentials\": credentials.dict(by_alias=True)\n    }\n    model = connector_class._create(payload, name, project, client)\n\n    connector = connector_class._init_from_model_data(model)\n    connector._project = project\n    return connector\n</code></pre>"},{"location":"sdk/reference/api/connectors/connector/#ydata.sdk.connectors.connector.Connector.get","title":"<code>get(uid, project=None, client=None)</code>  <code>staticmethod</code>","text":"<p>Get an existing connector.</p> <p>Parameters:</p> Name Type Description Default <code>uid</code> <code>UID</code> <p>Connector identifier</p> required <code>project</code> <code>Optional[Project]</code> <p>(optional) Project name from where to get the connector</p> <code>None</code> <code>client</code> <code>Optional[Client]</code> <p>(optional) Client to connect to the backend</p> <code>None</code> <p>Returns:</p> Type Description <code>_T</code> <p>Connector</p> Source code in <code>ydata/sdk/connectors/connector.py</code> <pre><code>@staticmethod\n@init_client\ndef get(\n    uid: UID, project: Optional[Project] = None, client: Optional[Client] = None\n) -&gt; _T:\n    \"\"\"Get an existing connector.\n\n    Arguments:\n        uid (UID): Connector identifier\n        project (Optional[Project]): (optional) Project name from where to get the connector\n        client (Optional[Client]): (optional) Client to connect to the backend\n\n    Returns:\n        Connector\n    \"\"\"\n    response = client.get(f'/connector/{uid}', project=project)\n    data = response.json()\n    data_type = data[\"type\"]\n    connector_class = _connector_type_to_model(\n        ConnectorType._init_connector_type(data_type))\n    connector = connector_class._init_from_model_data(\n        connector_class._MODEL_CLASS(**data))\n    connector._project = project\n\n    return connector\n</code></pre>"},{"location":"sdk/reference/api/connectors/connector/#ydata.sdk.connectors.connector.Connector.list","title":"<code>list(project=None, client=None)</code>  <code>staticmethod</code>","text":"<p>List the connectors instances.</p> <p>Parameters:</p> Name Type Description Default <code>project</code> <code>Optional[Project]</code> <p>(optional) Project name from where to list the connectors</p> <code>None</code> <code>client</code> <code>Client</code> <p>(optional) Client to connect to the backend</p> <code>None</code> <p>Returns:</p> Type Description <code>ConnectorsList</code> <p>List of connectors</p> Source code in <code>ydata/sdk/connectors/connector.py</code> <pre><code>@staticmethod\n@init_client\ndef list(project: Optional[Project] = None, client: Optional[Client] = None) -&gt; ConnectorsList:\n    \"\"\"List the connectors instances.\n\n    Arguments:\n        project (Optional[Project]): (optional) Project name from where to list the connectors\n        client (Client): (optional) Client to connect to the backend\n\n    Returns:\n        List of connectors\n    \"\"\"\n    response = client.get('/connector', project=project)\n    data: list = response.json()\n    return ConnectorsList(data)\n</code></pre>"},{"location":"sdk/reference/api/connectors/connector/#connectortype","title":"ConnectorType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p>"},{"location":"sdk/reference/api/connectors/connector/#ydata.sdk.connectors.ConnectorType.AWS_S3","title":"<code>AWS_S3 = 'aws-s3'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>AWS S3 connector</p>"},{"location":"sdk/reference/api/connectors/connector/#ydata.sdk.connectors.ConnectorType.AZURE_BLOB","title":"<code>AZURE_BLOB = 'azure-blob'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Azure Blob connector</p>"},{"location":"sdk/reference/api/connectors/connector/#ydata.sdk.connectors.ConnectorType.AZURE_SQL","title":"<code>AZURE_SQL = 'azure-sql'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>AzureSQL connector</p>"},{"location":"sdk/reference/api/connectors/connector/#ydata.sdk.connectors.ConnectorType.BIGQUERY","title":"<code>BIGQUERY = 'google-bigquery'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>BigQuery connector</p>"},{"location":"sdk/reference/api/connectors/connector/#ydata.sdk.connectors.ConnectorType.FILE","title":"<code>FILE = 'file'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>File connector (placeholder)</p>"},{"location":"sdk/reference/api/connectors/connector/#ydata.sdk.connectors.ConnectorType.GCS","title":"<code>GCS = 'gcs'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Google Cloud Storage connector</p>"},{"location":"sdk/reference/api/connectors/connector/#ydata.sdk.connectors.ConnectorType.MYSQL","title":"<code>MYSQL = 'mysql'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>MySQL connector</p>"},{"location":"sdk/reference/api/connectors/connector/#ydata.sdk.connectors.ConnectorType.SNOWFLAKE","title":"<code>SNOWFLAKE = 'snowflake'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Snowflake connector</p>"},{"location":"sdk/reference/api/datasources/datasource/","title":"DataSource","text":"<p>               Bases: <code>ModelFactoryMixin</code></p> <p>A <code>DataSource</code> represents a dataset to be used by a Synthesizer as training data.</p> <p>Parameters:</p> Name Type Description Default <code>connector</code> <code>Connector</code> <p>Connector from which the datasource is created</p> required <code>datatype</code> <code>Optional[Union[DataSourceType, str]]</code> <p>(optional) DataSource type</p> <code>TABULAR</code> <code>name</code> <code>Optional[str]</code> <p>(optional) DataSource name</p> <code>None</code> <code>project</code> <code>Optional[Project]</code> <p>(optional) Project name for this datasource</p> <code>None</code> <code>wait_for_metadata</code> <code>bool</code> <p>If <code>True</code>, wait until the metadata is fully calculated</p> <code>True</code> <code>client</code> <code>Client</code> <p>(optional) Client to connect to the backend</p> <code>None</code> <code>**config</code> <p>Datasource specific configuration</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>uid</code> <code>UID</code> <p>UID fo the datasource instance</p> <code>datatype</code> <code>DataSourceType</code> <p>Data source type</p> <code>status</code> <code>Status</code> <p>Status of the datasource</p> <code>metadata</code> <code>Metadata</code> <p>Metadata associated to the datasource</p> Source code in <code>ydata/sdk/datasources/datasource.py</code> <pre><code>class DataSource(ModelFactoryMixin):\n    \"\"\"A [`DataSource`][ydata.sdk.datasources.DataSource] represents a dataset\n    to be used by a Synthesizer as training data.\n\n    Arguments:\n        connector (Connector): Connector from which the datasource is created\n        datatype (Optional[Union[DataSourceType, str]]): (optional) DataSource type\n        name (Optional[str]): (optional) DataSource name\n        project (Optional[Project]): (optional) Project name for this datasource\n        wait_for_metadata (bool): If `True`, wait until the metadata is fully calculated\n        client (Client): (optional) Client to connect to the backend\n        **config: Datasource specific configuration\n\n    Attributes:\n        uid (UID): UID fo the datasource instance\n        datatype (DataSourceType): Data source type\n        status (Status): Status of the datasource\n        metadata (Metadata): Metadata associated to the datasource\n    \"\"\"\n\n    def __init__(\n        self, connector: Connector, datatype: Optional[Union[DataSourceType, str]] = DataSourceType.TABULAR,\n        name: Optional[str] = None, project: Optional[Project] = None, wait_for_metadata: bool = True,\n        client: Optional[Client] = None, **config\n    ):\n        datasource_type = CONNECTOR_TO_DATASOURCE.get(connector.type)\n        self._init_common(client=client)\n        self._model: Optional[mDataSource] = self._create_model(\n            connector=connector, datasource_type=datasource_type, datatype=datatype,\n            config=config, name=name, client=self._client)\n\n        if wait_for_metadata:\n            self._model = DataSource._wait_for_metadata(self)._model\n\n        self._project = project\n\n    @init_client\n    def _init_common(self, client: Optional[Client] = None):\n        self._client = client\n        self._logger = create_logger(__name__, level=LOG_LEVEL)\n\n    @property\n    def uid(self) -&gt; UID:\n        return self._model.uid\n\n    @property\n    def datatype(self) -&gt; DataSourceType:\n        return self._model.datatype\n\n    @property\n    def project(self) -&gt; Project:\n        return self._project or self._client.project\n\n    @property\n    def status(self) -&gt; Status:\n        try:\n            self._model = self.get(uid=self._model.uid,\n                                   project=self.project, client=self._client)._model\n            return self._model.status\n        except Exception:  # noqa: PIE786\n            return Status.unknown()\n\n    @property\n    def metadata(self) -&gt; Optional[Metadata]:\n        return self._model.metadata\n\n    @staticmethod\n    @init_client\n    def list(project: Optional[Project] = None, client: Optional[Client] = None) -&gt; DataSourceList:\n        \"\"\"List the  [`DataSource`][ydata.sdk.datasources.DataSource]\n        instances.\n\n        Arguments:\n            project (Optional[Project]): (optional) Project name from where to list the datasources\n            client (Client): (optional) Client to connect to the backend\n\n        Returns:\n            List of datasources\n        \"\"\"\n        def __process_data(data: list) -&gt; list:\n            to_del = ['metadata']\n            for e in data:\n                for k in to_del:\n                    e.pop(k, None)\n            return data\n\n        response = client.get('/datasource', project=project)\n        data: list = response.json()\n        data = __process_data(data)\n\n        return DataSourceList(data)\n\n    @staticmethod\n    @init_client\n    def get(uid: UID, project: Optional[Project] = None, client: Optional[Client] = None) -&gt; \"DataSource\":\n        \"\"\"Get an existing [`DataSource`][ydata.sdk.datasources.DataSource].\n\n        Arguments:\n            uid (UID): DataSource identifier\n            project (Optional[Project]): (optional) Project name from where to get the connector\n            client (Client): (optional) Client to connect to the backend\n\n        Returns:\n            DataSource\n        \"\"\"\n        response = client.get(f'/datasource/{uid}', project=project)\n        data: list = response.json()\n        datasource_type = CONNECTOR_TO_DATASOURCE.get(\n            ConnectorType(data['connector']['type']))\n        model = DataSource._model_from_api(data, datasource_type)\n        datasource = DataSource._init_from_model_data(model)\n        datasource._project = project\n        return datasource\n\n    @classmethod\n    def create(\n        cls, connector: Connector, datatype: Optional[Union[DataSourceType, str]] = DataSourceType.TABULAR,\n        name: Optional[str] = None, project: Optional[Project] = None, wait_for_metadata: bool = True,\n        client: Optional[Client] = None, **config\n    ) -&gt; \"DataSource\":\n        \"\"\"Create a new [`DataSource`][ydata.sdk.datasources.DataSource].\n\n        Arguments:\n            connector (Connector): Connector from which the datasource is created\n            datatype (Optional[Union[DataSourceType, str]]): (optional) DataSource type\n            name (Optional[str]): (optional) DataSource name\n            project (Optional[Project]): (optional) Project name for this datasource\n            wait_for_metadata (bool): If `True`, wait until the metadata is fully calculated\n            client (Client): (optional) Client to connect to the backend\n            **config: Datasource specific configuration\n\n        Returns:\n            DataSource\n        \"\"\"\n        datasource_type = CONNECTOR_TO_DATASOURCE.get(connector.type)\n        return cls._create(\n            connector=connector, datasource_type=datasource_type, datatype=datatype, config=config, name=name,\n            project=project, wait_for_metadata=wait_for_metadata, client=client)\n\n    @classmethod\n    def _create(\n        cls, connector: Connector, datasource_type: Type[mDataSource],\n        datatype: Optional[Union[DataSourceType, str]] = DataSourceType.TABULAR, config: Optional[Dict] = None,\n        name: Optional[str] = None, project: Optional[Project] = None, wait_for_metadata: bool = True,\n        client: Optional[Client] = None\n    ) -&gt; \"DataSource\":\n        model = DataSource._create_model(\n            connector, datasource_type, datatype, config, name, project, client)\n        datasource = DataSource._init_from_model_data(model)\n\n        if wait_for_metadata:\n            datasource._model = DataSource._wait_for_metadata(datasource)._model\n\n        datasource._project = project\n\n        return datasource\n\n    @classmethod\n    @init_client\n    def _create_model(\n        cls, connector: Connector, datasource_type: Type[mDataSource],\n        datatype: Optional[Union[DataSourceType, str]] = DataSourceType.TABULAR, config: Optional[Dict] = None,\n        name: Optional[str] = None, project: Optional[Project] = None, client: Optional[Client] = None\n    ) -&gt; mDataSource:\n        _name = name if name is not None else str(uuid4())\n        _config = config if config is not None else {}\n        payload = {\n            \"name\": _name,\n            \"connector\": {\n                \"uid\": connector.uid,\n                \"type\": ConnectorType(connector.type).value\n            },\n            \"dataType\": DataSourceType(datatype).value\n        }\n        if connector.type != ConnectorType.FILE:\n            _config = datasource_type(**config).to_payload()\n        payload.update(_config)\n        response = client.post('/datasource/', project=project, json=payload)\n        data: list = response.json()\n        return DataSource._model_from_api(data, datasource_type)\n\n    @staticmethod\n    def _wait_for_metadata(datasource):\n        logger = create_logger(__name__, level=LOG_LEVEL)\n        while State(datasource.status.state) not in [State.AVAILABLE, State.FAILED, State.UNAVAILABLE]:\n            logger.info(f'Calculating metadata [{datasource.status}]')\n            datasource = DataSource.get(uid=datasource.uid, client=datasource._client)\n            sleep(BACKOFF)\n        return datasource\n\n    @staticmethod\n    def _model_from_api(data: Dict, datasource_type: Type[mDataSource]) -&gt; mDataSource:\n        data['datatype'] = data.pop('dataType', None)\n        data = filter_dict(datasource_type, data)\n        model = datasource_type(**data)\n        return model\n\n    def __repr__(self):\n        return self._model.__repr__()\n</code></pre>"},{"location":"sdk/reference/api/datasources/datasource/#ydata.sdk.datasources.datasource.DataSource.create","title":"<code>create(connector, datatype=DataSourceType.TABULAR, name=None, project=None, wait_for_metadata=True, client=None, **config)</code>  <code>classmethod</code>","text":"<p>Create a new <code>DataSource</code>.</p> <p>Parameters:</p> Name Type Description Default <code>connector</code> <code>Connector</code> <p>Connector from which the datasource is created</p> required <code>datatype</code> <code>Optional[Union[DataSourceType, str]]</code> <p>(optional) DataSource type</p> <code>TABULAR</code> <code>name</code> <code>Optional[str]</code> <p>(optional) DataSource name</p> <code>None</code> <code>project</code> <code>Optional[Project]</code> <p>(optional) Project name for this datasource</p> <code>None</code> <code>wait_for_metadata</code> <code>bool</code> <p>If <code>True</code>, wait until the metadata is fully calculated</p> <code>True</code> <code>client</code> <code>Client</code> <p>(optional) Client to connect to the backend</p> <code>None</code> <code>**config</code> <p>Datasource specific configuration</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataSource</code> <p>DataSource</p> Source code in <code>ydata/sdk/datasources/datasource.py</code> <pre><code>@classmethod\ndef create(\n    cls, connector: Connector, datatype: Optional[Union[DataSourceType, str]] = DataSourceType.TABULAR,\n    name: Optional[str] = None, project: Optional[Project] = None, wait_for_metadata: bool = True,\n    client: Optional[Client] = None, **config\n) -&gt; \"DataSource\":\n    \"\"\"Create a new [`DataSource`][ydata.sdk.datasources.DataSource].\n\n    Arguments:\n        connector (Connector): Connector from which the datasource is created\n        datatype (Optional[Union[DataSourceType, str]]): (optional) DataSource type\n        name (Optional[str]): (optional) DataSource name\n        project (Optional[Project]): (optional) Project name for this datasource\n        wait_for_metadata (bool): If `True`, wait until the metadata is fully calculated\n        client (Client): (optional) Client to connect to the backend\n        **config: Datasource specific configuration\n\n    Returns:\n        DataSource\n    \"\"\"\n    datasource_type = CONNECTOR_TO_DATASOURCE.get(connector.type)\n    return cls._create(\n        connector=connector, datasource_type=datasource_type, datatype=datatype, config=config, name=name,\n        project=project, wait_for_metadata=wait_for_metadata, client=client)\n</code></pre>"},{"location":"sdk/reference/api/datasources/datasource/#ydata.sdk.datasources.datasource.DataSource.get","title":"<code>get(uid, project=None, client=None)</code>  <code>staticmethod</code>","text":"<p>Get an existing <code>DataSource</code>.</p> <p>Parameters:</p> Name Type Description Default <code>uid</code> <code>UID</code> <p>DataSource identifier</p> required <code>project</code> <code>Optional[Project]</code> <p>(optional) Project name from where to get the connector</p> <code>None</code> <code>client</code> <code>Client</code> <p>(optional) Client to connect to the backend</p> <code>None</code> <p>Returns:</p> Type Description <code>DataSource</code> <p>DataSource</p> Source code in <code>ydata/sdk/datasources/datasource.py</code> <pre><code>@staticmethod\n@init_client\ndef get(uid: UID, project: Optional[Project] = None, client: Optional[Client] = None) -&gt; \"DataSource\":\n    \"\"\"Get an existing [`DataSource`][ydata.sdk.datasources.DataSource].\n\n    Arguments:\n        uid (UID): DataSource identifier\n        project (Optional[Project]): (optional) Project name from where to get the connector\n        client (Client): (optional) Client to connect to the backend\n\n    Returns:\n        DataSource\n    \"\"\"\n    response = client.get(f'/datasource/{uid}', project=project)\n    data: list = response.json()\n    datasource_type = CONNECTOR_TO_DATASOURCE.get(\n        ConnectorType(data['connector']['type']))\n    model = DataSource._model_from_api(data, datasource_type)\n    datasource = DataSource._init_from_model_data(model)\n    datasource._project = project\n    return datasource\n</code></pre>"},{"location":"sdk/reference/api/datasources/datasource/#ydata.sdk.datasources.datasource.DataSource.list","title":"<code>list(project=None, client=None)</code>  <code>staticmethod</code>","text":"<p>List the  <code>DataSource</code> instances.</p> <p>Parameters:</p> Name Type Description Default <code>project</code> <code>Optional[Project]</code> <p>(optional) Project name from where to list the datasources</p> <code>None</code> <code>client</code> <code>Client</code> <p>(optional) Client to connect to the backend</p> <code>None</code> <p>Returns:</p> Type Description <code>DataSourceList</code> <p>List of datasources</p> Source code in <code>ydata/sdk/datasources/datasource.py</code> <pre><code>@staticmethod\n@init_client\ndef list(project: Optional[Project] = None, client: Optional[Client] = None) -&gt; DataSourceList:\n    \"\"\"List the  [`DataSource`][ydata.sdk.datasources.DataSource]\n    instances.\n\n    Arguments:\n        project (Optional[Project]): (optional) Project name from where to list the datasources\n        client (Client): (optional) Client to connect to the backend\n\n    Returns:\n        List of datasources\n    \"\"\"\n    def __process_data(data: list) -&gt; list:\n        to_del = ['metadata']\n        for e in data:\n            for k in to_del:\n                e.pop(k, None)\n        return data\n\n    response = client.get('/datasource', project=project)\n    data: list = response.json()\n    data = __process_data(data)\n\n    return DataSourceList(data)\n</code></pre>"},{"location":"sdk/reference/api/datasources/datasource/#status","title":"Status","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"sdk/reference/api/datasources/datasource/#datasourcetype","title":"DataSourceType","text":"<p>               Bases: <code>StringEnum</code></p>"},{"location":"sdk/reference/api/datasources/datasource/#ydata.sdk.datasources.DataSourceType.MULTITABLE","title":"<code>MULTITABLE = 'multiTable'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The <code>DataSource</code> is a multi table RDBMS.</p>"},{"location":"sdk/reference/api/datasources/datasource/#ydata.sdk.datasources.DataSourceType.TABULAR","title":"<code>TABULAR = 'tabular'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The <code>DataSource</code> is tabular (i.e. it does not have a temporal dimension).</p>"},{"location":"sdk/reference/api/datasources/datasource/#ydata.sdk.datasources.DataSourceType.TIMESERIES","title":"<code>TIMESERIES = 'timeseries'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The <code>DataSource</code> has a temporal dimension.</p>"},{"location":"sdk/reference/api/datasources/metadata/","title":"Metadata","text":"<p>               Bases: <code>BaseModel</code></p> <p>The Metadata object contains descriptive information about a.</p> <p><code>DataSource</code></p> <p>Attributes:</p> Name Type Description <code>columns</code> <code>List[Column]</code> <p>columns information</p>"},{"location":"sdk/reference/api/synthesizers/base/","title":"Synthesizer","text":"<p>               Bases: <code>ABC</code>, <code>ModelFactoryMixin</code></p> <p>Main synthesizer class.</p> <p>This class cannot be directly instanciated because of the specificities between <code>RegularSynthesizer</code>, <code>TimeSeriesSynthesizer</code> or <code>MultiTableSynthesizer</code> <code>sample</code> methods.</p>"},{"location":"sdk/reference/api/synthesizers/base/#ydata.sdk.synthesizers.synthesizer.BaseSynthesizer--methods","title":"Methods","text":"<ul> <li><code>fit</code>: train a synthesizer instance.</li> <li><code>sample</code>: request synthetic data.</li> <li><code>status</code>: current status of the synthesizer instance.</li> </ul> Note <p>The synthesizer instance is created in the backend only when the <code>fit</code> method is called.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>(optional) Client to connect to the backend</p> <code>None</code> Source code in <code>ydata/sdk/synthesizers/synthesizer.py</code> <pre><code>@typechecked\nclass BaseSynthesizer(ABC, ModelFactoryMixin):\n    \"\"\"Main synthesizer class.\n\n    This class cannot be directly instanciated because of the specificities between [`RegularSynthesizer`][ydata.sdk.synthesizers.RegularSynthesizer], [`TimeSeriesSynthesizer`][ydata.sdk.synthesizers.TimeSeriesSynthesizer] or [`MultiTableSynthesizer`][ydata.sdk.synthesizers.MultiTableSynthesizer] `sample` methods.\n\n    Methods\n    -------\n    - `fit`: train a synthesizer instance.\n    - `sample`: request synthetic data.\n    - `status`: current status of the synthesizer instance.\n\n    Note:\n            The synthesizer instance is created in the backend only when the `fit` method is called.\n\n    Arguments:\n        client (Client): (optional) Client to connect to the backend\n    \"\"\"\n\n    def __init__(\n            self, uid: Optional[UID] = None, name: Optional[str] = None,\n            project: Optional[Project] = None, client: Optional[Client] = None):\n        self._init_common(client=client)\n        self._model = mSynthesizer(uid=uid, name=name or str(uuid4()))\n        self._project = project\n\n    @init_client\n    def _init_common(self, client: Optional[Client] = None):\n        self._client = client\n        self._logger = create_logger(__name__, level=LOG_LEVEL)\n\n    @property\n    def project(self) -&gt; Project:\n        return self._project or self._client.project\n\n    def fit(self, X: Union[DataSource, pdDataFrame],\n            privacy_level: PrivacyLevel = PrivacyLevel.HIGH_FIDELITY,\n            datatype: Optional[Union[DataSourceType, str]] = None,\n            sortbykey: Optional[Union[str, List[str]]] = None,\n            entities: Optional[Union[str, List[str]]] = None,\n            generate_cols: Optional[List[str]] = None,\n            exclude_cols: Optional[List[str]] = None,\n            dtypes: Optional[Dict[str, Union[str, DataType]]] = None,\n            target: Optional[str] = None,\n            anonymize: Optional[dict] = None,\n            condition_on: Optional[List[str]] = None) -&gt; None:\n        \"\"\"Fit the synthesizer.\n\n        The synthesizer accepts as training dataset either a pandas [`DataFrame`][pandas.DataFrame] directly or a YData [`DataSource`][ydata.sdk.datasources.DataSource].\n        When the training dataset is a pandas [`DataFrame`][pandas.DataFrame], the argument `datatype` is required as it cannot be deduced.\n\n        The argument`sortbykey` is mandatory for [`TimeSeries`][ydata.sdk.datasources.DataSourceType.TIMESERIES].\n\n        By default, if `generate_cols` or `exclude_cols` are not specified, all columns are generated by the synthesizer.\n        The argument `exclude_cols` has precedence over `generate_cols`, i.e. a column `col` will not be generated if it is in both list.\n\n        Arguments:\n            X (Union[DataSource, pandas.DataFrame]): Training dataset\n            privacy_level (PrivacyLevel): Synthesizer privacy level (defaults to high fidelity)\n            datatype (Optional[Union[DataSourceType, str]]): (optional) Dataset datatype - required if `X` is a [`pandas.DataFrame`][pandas.DataFrame]\n            sortbykey (Union[str, List[str]]): (optional) column(s) to use to sort timeseries datasets\n            entities (Union[str, List[str]]): (optional) columns representing entities ID\n            generate_cols (List[str]): (optional) columns that should be synthesized\n            exclude_cols (List[str]): (optional) columns that should not be synthesized\n            dtypes (Dict[str, Union[str, DataType]]): (optional) datatype mapping that will overwrite the datasource metadata column datatypes\n            target (Optional[str]): (optional) Target for the dataset\n            name (Optional[str]): (optional) Synthesizer instance name\n            anonymize (Optional[str]): (optional) fields to anonymize and the anonymization strategy\n            condition_on: (Optional[List[str]]): (optional) list of features to condition upon\n        \"\"\"\n        if self._already_fitted():\n            raise AlreadyFittedError()\n\n        datatype = DataSourceType(datatype)\n\n        dataset_attrs = self._init_datasource_attributes(\n            sortbykey, entities, generate_cols, exclude_cols, dtypes)\n        self._validate_datasource_attributes(X, dataset_attrs, datatype, target)\n\n        # If the training data is a pandas dataframe, we first need to create a data source and then the instance\n        if isinstance(X, pdDataFrame):\n            if X.empty:\n                raise EmptyDataError(\"The DataFrame is empty\")\n            self._logger.info('creating local connector with pandas dataframe')\n            connector = LocalConnector.create(\n                source=X, project=self._project, client=self._client)\n            self._logger.info(\n                f'created local connector. creating datasource with {connector}')\n            _X = LocalDataSource(connector=connector, project=self._project,\n                                 datatype=datatype, client=self._client)\n            self._logger.info(f'created datasource {_X}')\n        else:\n            _X = X\n\n        if dsState(_X.status.state) != dsState.AVAILABLE:\n            raise DataSourceNotAvailableError(\n                f\"The datasource '{_X.uid}' is not available (status = {_X.status})\")\n\n        if isinstance(dataset_attrs, dict):\n            dataset_attrs = DataSourceAttrs(**dataset_attrs)\n\n        self._fit_from_datasource(\n            X=_X, datatype=datatype, dataset_attrs=dataset_attrs, target=target,\n            anonymize=anonymize, privacy_level=privacy_level, condition_on=condition_on)\n\n    @staticmethod\n    def _init_datasource_attributes(\n            sortbykey: Optional[Union[str, List[str]]],\n            entities: Optional[Union[str, List[str]]],\n            generate_cols: Optional[List[str]],\n            exclude_cols: Optional[List[str]],\n            dtypes: Optional[Dict[str, Union[str, DataType]]]) -&gt; DataSourceAttrs:\n        dataset_attrs = {\n            'sortbykey': sortbykey if sortbykey is not None else [],\n            'entities': entities if entities is not None else [],\n            'generate_cols': generate_cols if generate_cols is not None else [],\n            'exclude_cols': exclude_cols if exclude_cols is not None else [],\n            'dtypes': {k: DataType(v) for k, v in dtypes.items()} if dtypes is not None else {}\n        }\n        return DataSourceAttrs(**dataset_attrs)\n\n    @staticmethod\n    def _validate_datasource_attributes(X: Union[DataSource, pdDataFrame], dataset_attrs: DataSourceAttrs, datatype: DataSourceType, target: Optional[str]):\n        columns = []\n        if isinstance(X, pdDataFrame):\n            columns = X.columns\n            if datatype is None:\n                raise DataTypeMissingError(\n                    \"Argument `datatype` is mandatory for pandas.DataFrame training data\")\n        else:\n            columns = [c.name for c in X.metadata.columns]\n\n        if target is not None and target not in columns:\n            raise DataSourceAttrsError(\n                \"Invalid target: column '{target}' does not exist\")\n\n        if datatype == DataSourceType.TIMESERIES:\n            if not dataset_attrs.sortbykey:\n                raise DataSourceAttrsError(\n                    \"The argument `sortbykey` is mandatory for timeseries datasource.\")\n\n        invalid_fields = {}\n        for field, v in dataset_attrs.dict().items():\n            field_columns = v if field != 'dtypes' else v.keys()\n            not_in_cols = [c for c in field_columns if c not in columns]\n            if len(not_in_cols) &gt; 0:\n                invalid_fields[field] = not_in_cols\n\n        if len(invalid_fields) &gt; 0:\n            error_msgs = [\"\\t- Field '{}': columns {} do not exist\".format(\n                f, ', '.join(v)) for f, v in invalid_fields.items()]\n            raise DataSourceAttrsError(\n                \"The dataset attributes are invalid:\\n {}\".format('\\n'.join(error_msgs)))\n\n    @staticmethod\n    def _metadata_to_payload(\n        datatype: DataSourceType, ds_metadata: Metadata,\n        dataset_attrs: Optional[DataSourceAttrs] = None, target: Optional[str] = None\n    ) -&gt; dict:\n        \"\"\"Transform a the metadata and dataset attributes into a valid\n        payload.\n\n        Arguments:\n            datatype (DataSourceType): datasource type\n            ds_metadata (Metadata): datasource metadata object\n            dataset_attrs ( Optional[DataSourceAttrs] ): (optional) Dataset attributes\n            target (Optional[str]): (optional) target column name\n\n        Returns:\n            metadata payload dictionary\n        \"\"\"\n\n        columns = [\n            {\n                'name': c.name,\n                'generation': True and c.name not in dataset_attrs.exclude_cols,\n                'dataType': DataType(dataset_attrs.dtypes[c.name]).value if c.name in dataset_attrs.dtypes else c.datatype,\n                'varType': c.vartype,\n            }\n            for c in ds_metadata.columns]\n\n        metadata = {\n            'columns': columns,\n            'target': target\n        }\n\n        if dataset_attrs is not None:\n            if datatype == DataSourceType.TIMESERIES:\n                metadata['sortBy'] = [c for c in dataset_attrs.sortbykey]\n                metadata['entity'] = [c for c in dataset_attrs.entities]\n\n        return metadata\n\n    def _fit_from_datasource(\n        self,\n        X: DataSource,\n        datatype: DataSourceType,\n        privacy_level: Optional[PrivacyLevel] = None,\n        dataset_attrs: Optional[DataSourceAttrs] = None,\n        target: Optional[str] = None,\n        anonymize: Optional[dict] = None,\n        condition_on: Optional[List[str]] = None\n    ) -&gt; None:\n        payload = self._create_payload()\n\n        payload['dataSourceUID'] = X.uid\n\n        if privacy_level:\n            payload['privacyLevel'] = privacy_level.value\n\n        if X.metadata is not None:\n            payload['metadata'] = self._metadata_to_payload(\n                datatype, X.metadata, dataset_attrs, target)\n\n        payload['type'] = str(datatype.value)\n\n        if anonymize is not None:\n            payload[\"extraData\"][\"anonymize\"] = anonymize\n        if condition_on is not None:\n            payload[\"extraData\"][\"condition_on\"] = condition_on\n\n        response = self._client.post(\n            '/synthesizer/', json=payload, project=self._project)\n        data = response.json()\n        self._model = mSynthesizer(**data)\n        while self._check_fitting_not_finished(self.status):\n            self._logger.info('Training the synthesizer...')\n            sleep(BACKOFF)\n\n    def _create_payload(self) -&gt; dict:\n        payload = {\n            'extraData': {}\n        }\n\n        if self._model and self._model.name:\n            payload['name'] = self._model.name\n\n        return payload\n\n    def _check_fitting_not_finished(self, status: Status) -&gt; bool:\n        self._logger.debug(f'checking status {status}')\n\n        if Status.State(status.state) in [Status.State.READY, Status.State.REPORT]:\n            return False\n\n        self._logger.debug(f'status not ready yet {status.state}')\n\n        if status.prepare and PrepareState(status.prepare.state) == PrepareState.FAILED:\n            raise FittingError('Could not train the synthesizer')\n\n        if status.training and TrainingState(status.training.state) == TrainingState.FAILED:\n            raise FittingError('Could not train the synthesizer')\n\n        return True\n\n    @abstractmethod\n    def sample(self) -&gt; pdDataFrame:\n        \"\"\"Abstract method to sample from a synthesizer.\"\"\"\n\n    def _sample(self, payload: Dict) -&gt; pdDataFrame:\n        \"\"\"Sample from a synthesizer.\n\n        Arguments:\n            payload (dict): payload configuring the sample request\n\n        Returns:\n            pandas `DataFrame`\n        \"\"\"\n        response = self._client.post(\n            f\"/synthesizer/{self.uid}/sample\", json=payload, project=self._project)\n\n        data: Dict = response.json()\n        sample_uid = data.get('uid')\n        sample_status = None\n        while sample_status not in ['finished', 'failed']:\n            self._logger.info('Sampling from the synthesizer...')\n            response = self._client.get(\n                f'/synthesizer/{self.uid}/history', project=self._project)\n            history: Dict = response.json()\n            sample_data = next((s for s in history if s.get('uid') == sample_uid), None)\n            sample_status = sample_data.get('status', {}).get('state')\n            sleep(BACKOFF)\n\n        response = self._client.get_static_file(\n            f'/synthesizer/{self.uid}/sample/{sample_uid}/sample.csv', project=self._project)\n        data = StringIO(response.content.decode())\n        return read_csv(data)\n\n    @property\n    def uid(self) -&gt; UID:\n        \"\"\"Get the status of a synthesizer instance.\n\n        Returns:\n            Synthesizer status\n        \"\"\"\n        if not self._is_initialized():\n            return Status.State.NOT_INITIALIZED\n\n        return self._model.uid\n\n    @property\n    def status(self) -&gt; Status:\n        \"\"\"Get the status of a synthesizer instance.\n\n        Returns:\n            Synthesizer status\n        \"\"\"\n        if not self._is_initialized():\n            return Status.not_initialized()\n\n        try:\n            self = self.get()\n            return self._model.status\n        except Exception:  # noqa: PIE786\n            return Status.unknown()\n\n    def get(self):\n        assert self._is_initialized() and self._model.uid, InputError(\n            \"Please provide the synthesizer `uid`\")\n\n        response = self._client.get(f'/synthesizer/{self.uid}', project=self._project)\n        data = response.json()\n        self._model = mSynthesizer(**data)\n\n        return self\n\n    @staticmethod\n    @init_client\n    def list(client: Optional[Client] = None) -&gt; SynthesizersList:\n        \"\"\"List the synthesizer instances.\n\n        Arguments:\n            client (Client): (optional) Client to connect to the backend\n\n        Returns:\n            List of synthesizers\n        \"\"\"\n        def __process_data(data: list) -&gt; list:\n            to_del = ['metadata', 'report', 'mode']\n            for e in data:\n                for k in to_del:\n                    e.pop(k, None)\n            return data\n\n        response = client.get('/synthesizer')\n        data: list = response.json()\n        data = __process_data(data)\n\n        return SynthesizersList(data)\n\n    def _is_initialized(self) -&gt; bool:\n        \"\"\"Determine if a synthesizer is instanciated or not.\n\n        Returns:\n            True if the synthesizer is instanciated\n        \"\"\"\n        return self._model is not None\n\n    def _already_fitted(self) -&gt; bool:\n        \"\"\"Determine if a synthesizer is already fitted.\n\n        Returns:\n            True if the synthesizer is instanciated\n        \"\"\"\n\n        return self._is_initialized() and \\\n            (self._model.status is not None\n             and self._model.status.training is not None\n             and self._model.status.training.state is not [TrainingState.PREPARING])\n\n    @staticmethod\n    def _resolve_api_status(api_status: Dict) -&gt; Status:\n        \"\"\"Determine the status of the Synthesizer.\n\n        The status of the synthesizer instance is determined by the state of\n        its different components.\n\n        Arguments:\n            api_status (dict): json from the endpoint GET /synthesizer\n\n        Returns:\n            Synthesizer Status\n        \"\"\"\n        status = Status(api_status.get('state', Status.UNKNOWN.name))\n        if status == Status.PREPARE:\n            if PrepareState(api_status.get('prepare', {}).get(\n                    'state', PrepareState.UNKNOWN.name)) == PrepareState.FAILED:\n                return Status.FAILED\n        elif status == Status.TRAIN:\n            if TrainingState(api_status.get('training', {}).get(\n                    'state', TrainingState.UNKNOWN.name)) == TrainingState.FAILED:\n                return Status.FAILED\n        elif status == Status.REPORT:\n            return Status.READY\n        return status\n</code></pre>"},{"location":"sdk/reference/api/synthesizers/base/#ydata.sdk.synthesizers.synthesizer.BaseSynthesizer.status","title":"<code>status: Status</code>  <code>property</code>","text":"<p>Get the status of a synthesizer instance.</p> <p>Returns:</p> Type Description <code>Status</code> <p>Synthesizer status</p>"},{"location":"sdk/reference/api/synthesizers/base/#ydata.sdk.synthesizers.synthesizer.BaseSynthesizer.uid","title":"<code>uid: UID</code>  <code>property</code>","text":"<p>Get the status of a synthesizer instance.</p> <p>Returns:</p> Type Description <code>UID</code> <p>Synthesizer status</p>"},{"location":"sdk/reference/api/synthesizers/base/#ydata.sdk.synthesizers.synthesizer.BaseSynthesizer.fit","title":"<code>fit(X, privacy_level=PrivacyLevel.HIGH_FIDELITY, datatype=None, sortbykey=None, entities=None, generate_cols=None, exclude_cols=None, dtypes=None, target=None, anonymize=None, condition_on=None)</code>","text":"<p>Fit the synthesizer.</p> <p>The synthesizer accepts as training dataset either a pandas <code>DataFrame</code> directly or a YData <code>DataSource</code>. When the training dataset is a pandas <code>DataFrame</code>, the argument <code>datatype</code> is required as it cannot be deduced.</p> <p>The argument<code>sortbykey</code> is mandatory for <code>TimeSeries</code>.</p> <p>By default, if <code>generate_cols</code> or <code>exclude_cols</code> are not specified, all columns are generated by the synthesizer. The argument <code>exclude_cols</code> has precedence over <code>generate_cols</code>, i.e. a column <code>col</code> will not be generated if it is in both list.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[DataSource, DataFrame]</code> <p>Training dataset</p> required <code>privacy_level</code> <code>PrivacyLevel</code> <p>Synthesizer privacy level (defaults to high fidelity)</p> <code>HIGH_FIDELITY</code> <code>datatype</code> <code>Optional[Union[DataSourceType, str]]</code> <p>(optional) Dataset datatype - required if <code>X</code> is a <code>pandas.DataFrame</code></p> <code>None</code> <code>sortbykey</code> <code>Union[str, List[str]]</code> <p>(optional) column(s) to use to sort timeseries datasets</p> <code>None</code> <code>entities</code> <code>Union[str, List[str]]</code> <p>(optional) columns representing entities ID</p> <code>None</code> <code>generate_cols</code> <code>List[str]</code> <p>(optional) columns that should be synthesized</p> <code>None</code> <code>exclude_cols</code> <code>List[str]</code> <p>(optional) columns that should not be synthesized</p> <code>None</code> <code>dtypes</code> <code>Dict[str, Union[str, DataType]]</code> <p>(optional) datatype mapping that will overwrite the datasource metadata column datatypes</p> <code>None</code> <code>target</code> <code>Optional[str]</code> <p>(optional) Target for the dataset</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>(optional) Synthesizer instance name</p> required <code>anonymize</code> <code>Optional[str]</code> <p>(optional) fields to anonymize and the anonymization strategy</p> <code>None</code> <code>condition_on</code> <code>Optional[List[str]]</code> <p>(Optional[List[str]]): (optional) list of features to condition upon</p> <code>None</code> Source code in <code>ydata/sdk/synthesizers/synthesizer.py</code> <pre><code>def fit(self, X: Union[DataSource, pdDataFrame],\n        privacy_level: PrivacyLevel = PrivacyLevel.HIGH_FIDELITY,\n        datatype: Optional[Union[DataSourceType, str]] = None,\n        sortbykey: Optional[Union[str, List[str]]] = None,\n        entities: Optional[Union[str, List[str]]] = None,\n        generate_cols: Optional[List[str]] = None,\n        exclude_cols: Optional[List[str]] = None,\n        dtypes: Optional[Dict[str, Union[str, DataType]]] = None,\n        target: Optional[str] = None,\n        anonymize: Optional[dict] = None,\n        condition_on: Optional[List[str]] = None) -&gt; None:\n    \"\"\"Fit the synthesizer.\n\n    The synthesizer accepts as training dataset either a pandas [`DataFrame`][pandas.DataFrame] directly or a YData [`DataSource`][ydata.sdk.datasources.DataSource].\n    When the training dataset is a pandas [`DataFrame`][pandas.DataFrame], the argument `datatype` is required as it cannot be deduced.\n\n    The argument`sortbykey` is mandatory for [`TimeSeries`][ydata.sdk.datasources.DataSourceType.TIMESERIES].\n\n    By default, if `generate_cols` or `exclude_cols` are not specified, all columns are generated by the synthesizer.\n    The argument `exclude_cols` has precedence over `generate_cols`, i.e. a column `col` will not be generated if it is in both list.\n\n    Arguments:\n        X (Union[DataSource, pandas.DataFrame]): Training dataset\n        privacy_level (PrivacyLevel): Synthesizer privacy level (defaults to high fidelity)\n        datatype (Optional[Union[DataSourceType, str]]): (optional) Dataset datatype - required if `X` is a [`pandas.DataFrame`][pandas.DataFrame]\n        sortbykey (Union[str, List[str]]): (optional) column(s) to use to sort timeseries datasets\n        entities (Union[str, List[str]]): (optional) columns representing entities ID\n        generate_cols (List[str]): (optional) columns that should be synthesized\n        exclude_cols (List[str]): (optional) columns that should not be synthesized\n        dtypes (Dict[str, Union[str, DataType]]): (optional) datatype mapping that will overwrite the datasource metadata column datatypes\n        target (Optional[str]): (optional) Target for the dataset\n        name (Optional[str]): (optional) Synthesizer instance name\n        anonymize (Optional[str]): (optional) fields to anonymize and the anonymization strategy\n        condition_on: (Optional[List[str]]): (optional) list of features to condition upon\n    \"\"\"\n    if self._already_fitted():\n        raise AlreadyFittedError()\n\n    datatype = DataSourceType(datatype)\n\n    dataset_attrs = self._init_datasource_attributes(\n        sortbykey, entities, generate_cols, exclude_cols, dtypes)\n    self._validate_datasource_attributes(X, dataset_attrs, datatype, target)\n\n    # If the training data is a pandas dataframe, we first need to create a data source and then the instance\n    if isinstance(X, pdDataFrame):\n        if X.empty:\n            raise EmptyDataError(\"The DataFrame is empty\")\n        self._logger.info('creating local connector with pandas dataframe')\n        connector = LocalConnector.create(\n            source=X, project=self._project, client=self._client)\n        self._logger.info(\n            f'created local connector. creating datasource with {connector}')\n        _X = LocalDataSource(connector=connector, project=self._project,\n                             datatype=datatype, client=self._client)\n        self._logger.info(f'created datasource {_X}')\n    else:\n        _X = X\n\n    if dsState(_X.status.state) != dsState.AVAILABLE:\n        raise DataSourceNotAvailableError(\n            f\"The datasource '{_X.uid}' is not available (status = {_X.status})\")\n\n    if isinstance(dataset_attrs, dict):\n        dataset_attrs = DataSourceAttrs(**dataset_attrs)\n\n    self._fit_from_datasource(\n        X=_X, datatype=datatype, dataset_attrs=dataset_attrs, target=target,\n        anonymize=anonymize, privacy_level=privacy_level, condition_on=condition_on)\n</code></pre>"},{"location":"sdk/reference/api/synthesizers/base/#ydata.sdk.synthesizers.synthesizer.BaseSynthesizer.list","title":"<code>list(client=None)</code>  <code>staticmethod</code>","text":"<p>List the synthesizer instances.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>(optional) Client to connect to the backend</p> <code>None</code> <p>Returns:</p> Type Description <code>SynthesizersList</code> <p>List of synthesizers</p> Source code in <code>ydata/sdk/synthesizers/synthesizer.py</code> <pre><code>@staticmethod\n@init_client\ndef list(client: Optional[Client] = None) -&gt; SynthesizersList:\n    \"\"\"List the synthesizer instances.\n\n    Arguments:\n        client (Client): (optional) Client to connect to the backend\n\n    Returns:\n        List of synthesizers\n    \"\"\"\n    def __process_data(data: list) -&gt; list:\n        to_del = ['metadata', 'report', 'mode']\n        for e in data:\n            for k in to_del:\n                e.pop(k, None)\n        return data\n\n    response = client.get('/synthesizer')\n    data: list = response.json()\n    data = __process_data(data)\n\n    return SynthesizersList(data)\n</code></pre>"},{"location":"sdk/reference/api/synthesizers/base/#ydata.sdk.synthesizers.synthesizer.BaseSynthesizer.sample","title":"<code>sample()</code>  <code>abstractmethod</code>","text":"<p>Abstract method to sample from a synthesizer.</p> Source code in <code>ydata/sdk/synthesizers/synthesizer.py</code> <pre><code>@abstractmethod\ndef sample(self) -&gt; pdDataFrame:\n    \"\"\"Abstract method to sample from a synthesizer.\"\"\"\n</code></pre>"},{"location":"sdk/reference/api/synthesizers/base/#privacylevel","title":"PrivacyLevel","text":"<p>               Bases: <code>StringEnum</code></p> <p>Privacy level exposed to the end-user.</p>"},{"location":"sdk/reference/api/synthesizers/base/#ydata.sdk.synthesizers.PrivacyLevel.BALANCED_PRIVACY_FIDELITY","title":"<code>BALANCED_PRIVACY_FIDELITY = 'BALANCED_PRIVACY_FIDELITY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Balanced privacy/fidelity</p>"},{"location":"sdk/reference/api/synthesizers/base/#ydata.sdk.synthesizers.PrivacyLevel.HIGH_FIDELITY","title":"<code>HIGH_FIDELITY = 'HIGH_FIDELITY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>High fidelity</p>"},{"location":"sdk/reference/api/synthesizers/base/#ydata.sdk.synthesizers.PrivacyLevel.HIGH_PRIVACY","title":"<code>HIGH_PRIVACY = 'HIGH_PRIVACY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>High privacy</p>"},{"location":"sdk/reference/api/synthesizers/multitable/","title":"MultiTable","text":"<p>               Bases: <code>BaseSynthesizer</code></p> <p>MultiTable synthesizer class.</p>"},{"location":"sdk/reference/api/synthesizers/multitable/#ydata.sdk.synthesizers.multitable.MultiTableSynthesizer--methods","title":"Methods","text":"<ul> <li><code>fit</code>: train a synthesizer instance.</li> <li><code>sample</code>: request synthetic data.</li> <li><code>status</code>: current status of the synthesizer instance.</li> </ul> Note <p>The synthesizer instance is created in the backend only when the <code>fit</code> method is called.</p> <p>Parameters:</p> Name Type Description Default <code>write_connector</code> <code>UID | Connector</code> <p>Connector of type RDBMS to be used to write the samples</p> required <code>uid</code> <code>UID</code> <p>(optional) UID to identify this synthesizer</p> <code>None</code> <code>name</code> <code>str</code> <p>(optional) Name to be used when creating the synthesizer. Calculated internally if not provided</p> <code>None</code> <code>client</code> <code>Client</code> <p>(optional) Client to connect to the backend</p> <code>None</code> Source code in <code>ydata/sdk/synthesizers/multitable.py</code> <pre><code>class MultiTableSynthesizer(BaseSynthesizer):\n    \"\"\"MultiTable synthesizer class.\n\n    Methods\n    -------\n    - `fit`: train a synthesizer instance.\n    - `sample`: request synthetic data.\n    - `status`: current status of the synthesizer instance.\n\n    Note:\n            The synthesizer instance is created in the backend only when the `fit` method is called.\n\n    Arguments:\n        write_connector (UID | Connector): Connector of type RDBMS to be used to write the samples\n        uid (UID): (optional) UID to identify this synthesizer\n        name (str): (optional) Name to be used when creating the synthesizer. Calculated internally if not provided\n        client (Client): (optional) Client to connect to the backend\n    \"\"\"\n\n    def __init__(\n            self, write_connector: Union[Connector, UID], uid: Optional[UID] = None, name: Optional[str] = None,\n            project: Optional[Project] = None, client: Optional[Client] = None):\n\n        super().__init__(uid, name, project, client)\n\n        connector = self._check_or_fetch_connector(write_connector)\n        self.__write_connector = connector.uid\n\n    def fit(self, X: DataSource,\n            privacy_level: PrivacyLevel = PrivacyLevel.HIGH_FIDELITY,\n            datatype: Optional[Union[DataSourceType, str]] = None,\n            sortbykey: Optional[Union[str, List[str]]] = None,\n            entities: Optional[Union[str, List[str]]] = None,\n            generate_cols: Optional[List[str]] = None,\n            exclude_cols: Optional[List[str]] = None,\n            dtypes: Optional[Dict[str, Union[str, DataType]]] = None,\n            target: Optional[str] = None,\n            anonymize: Optional[dict] = None,\n            condition_on: Optional[List[str]] = None) -&gt; None:\n        \"\"\"Fit the synthesizer.\n\n        The synthesizer accepts as training dataset a YData [`DataSource`][ydata.sdk.datasources.DataSource].\n        Except X, all the other arguments are for now ignored until they are supported.\n\n        Arguments:\n            X (DataSource): DataSource to Train\n        \"\"\"\n\n        self._fit_from_datasource(X, datatype=DataSourceType.MULTITABLE)\n\n    def sample(self, frac: Union[int, float] = 1, write_connector: Optional[Union[Connector, UID]] = None) -&gt; None:\n        \"\"\"Sample from a [`MultiTableSynthesizer`][ydata.sdk.synthesizers.MultiTableSynthesizer]\n        instance.\n        The sample is saved in the connector that was provided in the synthesizer initialization\n        or in the\n\n        Arguments:\n            frac (int | float): fraction of the sample to be returned\n        \"\"\"\n\n        assert frac &gt;= 0.1, InputError(\n            \"It is not possible to generate an empty synthetic data schema. Please validate the input provided. \")\n        assert frac &lt;= 5, InputError(\n            \"It is not possible to generate a database that is 5x bigger than the original dataset. Please validate the input provided.\")\n\n        payload = {\n            'fraction': frac,\n        }\n\n        if write_connector is not None:\n            connector = self._check_or_fetch_connector(write_connector)\n            payload['writeConnector'] = connector.uid\n\n        response = self._client.post(\n            f\"/synthesizer/{self.uid}/sample\", json=payload, project=self._project)\n\n        data = response.json()\n        sample_uid = data.get('uid')\n        sample_status = None\n        while sample_status not in ['finished', 'failed']:\n            self._logger.info('Sampling from the synthesizer...')\n            response = self._client.get(\n                f'/synthesizer/{self.uid}/history', project=self._project)\n            history = response.json()\n            sample_data = next((s for s in history if s.get('uid') == sample_uid), None)\n            sample_status = sample_data.get('status', {}).get('state')\n            sleep(BACKOFF)\n\n        print(\n            f\"Sample created and saved into connector with ID {self.__write_connector or write_connector}\")\n\n    def _create_payload(self) -&gt; dict:\n        payload = super()._create_payload()\n        payload['writeConnector'] = self.__write_connector\n\n        return payload\n\n    def _check_or_fetch_connector(self, write_connector: Union[Connector, UID]) -&gt; Connector:\n        self._logger.debug(f'Write connector is {write_connector}')\n        if isinstance(write_connector, str):\n            self._logger.debug(f'Write connector is of type `UID` {write_connector}')\n            write_connector = Connector.get(write_connector)\n            self._logger.debug(f'Using fetched connector {write_connector}')\n\n        if write_connector.uid is None:\n            raise InputError(\"Invalid connector provided as input for write\")\n\n        if write_connector.type not in [ConnectorType.AZURE_SQL, ConnectorType.MYSQL, ConnectorType.SNOWFLAKE]:\n            raise ConnectorError(\n                f\"Invalid type `{write_connector.type}` for the provided connector\")\n\n        return write_connector\n</code></pre>"},{"location":"sdk/reference/api/synthesizers/multitable/#ydata.sdk.synthesizers.multitable.MultiTableSynthesizer.fit","title":"<code>fit(X, privacy_level=PrivacyLevel.HIGH_FIDELITY, datatype=None, sortbykey=None, entities=None, generate_cols=None, exclude_cols=None, dtypes=None, target=None, anonymize=None, condition_on=None)</code>","text":"<p>Fit the synthesizer.</p> <p>The synthesizer accepts as training dataset a YData <code>DataSource</code>. Except X, all the other arguments are for now ignored until they are supported.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataSource</code> <p>DataSource to Train</p> required Source code in <code>ydata/sdk/synthesizers/multitable.py</code> <pre><code>def fit(self, X: DataSource,\n        privacy_level: PrivacyLevel = PrivacyLevel.HIGH_FIDELITY,\n        datatype: Optional[Union[DataSourceType, str]] = None,\n        sortbykey: Optional[Union[str, List[str]]] = None,\n        entities: Optional[Union[str, List[str]]] = None,\n        generate_cols: Optional[List[str]] = None,\n        exclude_cols: Optional[List[str]] = None,\n        dtypes: Optional[Dict[str, Union[str, DataType]]] = None,\n        target: Optional[str] = None,\n        anonymize: Optional[dict] = None,\n        condition_on: Optional[List[str]] = None) -&gt; None:\n    \"\"\"Fit the synthesizer.\n\n    The synthesizer accepts as training dataset a YData [`DataSource`][ydata.sdk.datasources.DataSource].\n    Except X, all the other arguments are for now ignored until they are supported.\n\n    Arguments:\n        X (DataSource): DataSource to Train\n    \"\"\"\n\n    self._fit_from_datasource(X, datatype=DataSourceType.MULTITABLE)\n</code></pre>"},{"location":"sdk/reference/api/synthesizers/multitable/#ydata.sdk.synthesizers.multitable.MultiTableSynthesizer.sample","title":"<code>sample(frac=1, write_connector=None)</code>","text":"<p>Sample from a <code>MultiTableSynthesizer</code> instance. The sample is saved in the connector that was provided in the synthesizer initialization or in the</p> <p>Parameters:</p> Name Type Description Default <code>frac</code> <code>int | float</code> <p>fraction of the sample to be returned</p> <code>1</code> Source code in <code>ydata/sdk/synthesizers/multitable.py</code> <pre><code>def sample(self, frac: Union[int, float] = 1, write_connector: Optional[Union[Connector, UID]] = None) -&gt; None:\n    \"\"\"Sample from a [`MultiTableSynthesizer`][ydata.sdk.synthesizers.MultiTableSynthesizer]\n    instance.\n    The sample is saved in the connector that was provided in the synthesizer initialization\n    or in the\n\n    Arguments:\n        frac (int | float): fraction of the sample to be returned\n    \"\"\"\n\n    assert frac &gt;= 0.1, InputError(\n        \"It is not possible to generate an empty synthetic data schema. Please validate the input provided. \")\n    assert frac &lt;= 5, InputError(\n        \"It is not possible to generate a database that is 5x bigger than the original dataset. Please validate the input provided.\")\n\n    payload = {\n        'fraction': frac,\n    }\n\n    if write_connector is not None:\n        connector = self._check_or_fetch_connector(write_connector)\n        payload['writeConnector'] = connector.uid\n\n    response = self._client.post(\n        f\"/synthesizer/{self.uid}/sample\", json=payload, project=self._project)\n\n    data = response.json()\n    sample_uid = data.get('uid')\n    sample_status = None\n    while sample_status not in ['finished', 'failed']:\n        self._logger.info('Sampling from the synthesizer...')\n        response = self._client.get(\n            f'/synthesizer/{self.uid}/history', project=self._project)\n        history = response.json()\n        sample_data = next((s for s in history if s.get('uid') == sample_uid), None)\n        sample_status = sample_data.get('status', {}).get('state')\n        sleep(BACKOFF)\n\n    print(\n        f\"Sample created and saved into connector with ID {self.__write_connector or write_connector}\")\n</code></pre>"},{"location":"sdk/reference/api/synthesizers/regular/","title":"Regular","text":"<p>               Bases: <code>BaseSynthesizer</code></p> Source code in <code>ydata/sdk/synthesizers/regular.py</code> <pre><code>class RegularSynthesizer(BaseSynthesizer):\n\n    def sample(self, n_samples: int = 1, condition_on: Optional[dict] = None) -&gt; pdDataFrame:\n        \"\"\"Sample from a [`RegularSynthesizer`][ydata.sdk.synthesizers.RegularSynthesizer]\n        instance.\n\n        Arguments:\n            n_samples (int): number of rows in the sample\n            condition_on: (Optional[dict]): (optional) conditional sampling parameters\n\n        Returns:\n            synthetic data\n        \"\"\"\n        if n_samples &lt; 1:\n            raise InputError(\"Parameter 'n_samples' must be greater than 0\")\n\n        payload = {\"numberOfRecords\": n_samples}\n        if condition_on is not None:\n            payload[\"extraData\"] = {\n                \"condition_on\": condition_on\n            }\n        return self._sample(payload=payload)\n\n    def fit(self, X: Union[DataSource, pdDataFrame],\n            privacy_level: PrivacyLevel = PrivacyLevel.HIGH_FIDELITY,\n            entities: Optional[Union[str, List[str]]] = None,\n            generate_cols: Optional[List[str]] = None,\n            exclude_cols: Optional[List[str]] = None,\n            dtypes: Optional[Dict[str, Union[str, DataType]]] = None,\n            target: Optional[str] = None,\n            anonymize: Optional[dict] = None,\n            condition_on: Optional[List[str]] = None) -&gt; None:\n        \"\"\"Fit the synthesizer.\n\n        The synthesizer accepts as training dataset either a pandas [`DataFrame`][pandas.DataFrame] directly or a YData [`DataSource`][ydata.sdk.datasources.DataSource].\n\n        Arguments:\n            X (Union[DataSource, pandas.DataFrame]): Training dataset\n            privacy_level (PrivacyLevel): Synthesizer privacy level (defaults to high fidelity)\n            entities (Union[str, List[str]]): (optional) columns representing entities ID\n            generate_cols (List[str]): (optional) columns that should be synthesized\n            exclude_cols (List[str]): (optional) columns that should not be synthesized\n            dtypes (Dict[str, Union[str, DataType]]): (optional) datatype mapping that will overwrite the datasource metadata column datatypes\n            target (Optional[str]): (optional) Target column\n            name (Optional[str]): (optional) Synthesizer instance name\n            anonymize (Optional[str]): (optional) fields to anonymize and the anonymization strategy\n            condition_on: (Optional[List[str]]): (optional) list of features to condition upon\n        \"\"\"\n        BaseSynthesizer.fit(self, X=X, datatype=DataSourceType.TABULAR, entities=entities,\n                            generate_cols=generate_cols, exclude_cols=exclude_cols, dtypes=dtypes,\n                            target=target, anonymize=anonymize, privacy_level=privacy_level,\n                            condition_on=condition_on)\n\n    def __repr__(self):\n        if self._model is not None:\n            return self._model.__repr__()\n        else:\n            return \"RegularSynthesizer(Not Initialized)\"\n</code></pre>"},{"location":"sdk/reference/api/synthesizers/regular/#ydata.sdk.synthesizers.regular.RegularSynthesizer.fit","title":"<code>fit(X, privacy_level=PrivacyLevel.HIGH_FIDELITY, entities=None, generate_cols=None, exclude_cols=None, dtypes=None, target=None, anonymize=None, condition_on=None)</code>","text":"<p>Fit the synthesizer.</p> <p>The synthesizer accepts as training dataset either a pandas <code>DataFrame</code> directly or a YData <code>DataSource</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[DataSource, DataFrame]</code> <p>Training dataset</p> required <code>privacy_level</code> <code>PrivacyLevel</code> <p>Synthesizer privacy level (defaults to high fidelity)</p> <code>HIGH_FIDELITY</code> <code>entities</code> <code>Union[str, List[str]]</code> <p>(optional) columns representing entities ID</p> <code>None</code> <code>generate_cols</code> <code>List[str]</code> <p>(optional) columns that should be synthesized</p> <code>None</code> <code>exclude_cols</code> <code>List[str]</code> <p>(optional) columns that should not be synthesized</p> <code>None</code> <code>dtypes</code> <code>Dict[str, Union[str, DataType]]</code> <p>(optional) datatype mapping that will overwrite the datasource metadata column datatypes</p> <code>None</code> <code>target</code> <code>Optional[str]</code> <p>(optional) Target column</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>(optional) Synthesizer instance name</p> required <code>anonymize</code> <code>Optional[str]</code> <p>(optional) fields to anonymize and the anonymization strategy</p> <code>None</code> <code>condition_on</code> <code>Optional[List[str]]</code> <p>(Optional[List[str]]): (optional) list of features to condition upon</p> <code>None</code> Source code in <code>ydata/sdk/synthesizers/regular.py</code> <pre><code>def fit(self, X: Union[DataSource, pdDataFrame],\n        privacy_level: PrivacyLevel = PrivacyLevel.HIGH_FIDELITY,\n        entities: Optional[Union[str, List[str]]] = None,\n        generate_cols: Optional[List[str]] = None,\n        exclude_cols: Optional[List[str]] = None,\n        dtypes: Optional[Dict[str, Union[str, DataType]]] = None,\n        target: Optional[str] = None,\n        anonymize: Optional[dict] = None,\n        condition_on: Optional[List[str]] = None) -&gt; None:\n    \"\"\"Fit the synthesizer.\n\n    The synthesizer accepts as training dataset either a pandas [`DataFrame`][pandas.DataFrame] directly or a YData [`DataSource`][ydata.sdk.datasources.DataSource].\n\n    Arguments:\n        X (Union[DataSource, pandas.DataFrame]): Training dataset\n        privacy_level (PrivacyLevel): Synthesizer privacy level (defaults to high fidelity)\n        entities (Union[str, List[str]]): (optional) columns representing entities ID\n        generate_cols (List[str]): (optional) columns that should be synthesized\n        exclude_cols (List[str]): (optional) columns that should not be synthesized\n        dtypes (Dict[str, Union[str, DataType]]): (optional) datatype mapping that will overwrite the datasource metadata column datatypes\n        target (Optional[str]): (optional) Target column\n        name (Optional[str]): (optional) Synthesizer instance name\n        anonymize (Optional[str]): (optional) fields to anonymize and the anonymization strategy\n        condition_on: (Optional[List[str]]): (optional) list of features to condition upon\n    \"\"\"\n    BaseSynthesizer.fit(self, X=X, datatype=DataSourceType.TABULAR, entities=entities,\n                        generate_cols=generate_cols, exclude_cols=exclude_cols, dtypes=dtypes,\n                        target=target, anonymize=anonymize, privacy_level=privacy_level,\n                        condition_on=condition_on)\n</code></pre>"},{"location":"sdk/reference/api/synthesizers/regular/#ydata.sdk.synthesizers.regular.RegularSynthesizer.sample","title":"<code>sample(n_samples=1, condition_on=None)</code>","text":"<p>Sample from a <code>RegularSynthesizer</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>number of rows in the sample</p> <code>1</code> <code>condition_on</code> <code>Optional[dict]</code> <p>(Optional[dict]): (optional) conditional sampling parameters</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>synthetic data</p> Source code in <code>ydata/sdk/synthesizers/regular.py</code> <pre><code>def sample(self, n_samples: int = 1, condition_on: Optional[dict] = None) -&gt; pdDataFrame:\n    \"\"\"Sample from a [`RegularSynthesizer`][ydata.sdk.synthesizers.RegularSynthesizer]\n    instance.\n\n    Arguments:\n        n_samples (int): number of rows in the sample\n        condition_on: (Optional[dict]): (optional) conditional sampling parameters\n\n    Returns:\n        synthetic data\n    \"\"\"\n    if n_samples &lt; 1:\n        raise InputError(\"Parameter 'n_samples' must be greater than 0\")\n\n    payload = {\"numberOfRecords\": n_samples}\n    if condition_on is not None:\n        payload[\"extraData\"] = {\n            \"condition_on\": condition_on\n        }\n    return self._sample(payload=payload)\n</code></pre>"},{"location":"sdk/reference/api/synthesizers/regular/#privacylevel","title":"PrivacyLevel","text":"<p>               Bases: <code>StringEnum</code></p> <p>Privacy level exposed to the end-user.</p>"},{"location":"sdk/reference/api/synthesizers/regular/#ydata.sdk.synthesizers.PrivacyLevel.BALANCED_PRIVACY_FIDELITY","title":"<code>BALANCED_PRIVACY_FIDELITY = 'BALANCED_PRIVACY_FIDELITY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Balanced privacy/fidelity</p>"},{"location":"sdk/reference/api/synthesizers/regular/#ydata.sdk.synthesizers.PrivacyLevel.HIGH_FIDELITY","title":"<code>HIGH_FIDELITY = 'HIGH_FIDELITY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>High fidelity</p>"},{"location":"sdk/reference/api/synthesizers/regular/#ydata.sdk.synthesizers.PrivacyLevel.HIGH_PRIVACY","title":"<code>HIGH_PRIVACY = 'HIGH_PRIVACY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>High privacy</p>"},{"location":"sdk/reference/api/synthesizers/timeseries/","title":"TimeSeries","text":"<p>               Bases: <code>BaseSynthesizer</code></p> Source code in <code>ydata/sdk/synthesizers/timeseries.py</code> <pre><code>class TimeSeriesSynthesizer(BaseSynthesizer):\n\n    def sample(self, n_entities: int, condition_on: Optional[dict] = None) -&gt; pdDataFrame:\n        \"\"\"Sample from a [`TimeSeriesSynthesizer`][ydata.sdk.synthesizers.TimeSeriesSynthesizer] instance.\n\n        If a training dataset was not using any `entity` column, the Synthesizer assumes a single entity.\n        A [`TimeSeriesSynthesizer`][ydata.sdk.synthesizers.TimeSeriesSynthesizer] always sample the full trajectory of its entities.\n\n        Arguments:\n            n_entities (int): number of entities to sample\n            condition_on: (Optional[dict]): (optional) conditional sampling parameters\n\n        Returns:\n            synthetic data\n        \"\"\"\n        if n_entities is not None and n_entities &lt; 1:\n            raise InputError(\"Parameter 'n_entities' must be greater than 0\")\n\n        payload = {\"numberOfRecords\": n_entities}\n        if condition_on is not None:\n            payload[\"extraData\"] = {\n                \"condition_on\": condition_on\n            }\n        return self._sample(payload=payload)\n\n    def fit(self, X: Union[DataSource, pdDataFrame],\n            sortbykey: Optional[Union[str, List[str]]],\n            privacy_level: PrivacyLevel = PrivacyLevel.HIGH_FIDELITY,\n            entities: Optional[Union[str, List[str]]] = None,\n            generate_cols: Optional[List[str]] = None,\n            exclude_cols: Optional[List[str]] = None,\n            dtypes: Optional[Dict[str, Union[str, DataType]]] = None,\n            target: Optional[str] = None,\n            anonymize: Optional[dict] = None,\n            condition_on: Optional[List[str]] = None) -&gt; None:\n        \"\"\"Fit the synthesizer.\n\n        The synthesizer accepts as training dataset either a pandas [`DataFrame`][pandas.DataFrame] directly or a YData [`DataSource`][ydata.sdk.datasources.DataSource].\n\n        Arguments:\n            X (Union[DataSource, pandas.DataFrame]): Training dataset\n            sortbykey (Union[str, List[str]]): column(s) to use to sort timeseries datasets\n            privacy_level (PrivacyLevel): Synthesizer privacy level (defaults to high fidelity)\n            entities (Union[str, List[str]]): (optional) columns representing entities ID\n            generate_cols (List[str]): (optional) columns that should be synthesized\n            exclude_cols (List[str]): (optional) columns that should not be synthesized\n            dtypes (Dict[str, Union[str, DataType]]): (optional) datatype mapping that will overwrite the datasource metadata column datatypes\n            target (Optional[str]): (optional) Metadata associated to the datasource\n            name (Optional[str]): (optional) Synthesizer instance name\n            anonymize (Optional[str]): (optional) fields to anonymize and the anonymization strategy\n            condition_on: (Optional[List[str]]): (optional) list of features to condition upon\n        \"\"\"\n        BaseSynthesizer.fit(self, X=X, datatype=DataSourceType.TIMESERIES, sortbykey=sortbykey,\n                            entities=entities, generate_cols=generate_cols, exclude_cols=exclude_cols,\n                            dtypes=dtypes, target=target, anonymize=anonymize, privacy_level=privacy_level,\n                            condition_on=condition_on)\n\n    def __repr__(self):\n        if self._model is not None:\n            return self._model.__repr__()\n        else:\n            return \"TimeSeriesSynthesizer(Not Initialized)\"\n</code></pre>"},{"location":"sdk/reference/api/synthesizers/timeseries/#ydata.sdk.synthesizers.timeseries.TimeSeriesSynthesizer.fit","title":"<code>fit(X, sortbykey, privacy_level=PrivacyLevel.HIGH_FIDELITY, entities=None, generate_cols=None, exclude_cols=None, dtypes=None, target=None, anonymize=None, condition_on=None)</code>","text":"<p>Fit the synthesizer.</p> <p>The synthesizer accepts as training dataset either a pandas <code>DataFrame</code> directly or a YData <code>DataSource</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[DataSource, DataFrame]</code> <p>Training dataset</p> required <code>sortbykey</code> <code>Union[str, List[str]]</code> <p>column(s) to use to sort timeseries datasets</p> required <code>privacy_level</code> <code>PrivacyLevel</code> <p>Synthesizer privacy level (defaults to high fidelity)</p> <code>HIGH_FIDELITY</code> <code>entities</code> <code>Union[str, List[str]]</code> <p>(optional) columns representing entities ID</p> <code>None</code> <code>generate_cols</code> <code>List[str]</code> <p>(optional) columns that should be synthesized</p> <code>None</code> <code>exclude_cols</code> <code>List[str]</code> <p>(optional) columns that should not be synthesized</p> <code>None</code> <code>dtypes</code> <code>Dict[str, Union[str, DataType]]</code> <p>(optional) datatype mapping that will overwrite the datasource metadata column datatypes</p> <code>None</code> <code>target</code> <code>Optional[str]</code> <p>(optional) Metadata associated to the datasource</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>(optional) Synthesizer instance name</p> required <code>anonymize</code> <code>Optional[str]</code> <p>(optional) fields to anonymize and the anonymization strategy</p> <code>None</code> <code>condition_on</code> <code>Optional[List[str]]</code> <p>(Optional[List[str]]): (optional) list of features to condition upon</p> <code>None</code> Source code in <code>ydata/sdk/synthesizers/timeseries.py</code> <pre><code>def fit(self, X: Union[DataSource, pdDataFrame],\n        sortbykey: Optional[Union[str, List[str]]],\n        privacy_level: PrivacyLevel = PrivacyLevel.HIGH_FIDELITY,\n        entities: Optional[Union[str, List[str]]] = None,\n        generate_cols: Optional[List[str]] = None,\n        exclude_cols: Optional[List[str]] = None,\n        dtypes: Optional[Dict[str, Union[str, DataType]]] = None,\n        target: Optional[str] = None,\n        anonymize: Optional[dict] = None,\n        condition_on: Optional[List[str]] = None) -&gt; None:\n    \"\"\"Fit the synthesizer.\n\n    The synthesizer accepts as training dataset either a pandas [`DataFrame`][pandas.DataFrame] directly or a YData [`DataSource`][ydata.sdk.datasources.DataSource].\n\n    Arguments:\n        X (Union[DataSource, pandas.DataFrame]): Training dataset\n        sortbykey (Union[str, List[str]]): column(s) to use to sort timeseries datasets\n        privacy_level (PrivacyLevel): Synthesizer privacy level (defaults to high fidelity)\n        entities (Union[str, List[str]]): (optional) columns representing entities ID\n        generate_cols (List[str]): (optional) columns that should be synthesized\n        exclude_cols (List[str]): (optional) columns that should not be synthesized\n        dtypes (Dict[str, Union[str, DataType]]): (optional) datatype mapping that will overwrite the datasource metadata column datatypes\n        target (Optional[str]): (optional) Metadata associated to the datasource\n        name (Optional[str]): (optional) Synthesizer instance name\n        anonymize (Optional[str]): (optional) fields to anonymize and the anonymization strategy\n        condition_on: (Optional[List[str]]): (optional) list of features to condition upon\n    \"\"\"\n    BaseSynthesizer.fit(self, X=X, datatype=DataSourceType.TIMESERIES, sortbykey=sortbykey,\n                        entities=entities, generate_cols=generate_cols, exclude_cols=exclude_cols,\n                        dtypes=dtypes, target=target, anonymize=anonymize, privacy_level=privacy_level,\n                        condition_on=condition_on)\n</code></pre>"},{"location":"sdk/reference/api/synthesizers/timeseries/#ydata.sdk.synthesizers.timeseries.TimeSeriesSynthesizer.sample","title":"<code>sample(n_entities, condition_on=None)</code>","text":"<p>Sample from a <code>TimeSeriesSynthesizer</code> instance.</p> <p>If a training dataset was not using any <code>entity</code> column, the Synthesizer assumes a single entity. A <code>TimeSeriesSynthesizer</code> always sample the full trajectory of its entities.</p> <p>Parameters:</p> Name Type Description Default <code>n_entities</code> <code>int</code> <p>number of entities to sample</p> required <code>condition_on</code> <code>Optional[dict]</code> <p>(Optional[dict]): (optional) conditional sampling parameters</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>synthetic data</p> Source code in <code>ydata/sdk/synthesizers/timeseries.py</code> <pre><code>def sample(self, n_entities: int, condition_on: Optional[dict] = None) -&gt; pdDataFrame:\n    \"\"\"Sample from a [`TimeSeriesSynthesizer`][ydata.sdk.synthesizers.TimeSeriesSynthesizer] instance.\n\n    If a training dataset was not using any `entity` column, the Synthesizer assumes a single entity.\n    A [`TimeSeriesSynthesizer`][ydata.sdk.synthesizers.TimeSeriesSynthesizer] always sample the full trajectory of its entities.\n\n    Arguments:\n        n_entities (int): number of entities to sample\n        condition_on: (Optional[dict]): (optional) conditional sampling parameters\n\n    Returns:\n        synthetic data\n    \"\"\"\n    if n_entities is not None and n_entities &lt; 1:\n        raise InputError(\"Parameter 'n_entities' must be greater than 0\")\n\n    payload = {\"numberOfRecords\": n_entities}\n    if condition_on is not None:\n        payload[\"extraData\"] = {\n            \"condition_on\": condition_on\n        }\n    return self._sample(payload=payload)\n</code></pre>"},{"location":"sdk/reference/api/synthesizers/timeseries/#privacylevel","title":"PrivacyLevel","text":"<p>               Bases: <code>StringEnum</code></p> <p>Privacy level exposed to the end-user.</p>"},{"location":"sdk/reference/api/synthesizers/timeseries/#ydata.sdk.synthesizers.PrivacyLevel.BALANCED_PRIVACY_FIDELITY","title":"<code>BALANCED_PRIVACY_FIDELITY = 'BALANCED_PRIVACY_FIDELITY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Balanced privacy/fidelity</p>"},{"location":"sdk/reference/api/synthesizers/timeseries/#ydata.sdk.synthesizers.PrivacyLevel.HIGH_FIDELITY","title":"<code>HIGH_FIDELITY = 'HIGH_FIDELITY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>High fidelity</p>"},{"location":"sdk/reference/api/synthesizers/timeseries/#ydata.sdk.synthesizers.PrivacyLevel.HIGH_PRIVACY","title":"<code>HIGH_PRIVACY = 'HIGH_PRIVACY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>High privacy</p>"},{"location":"support/help-troubleshooting/","title":"Help &amp; Troubleshooting","text":""},{"location":"synthetic_data/","title":"Synthetic Data generation","text":"<p>YData Fabric's Synthetic data Generation capabilities leverages the latest generative models to create high-quality artificial data that replicates real-world data properties. Regardless it is a table, a database or a tex corpus this powerful capability ensures privacy, enhances data availability, and boosts model performance across various industries. In this section discover how YData Fabric's synthetic data solutions can transform your data-driven initiatives.</p>"},{"location":"synthetic_data/#what-is-synthetic-data","title":"What is Synthetic Data?","text":"<p>Synthetic data is artificially generated data that mimics the statistical properties and structure of real-world data without directly copying it. It is created using algorithms and models designed to replicate the characteristics of actual data sets. This process ensures that synthetic data retains the essential patterns and relationships present in the original data, making it a valuable asset for various applications, particularly in situations where using real data might pose privacy, security, or availability concerns. It can be used for:</p> <ul> <li>Guaranteeing privacy and compliance when sharing datasets (for quality assurance, product development and other analytics teams)</li> <li>Removing bias by upsampling rare events</li> <li>Balancing datasets</li> <li>Augment existing datasets to improve the performance of machine learning models or use in stress testing</li> <li>Smartly fill in missing values based on context</li> <li>Simulate new scenarios and hypothesis</li> </ul>"},{"location":"synthetic_data/#the-benefits-of-synthetic-data","title":"The benefits of Synthetic Data","text":"<p>Leveraging synthetic data offers numerous benefits:</p> <ul> <li>Privacy and Security: Synthetic data eliminates the risk of exposing sensitive information, making it an ideal solution for industries handling sensitive data, such as healthcare, finance, and telecommunications.</li> <li>Data Augmentation: It enables organizations to augment existing data sets, enhancing model training by providing diverse and representative samples, thereby improving model accuracy and robustness.</li> <li>Cost Efficiency: Generating synthetic data can be more cost-effective than collecting and labeling large volumes of real data, particularly for rare events or scenarios that are difficult to capture.</li> <li>Testing and Development: Synthetic data provides a safe environment for testing and developing algorithms, ensuring that models are robust before deployment in real-world scenarios.</li> </ul>"},{"location":"synthetic_data/#synthetic-data-in-fabric","title":"Synthetic Data in Fabric","text":"<p>YData Fabric offers robust support for creating high-quality synthetic data using generative models and/or through bootstrapping. The platform is designed to address the diverse needs of data scientists, engineers, and analysts by providing a comprehensive set of tools and features.</p>"},{"location":"synthetic_data/#data-types-supported","title":"Data Types Supported:","text":"<p>YData Fabric supports the generation of various data types, including:</p> <ul> <li>Tabular Data: Generate synthetic versions of structured data typically found in spreadsheets and databases, with support for categorical, numerical, and mixed data types.</li> <li>Time Series Data: Create synthetic time series data that preserves the temporal dependencies and trends, useful for applications like financial forecasting and sensor data analysis.</li> <li>Multi-Table or Database Synthesis: Synthesize complex databases with multiple interrelated tables, maintaining the relational integrity and dependencies, which is crucial for comprehensive data analysis and testing applications.</li> <li>Text Data: Produce synthetic text data for natural language processing (NLP) tasks, ensuring the generated text maintains the linguistic properties and context of the original data.</li> </ul>"},{"location":"synthetic_data/#related-materials","title":"Related Materials","text":"<ul> <li>\ud83d\udcd6 The 5 Benefits of Synthetic data generation for modern AI</li> <li>\ud83d\udcd6 The role of Synthetic data in Healthcare</li> <li>\ud83d\udcd6 The role of Synthetic data to overcome Bias</li> </ul>"},{"location":"synthetic_data/relational_database/","title":"Multi-Table Synthetic data generation","text":"<p>Multi-Table or Database's synthetic data generation is a powerful method to create high-quality artificial datasets that mirror the statistical properties and relational structures of original multi-table databases. A multi-table database consists of multiple interrelated tables, often with various data types (dates, categorical, numerical, etc.) and complex relationships between records. Key use cases include privacy-preserving access to full production databases and the creation of realistic test environments. Synthetic data allows organizations to share and analyze full production databases without exposing sensitive information, ensuring compliance with data privacy regulations. Additionally, it is invaluable for creating realistic test environments, enabling developers and testers to simulate real-world scenarios, identify potential issues, and validate database applications without risking data breaches. By leveraging synthetic multi-table data, organizations can simulate complex relational data environments, enhance the robustness of database applications, and ensure data privacy, making it a valuable tool for industries that rely on intricate data structures and interdependencies.</p>"},{"location":"synthetic_data/relational_database/#tutorials-recipes","title":"Tutorials &amp; Recipes","text":"<p>To get-started with Synthetic Data Generation you can follow out quickstart guide.</p> <p>For more tutorial and recipes, follow the link to YData's Academy.</p>"},{"location":"synthetic_data/relational_database/#related-materials","title":"Related Materials","text":"<ul> <li> How to generate Synthetic Data from a Database</li> <li> How to generate Multi-Table step-by-step</li> <li> How to generate Multi-Table synthetic data in Google Colab</li> </ul>"},{"location":"synthetic_data/single_table/","title":"Tabular synthetic data generation","text":"<p>Tabular synthetic data generation is a powerful method to create high-quality artificial datasets that mirror the statistical properties of original tabular data. A tabular dataset is usually composed by several columns with structured data and mixed data types (dates, categorical, numerical, etc) with not time dependence between records. This ability of generating synthetic data from this type of datasets is essential for a wide range of applications, from data augmentation to privacy preservation, and is particularly useful in scenarios where obtaining or using real data is challenging.</p>"},{"location":"synthetic_data/single_table/#tutorials-recipes","title":"Tutorials &amp; Recipes","text":"<p>To get-started with Synthetic Data Generation you can follow out quickstart guide.</p> <p>For more tutorial and recipes, follow the link to YData's Academy.</p>"},{"location":"synthetic_data/single_table/#related-materials","title":"Related Materials","text":"<ul> <li>\ud83d\udcd6 Generating Synthetic data from a Tabular dataset with a large number of columns</li> <li>\ud83d\udcd6 Synthetic data to improve Credit Scoring models</li> <li> Generate Synthetic data with Python code</li> <li> Synthetic data generation with API</li> </ul>"},{"location":"synthetic_data/text/","title":"Text Synthetic Data generation","text":"<p>Synthetic data generation for text creates high-quality artificial text datasets that mimic the properties and patterns of original text data, playing a crucial role in Generative AI applications. This technique enhances the performance of large language models (LLMs) by providing extensive training datasets, which improve model accuracy and robustness. It addresses data scarcity by generating text for specialized domains or languages where data is limited. Additionally, synthetic text generation ensures privacy preservation, allowing organizations to create useful datasets without compromising sensitive information, thereby complying with data privacy regulations while enabling comprehensive data analysis and model training\u200b</p> <p>Feature in Preview</p> <p>This feature is in preview and not available for all users. Contact us if you are interested in giving it a try!</p>"},{"location":"synthetic_data/text/#related-materials","title":"Related Materials","text":"<ul> <li> How to generate Synthetic Text Data?</li> </ul>"},{"location":"synthetic_data/timeseries/","title":"Time-series synthetic data generation","text":"<p>Time-series synthetic data generation is a powerful method to create high-quality artificial datasets that mirror the statistical properties of original time-series data. A time-series dataset is composed of sequential data points recorded at specific time intervals, capturing trends, patterns, and temporal dependencies. This ability to generate synthetic data from time-series datasets is essential for a wide range of applications, from data augmentation to privacy preservation, and is particularly useful in scenarios where obtaining or using real data is challenging. By leveraging synthetic time-series data, organizations can simulate various conditions and events, enhance model robustness, and ensure data privacy, making it a valuable tool for industries reliant on temporal data analysis. This type of data is prevalent in various fields, including finance, healthcare, energy, and IoT (Internet of Things).</p>"},{"location":"synthetic_data/timeseries/#tutorials-recipes","title":"Tutorials &amp; Recipes","text":"<p>To get-started with Synthetic Data Generation you can follow out quickstart guide.</p> <p>For more tutorial and recipes, follow the link to YData's Academy.</p>"},{"location":"synthetic_data/timeseries/#related-materials","title":"Related Materials","text":"<ul> <li>\ud83d\udcd6 Understanding the structure of a time-series dataset</li> <li>\ud83d\udcd6 Time-series synthetic data generation</li> <li>\ud83d\udcd6 Synthetic multivariate time-series data</li> <li> How to generate time-series synthetic data?</li> </ul>"}]}
{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>YData Fabric is a Data-Centric AI development platform that accelerates AI development by helping data practitioners achieve production-quality data.</p> <p>Much like for software engineering the quality of code is a must for the success of software development, Fabric accounts for the data quality requirements for data-driven applications. It introduces standards, processes, and acceleration to empower data science, analytics, and data engineering teams.</p> <p></p>"},{"location":"#try-fabric","title":"Try Fabric","text":"<ul> <li>Get started with Fabric Community</li> </ul>"},{"location":"#why-adopt-ydata-fabric","title":"Why adopt YData Fabric?","text":"<p>With Fabric, you can standardize the understanding of your data, quickly identify data quality issues, streamline and version your data preparation workflows and finally leverage synthetic data for privacy-compliance or as a tool to boost ML performance. Fabric is a development environment that supports a faster and easier process of preparing data for AI development. Data practitioners are using Fabric to:</p> <ul> <li>Establish a centralized and collaborative repository for data projects.</li> <li>Create and share comprehensive documentation of data, encompassing data schema, structure, and personally identifiable information (PII).</li> <li>Prevent data quality issues with standardized data quality profiling, providing visual understanding and warnings on potential issues.</li> <li>Accelerate data preparation with customizable recipes.</li> <li>Improve machine learning performance with optimal data preparation through solutions such as synthetic data.</li> <li>Shorten access to data with privacy-compliant synthetic data generatio.</li> <li>Build and streamline data preparation workflows effortlessly through a user-friendly drag-and-drop interface.</li> <li>Efficiently manage business rules, conduct comparisons, and implement version control for data workflows using pipelines.</li> </ul>"},{"location":"#key-features","title":"\ud83d\udcdd Key features","text":""},{"location":"#data-catalog","title":"Data Catalog","text":"<p>Fabric Data Catalog provides a centralized perspective on datasets within a project-basis, optimizing data management through seamless integration with the organization's existing data architectures via scalable connectors (e.g., MySQL, Google Cloud Storage, AWS S3). It standardizes data quality profiling, streamlining the processes of efficient data cleaning and preparation, while also automating the identification of Personally Identifiable Information (PII) to facilitate compliance with privacy regulations.</p> <p>Explore how a Data Catalog through a centralized repository of your datasets, schema validation, and automated data profiling.</p>"},{"location":"#labs","title":"Labs","text":"<p>Fabric's Labs environments provide collaborative, scalable, and secure workspaces layered on a flexible infrastructure, enabling users to seamlessly switch between CPUs and GPUs based on their computational needs. Labs are familiar environments that empower data developers with powerful IDEs (Jupyter Notebooks, Visual Code or H2O flow) and a seamless experience with the tools they already love combined with YData's cutting-edge SDK for data preparation.</p> <p>Learn how to use the Labs to generate synthetic data in a familiar Python interface.</p>"},{"location":"#synthetic-data","title":"Synthetic data","text":"<p>Synthetic data, enabled by YData Fabric, provides data developers with a user-friendly interfaces (UI and code) for generating artificial datasets, offering a versatile solution across formats like tabular, time-series and multi-table datasets. The generated synthetic data holds the same value of the original and aligns intricately with specific business rules, contributing to machine learning models enhancement, mitigation of privacy concerns and more robustness for data developments. Fabric offers synthetic data that is ease to adapt and configure, allows customization in what concerns privacy-utility trade-offs.</p> <p>Learn how you to create high-quality synthetic data within a user-friendly UI using Fabric\u2019s data synthesis flow.</p>"},{"location":"#pipelines","title":"Pipelines","text":"<p>Fabric Pipelines streamlines data preparation workflows by automating, orchestrating, and optimizing data pipelines, providing benefits such as flexibility, scalability, monitoring, and reproducibility for efficient and reliable data processing. The intuitive drag-and-drop interface, leveraging Jupyter notebooks or Python scripts, expedites the pipeline setup process, providing data developers with a quick and user-friendly experience.</p> <p>Explore how you can leverage Fabric Pipelines to build versionable and reproducible data preparation workflows for ML development.</p>"},{"location":"#tutorials","title":"Tutorials","text":"<p>To understand how to best apply Fabric to your use cases, start by exploring the following tutorials:</p> <ul> <li> <p>Handling Imbalanced Data for Improved Fraud DetectionLearn how to implement high-performant fraud detection models by incorporating synthetic data to balance your datasets.</p> </li> <li> <p>Prediction with Quality Inspection Learn how to develop data preparation workflows with automated data quality checks and Pipelines.</p> </li> <li> <p>Generating Synthetic Data for Financial TransactionsLearn how to use synthetic data generation to replicate your existing relational databases while ensuring referential integrity.</p> </li> </ul> <p>You can find additional examples and use cases at YData Academy GitHub Repository.</p>"},{"location":"#support","title":"\ud83d\ude4b Support","text":"<p>Facing an issue? We\u2019re committed to providing all the support you need to ensure a smooth experience using Fabric:</p> <ul> <li>Create a support ticket: our team will help you move forward!</li> <li>Contact a Fabric specialist: for personalized guidance or full access to the platform</li> </ul>"},{"location":"data_catalog/","title":"Data Catalog","text":"<p>In the realm of data management and analysis, the ability to efficiently discover, understand, and access data is crucial. Fabric's Data Catalog emerges as a pivotal solution in this context, designed to facilitate an organized, searchable, and accessible repository of metadata. This chapter introduces the concept, functionality, and advantages of the Data Catalog within Fabric's ecosystem, offering developers a comprehensive overview of its significance and utility.</p> <p>To ensure that large volumes of data can be processed through the entire data pipeline, Fabric is equipped with integrated connectors for various types of storages (from RDBMS to cloud object storage), guaranteeing the data never leaves your premises. Furthermore Fabric's Catalog ensures a timely and scalable data analysis as it runs on top of a distributed architecture powered by Kubernetes and Dask.</p> <p>The benefits of Fabric's Data Catalog for data teams are manifold, enhancing not only the efficiency but also the effectiveness of data understanding operations:</p> <ul> <li>Improved Data Accessibility: With the Data Catalog, developers can consume the data they need for a certain project through a user-friendly interface, significantly reducing the time spent searching for data across disparate sources. This enhanced discoverability makes it easier to initiate data analysis, machine learning projects,</li> <li> <p>or any other data-driven tasks.</p> </li> <li> <p>Enhanced Data Governance and Quality: Fabric's Data Catalog provides comprehensive tools for data-drive projects governance in terms of data assets, including data quality profiling and metadata management. These tools help maintain high-data quality and compliance with regulatory standards, ensuring that developers work with reliable and standardized information throughout the project.</p> </li> <li> <p>Knowledge and Insight Sharing: Through detailed metadata, data quality warnings and detailed profiling, Fabric's Data Catalog enhances the understanding of data's context and behaviour. This shared knowledge base supports better decision-making and innovation in a data-driven project.</p> </li> </ul>"},{"location":"data_catalog/#related-materials","title":"Related Materials","text":"<ul> <li>\ud83d\udcd6 Data Catalogs in the modern data stack</li> <li> How to create your first Datasource from a CSV file?</li> <li> How to create a Database in the Data Catalog?</li> <li> How to automate data quality profiling?</li> </ul>"},{"location":"data_catalog/connectors/","title":"Connectors","text":"<p>Fabric connectors play an important role in the landscape of data-driven projects, acting as essential components that facilitate the movement and integration of data across different systems, platforms, and applications. Fabric connectors where designe to offer a seamless and easy connectivity for data exchange between disparate data sources (such as databases, cloud storage systems, etc).</p>"},{"location":"data_catalog/connectors/#benefits","title":"Benefits","text":"<ul> <li>Data Integration: Fabric Connectors are primarily used to consume and integrate data a variety of different sources in a single project, ensuring that data can be easily combined, transformed, and made ready for analysis or operational use.</li> <li>Automation of data flows: They automate the process of data extraction, transformation and loading (ETL), which is crucial for maintaining up-to-date and accurate the data that is being used for a certain project.</li> <li>Simplification of data access: Fabric connectors experience simplify the process of accessing and using data from specialized or complex systems, making it easier for users without deep technical expertise to leverage data for insights.</li> <li>Enhancement of Data Security: Designed to manage in a secure way the credentials and access to your different storage.</li> </ul>"},{"location":"data_catalog/connectors/#get-started-with-fabric-connectors","title":"Get started with Fabric Connectors","text":"<ul> <li> How to create a connector in Fabric?</li> <li> How to use Object Storage Connectors through Labs?</li> <li> How to use RDBMS connectors through Labs?</li> </ul>"},{"location":"data_catalog/connectors/create_connector/","title":"How to create a connector in Fabric's Data Catalog?","text":"<p> How to create a connector to an RDBMS in Fabric?</p> <p>To create a connector in YData Fabric, select the \"Connectors\" page from the left side menu, as illustrated in the image below.</p> <p></p> <p>Click in \"Add Connector\" and a list of connector types to choose from will be shown to you.</p> <p></p> <p>For the purpose of this example we will be creating a connector to our AWS S3 storage. The credentials/secrets to your storage will be requested. After adding them, you can \"Test connection\" to ensure that all the details are correct. A confirmation message, similar to the one shown in the image below, should appear in our screen, letting you know that you can now save your connector successfully!</p> <p></p> <p>Congrats! \ud83d\ude80 You have now created your first Connector! You can now create different Datasources in your project's Data Catalog. Get ready for your journey of improved quality data for AI.</p>"},{"location":"data_catalog/connectors/supported_connections/","title":"Supported connections","text":"<p>Fabric can read and write data from a variety of data sources.</p>"},{"location":"data_catalog/connectors/supported_connections/#connectors","title":"Connectors","text":"<p>Here is the list of the available connectors in Fabric.</p> Connector Name Type Supported file types Notes AWS S3 Object Storage <code>Parquet</code> <code>CSV</code> Azure Blog Storage Object Storage <code>Parquet</code> <code>CSV</code> Azure Data Lake Object Storage <code>Parquet</code> <code>CSV</code> Google Cloud storage Object Storage <code>Parquet</code> <code>CSV</code> Upload file File <code>Parquet</code> <code>CSV</code> Maximum file size is 700MB. Bigger files should be uploaded and read from remote object storages Google BigQuery Big Table <code>Not applicable</code> MySQL RDBMS <code>Not applicable</code> Supports reading whole schemas or specifying a query Azure SQL Server RDBMS <code>Not applicable</code> Supports reading whole schemas or specifying a query PostGreSQL RDBMS <code>Not applicable</code> Supports reading whole schemas or specifying a query Snowflake RDBMS <code>Not applicable</code> Supports reading whole schemas or specifying a query Oracle DB RDBMS <code>Not applicable</code> Supports reading whole schemas or specifying a query"},{"location":"data_catalog/connectors/supported_connections/#havent-found-your-storage","title":"Haven't found your storage?","text":"<p>To understand our development roadmap or to request prioritization of new data connector, reach out to us at ydata.ai/contact-us.</p>"},{"location":"data_catalog/datasources/","title":"Overview","text":"<p>To enable a full understanding of the available data assets, Fabric further incorporates a module for Data Profiling, which allows you to further investigate the characteristics of your dataset more deeply, zooming in on the behavior and relationships between particular columns.</p> Profiling Large Datasets? <p>We've got you covered. Fabric Data Catalog offers an interactive, flexible, and intuitive experience when handling datasets with thousands of columns and any number of rows. Learn more about the benefits of Fabric in profiling high-dimensional datasets and sign up for the Community Version to experiment with your own data assets.</p> <p>The data profiling essentially enables the following analysis:</p> <ul> <li> <p>Univariate Analysis and Feature Statistics: Fabric incorporates type inference, automatically detecting the data types in a dataset. Depending on the column\u2019s data type, adjusted descriptive statistics are presented. The same applies for the visualizations chosen for each column.</p> </li> <li> <p>Multivariate Analysis and Correlation Assessment: To enable multivariate analysis and the evaluation of existing relationships between columns, Fabric includes informative visualizations regarding the interactions and correlations between columns, and the investigation of missing data and outliers.</p> </li> </ul> <p></p> <p>The data profiling highlights a set of statistical properties, such as:</p> <ul> <li>Variables Properties:<ul> <li>Descriptive statistics</li> <li>Quantile statistics</li> <li>Histogram, Common Values, and Extreme Values</li> </ul> </li> <li>Interactions and Correlations:<ul> <li>Heat maps and bar plot formats with interactive selection;</li> <li>Spearman\u2019s and Cramer\u2019s V analysis</li> </ul> </li> <li>Missing Values (MAR, MNAR, and MCAR):<ul> <li>Count and Matrix</li> </ul> </li> <li>Autoregressive and Stationarity Detection (Time Series Data)<ul> <li>ACF and PACF analysis</li> </ul> </li> <li>Text Analysis<ul> <li>Most occurring characters, words, categories, among others</li> </ul> </li> </ul> Profiling Sensitive Data? <p>By default, Fabric assumes that any data to be profile can contain sensitive information. For that reason, it includes several features to enable a secure and fair data profiling such as the aggregation of easily-identifiable groups and the obfuscation of values for categorical columns. Sign up for the Community Version and move towards a responsible exploration of your data.</p>"},{"location":"data_catalog/datasources/pii/","title":"Pii","text":"<p>To overcome the concerns around data privacy and enable secure data sharing, Fabric incorporates an automated Personal Identifiable Information (PII) identification engine to help detect and handle potential PII.</p> What can be considered Personal Identifiable Information (PII)? <p>PII is information that, when used alone or with other relevant data, can uniquely identify an individual. PII may contain direct indentifiers (e.g., ID, VAT, Credit Card Number) and/or quasi-identifiers (e.g., age, gender, race, occupation). Correctly classifying these is crucial to reduce the risk of re-identification. Learn more about how Fabric mitigates the risk of re-identification using synthetic data.</p> <p>Fabric offers a standardized classification of PII that automatically highlights and tags potential PII. The automatic detection of PII can be enabled during the loading process of your datasets and can be leveraged to generate privacy-preserving synthetic data.</p> <p></p> <p>After the detection, the PII information will be available through the Metadata &gt; PII Types, where each column that may represent potential PII is associated to one or several tags that identify the type of information it might be leaking.</p> <p></p> <p>You can review the automatic PII classification and add additional PII tags of your own by editing the metadata and select additional tags available in a pre-defined list of values, containing the most common types of potential PII information: email, phone, VAT, zip code, among others.</p> <p></p> Need a solution to enable data sharing and comply with GDPR and CCPA regulations? <p>Using synthetic data has proven to foster a culture of data-sharing within organizations, overcoming the limitations of traditional privacy methods and maximizing data value. Try Fabric Community Version to enable secure data sharing.</p>"},{"location":"data_catalog/datasources/warnings/","title":"Warnings","text":"<p>The first technical step in any data science project is to examine the data and understand its quality, value and fitness for purpose. For this reason,  Fabric\u2019s Data Catalog includes an Overview and Warnings module  for a better understanding of the available datasets.</p>"},{"location":"data_catalog/datasources/warnings/#overview","title":"Overview","text":"<p>When clicking on a Dataset available from the Data Catalog, it will show its details page, revealing an Overview and Warnings section.</p> <p></p> <p>In the Overview, you\u2019ll get a an overall perspective of your dataset\u2019s characteristics, where descriptive statistics will be presented, including:</p> <ul> <li>Basic description and tags/concepts associated to the dataset</li> <li>Memory consumption</li> <li>Number of rows</li> <li>Duplicate rows (percentage / number of records)</li> <li>Number of columns</li> <li>Total data types (numeric, categorical, string, long text, ID, date)</li> <li>Missing data (percentage / number of cells)</li> <li>Main data quality warnings</li> </ul>"},{"location":"data_catalog/datasources/warnings/#data-quality-warnings","title":"Data Quality Warnings","text":"<p>To enable data-centric development, Fabric automatically detects and signals potential data quality warnings. Warnings highlight certain peculiarities of data that might require further investigation prior to model development and deployment. However, the validity of each issued warning and whether follow-up mitigation work is needed will depend on the specific use case and on domain knowledge.</p> <p></p> <p>Fabric currently supports the following warnings:</p> <ul> <li>Constant: the column presents the same value for all observations</li> <li>Zeros:  the column presents the value \u201c0\u201d for several observations</li> <li>Unique: the column contains only unique/distinct values</li> <li>Cardinality: the columns (categorical) has a large number of distinct values</li> <li>Infinity: the column presents infinite (\\(\\inf\\)) values</li> <li>Constant_length: the column (text) has constant length</li> <li>Correlation: the columns is highly correlated with other(s)</li> <li>Skeweness: the column distribution (numerical) is skewed</li> <li>Missings: the column presents several missing values</li> <li>Non-stationarity: the column (time series) presents statistical properties that change through time</li> <li>Seasonal: the column (time series) exhibits a seasonal pattern</li> <li>Uniform: the column (numerical) follows a uniform distribution</li> <li>Imbalance: the column (categorical) presents a high imbalance ratio between existing categories</li> </ul> <p>Fabric further enables the interactive exploration of warnings, filtering over specific warnings and severity types (i.e., Moderate and High):</p> <p></p>"},{"location":"get-started/","title":"Get started with Fabric","text":"<p>The get started is here to help you if you are not yet familiar with YData Fabric or if you just want to learn more about data quality, data preparation workflows and how you can start leveraging synthetic data. Mention to YData Fabric Community</p>"},{"location":"get-started/#create-your-first-dataset-with-the-data-catalog","title":"\ud83d\udcda Create your first Dataset with the Data Catalog","text":""},{"location":"get-started/#create-your-multi-table-dataset-with-the-data-catalog","title":"\ud83d\udcbe Create your Multi-Table Dataset with the Data Catalog","text":""},{"location":"get-started/#create-your-first-synthetic-data-generator","title":"\u2699\ufe0f Create your first Synthetic Data generator","text":""},{"location":"get-started/#create-a-relational-database-synthetic-data-generator","title":"\ud83d\uddc4\ufe0f Create a Relational Database Synthetic Data generator","text":""},{"location":"get-started/#create-your-first-lab","title":"\ud83e\uddea Create your first Lab","text":""},{"location":"get-started/#create-your-first-data-pipeline","title":"\ud83c\udf00 Create your first data Pipeline","text":""},{"location":"get-started/create_database_sd_generator/","title":"How to create your first Relational Database Synthetic Data generator","text":"<p> Check this quickstart video on how to create your first Relational Database Synthetic Data generator.</p> <p>To generate your first synthetic relational database, you need to have a Multi-Dataset already available in your Data Catalog. Check this tutorial to see how you can add your first dataset to Fabric\u2019s Data Catalog.</p> <p>With your database created as a Datasource, you are now able to start configure your Synthetic Data (SD) generator to create a replicate of your database. You can either select \"Synthetic Data\" from your left side menu, or you can select \"Create Synthetic Data\" in your project Home as shown in the image below.</p> <p></p> <p>You'll be asked to select the dataset you wish to generate synthetic data from and verify the tables you'd like to include in the synthesis process, validating their data types - Time-series or Tabular.</p> <p>Table data types are relevant for synthetic data quality</p> <p>In case some of your tables hold time-series information (meaning there is a time relation between records) it is very important that during the process of configuring your synthetic data generator you do change update your tables data types accordingly. This will not only ensure the quality of that particular table, but also the overall database quality and relations.</p> <p></p> <p>All the PK and FK identified based on the database schema definition, have an automatically created anonymization setting defined. Aa standard and incremental integer will be used as the anonymization configuration, but user can change to other pre-defined generation options or regex base (user can provide the expected pattern of generation).</p> <p></p> <p>Finally, as the last step of our process it comes the Synthetic Data generator specific configurations, for this particular case we need to define both Display Name and the Destination connector. The Destination connector it is mandatory and allow to select the database where the generated synthetic database is expected to be written. After providing both inputs we can finish the process by clicking in the \"Save\" button as per the image below.</p> <p></p> <p>Your Synthetic Data generator is now training and listed under \"Synthetic Data\". While the model is being trained, the Status will be \ud83d\udfe1, as soon as the training is completed successfully it will transition to \ud83d\udfe2. Once the Synthetic Data generator has finished training, you're ready to start generating your first synthetic dataset. You can start by exploring an overview of the model configurations and even validate the quality of the synthetic data generator from a referential integrity point of view.</p> <p></p> <p>Next, you can generate synthetic data samples by accessing the Generation tab or click on \"Go to Generation\". In this section, you are able to generate as many synthetic samples as you want. For that you need to define the size of your database in comparison to the real one. This ratio is provided as a percentage. In the example below, we have asked a sample with 100% size, meaning, a synthetic database with the same size as the original.</p> <p></p> <p>A new line in your \"Sample History\" will be shown and as soon as the sample generation is completed you will be able to check the quality the synthetic data already available in your destination database.</p> <p>Congrats! \ud83d\ude80 You have now successfully created your first Relation Synthetic Database with Fabric. Get ready for your journey of improved quality data for AI.</p>"},{"location":"get-started/create_lab/","title":"How to create your first Lab environment","text":"<p>Labs are code environments for a more flexible development of data-driven solutions while leveraging Fabric capabilities combined with already loved tools such as scikit-learn, numpy and pandas. To create your first Lab, you can use the \u201cCreate Lab\u201d from Fabric\u2019s home, or you can access it from the Labs module by selecting it on the left side menu, and clicking the \u201cCreate Lab\u201d button.</p> <p></p> <p>Next, a menu with different IDEs will be shown. As a quickstart select Jupyter Lab. As labs are development environments you will be also asked what language you would prefer your environment to support: R or Python. Select Python.</p> Select IDE Select language <p>Bundles are environments with pre-installed packages. Select YData bundle, so we can leverage some other Fabric features such as Data Profiling, Synthetic Data and Pipelines.</p> <p></p> <p>As a last step, you will be asked to configure the infrastructure resources for this new environment as well as giving it a Display Name. We will keep the defaults, but you have flexibility to select GPU acceleration or whether you need more computational resources for your developments.</p> <p></p> <p>Finally, your Lab will be created and added to the \"Labs\" list, as per the image below. The status of the lab will be \ud83d\udfe1 while preparing, and this process takes a few minutes, as the infrastructure is being allocated to your development environment. As soon as the status changes to \ud83d\udfe2, you can open your lab by clicking in the button as shown below:</p> <p></p> <p>Create a new notebook in the JupyterLab and give it a name. You are now ready to start your developments!</p> Create a new notebook Notebook created <p>Congrats! \ud83d\ude80 You have now successfully created your first Lab a code environment, so you can benefit from the most advanced Fabric features as well as compose complex data workflows. Get ready for your journey of improved quality data for AI.</p>"},{"location":"get-started/create_multitable_dataset/","title":"How to create your first Relational database in Fabric's Catalog","text":"<p>To create your first multi-table dataset in the Data Catalog, you can start by clicking on \"Add Dataset\" from the Home section. Or click to Data Catalog (on the left side menu) and click \u201cAdd Dataset\u201d.</p> <p></p> <p>After that the below modal will be shown. You will need to select a connector. To create a multi-table dataset, we need to choose an RDBMS connector like Azure SQL, Snowflake or MySQL. In this case let's select MySQL.</p> <p></p> <p>Once you've selected the \u201cMySQL\u201d connector, a new screen will appear, enabling you to introduce the connection details such as database username, host, password as well as the database name.</p> <p></p> <p>With the Connector created, you'll be able to add a dataset and specify its properties:</p> <ul> <li>Name: The name of your dataset;</li> <li>Table: You can create a dataset with all the tables from the schema or select the tables that you need in your project.</li> <li>Query: Create a single table dataset by providing a query</li> </ul> <p></p> <p>Now both the Connector to the MySQL Berka database and Berka dataset will be added to our Catalog. As soon as the status is green, you can navigate your Dataset. Click in Open dataset as per the image below.</p> <p></p> <p>Within the Dataset details, you can gain valuable insights like your database schema.</p> <p></p> <p>For each an every table you can explore the both an overview on the structure (number of columns, number of rows, etc.) but also a useful summary of the quality and warnings regarding your dataset behaviour.</p> <p></p> <p>Congrats! \ud83d\ude80 You have now successfully created your first Connector and Multi-table Dataset in Fabric\u2019s Data Catalog. To get the both the ID of your database and project you can decompose the URL from the Database schema overview page. The structure is as follows:</p> <pre><code>    https://fabric.ydata.ai/rdbms/{your-dataset-id}?ns={your-project-id}\n</code></pre> <p>Get ready for your journey of improved quality data for AI.</p>"},{"location":"get-started/create_pipeline/","title":"How to create your first Pipeline","text":"<p> Check this quickstart video on how to create your first Pipeline.</p> <p>The best way to get started with Pipelines is to use the interactive Pipeline editor available in the Labs with Jupyter Lab set as IDE. If you don't have a Lab yet, or you don't know how to create one, check our quickstart guide on how to create your first lab.</p> <p>Open an already existing lab.</p> <p>A Pipeline comprises one or more nodes that are connected (or not!) with each other to define execution dependencies. Each pipeline node is and should be implemented as a component that is expected to manage a single task, such as read the data, profiling the data, training a model, or even publishing a model to production environments.</p> <p>In this tutorial we will build a simple and generic pipeline that use a Dataset from Fabric's Data Catalog and profile to check it's quality. We have the notebooks template already available. For that you need to access the \"Academy\" folder as per the image below.</p> <p></p> <p>Make sure to copy all the files in the folder \"3 - Pipelines/quickstart\" to the root folder of your lab, as per the image below.</p> <p></p> <p>Now that we have our notebooks we need to make a small change in the notebook \"1. Read dataset\". Go back to your Data Catalog, from one of the datasets in your Catalog list, select the three vertical dots and click in \"Explore in Labs\" as shown in the image below.</p> <p></p> <p>The following screen will be shown. Click in copy.</p> <p></p> <p>Now that we have copied the code, let's get back to our \"1. Read data.ipynb\" notebook, and replace the first code cell by with the new code. This will allow us to use a dataset from the Data Catalog in our pipeline.</p> Placeholder code Replaced with code snippet <p>With our notebooks ready, we can now configure our Pipeline. For this quickstart we will be leveraging an already existing pipeline - double-click the file my_first_pipeline.pipeline. You should see a pipeline as depicted in the images below. To create a new Pipeline, you can open the lab launcher tab and select \"Pipeline Editor\".</p> Open Pipeline My first pipeline <p>Before running the pipeline, we need to check each component/step properties and configurations. Right-click each one of the steps, select \"Open Properties\", and a menu will be depicted in your right side. Make sure that you have \"YData - CPU\" selected as the Runtime Image as show below.</p> Open properties Runtime image <p>We are now ready to create and run our first pipeline. In the top left corner of the pipeline editor, the run button will be available for you to click.</p> <p></p> <p>Accept the default values shown in the run dialog and start the run</p> <p></p> <p>If the following message is shown, it means that you have create a run of your first pipeline.</p> <p></p> <p>Now that you have created your first pipeline, you can select the Pipeline from Fabric's left side menu.</p> <p></p> <p>Your most recent pipeline will be listed, as shown in below image.</p> <p></p> <p>To check the run of your pipeline, jump into the \"Run\" tab. You will be able to see your first pipeline running!</p> <p></p> <p>By clicking on top of the record you will be able to see the progress of the run step-by-step, and visualize the outputs of each and every step by clicking on each step and selecting the Visualizations tab.</p> <p></p> <p>Congrats! \ud83d\ude80 You have now successfully created your first Pipeline a code environment, so you can benefit from Fabric's orchestration engine to crate scalable, versionable and comparable data workflows. Get ready for your journey of improved quality data for AI.</p>"},{"location":"get-started/create_syntheticdata_generator/","title":"How to create your first Synthetic Data generator","text":"<p> Check this quickstart video on how to create your first Synthetic Data generator.</p> <p>To generate your first synthetic data, you need to have a Dataset already available in your Data Catalog. Check this tutorial to see how you can add your first dataset to Fabric\u2019s Data Catalog.</p> <p>With your first dataset created, you are now able to start the creation of your Synthetic Data generator. You can either select \"Synthetic Data\" from your left side menu, or you can select \"Create Synthetic Data\" in your project Home as shown in the image below.</p> <p></p> <p>You'll be asked to select the dataset you wish to generate synthetic data from and verify the columns you'd like to include in the synthesis process, validating their Variable and Data Types.</p> <p>Data types are relevant for synthetic data quality</p> <p>Data Types are important to be revisited and aligned with the objectives for the synthetic data as they can highly impact the quality of the generated data. For example, let's say we have a column that is a \"Name\", while is some situations it would make sense to consider it a String, under the light of a dataset where \"Name\" refers to the name of the product purchases, it might be more beneficial to set it as a Category.</p> <p></p> <p>Finally, as the last step of our process it comes the Synthetic Data specific configurations, for this particular case we only need to define a Display Name, and we can finish the process by clicking in the \"Save\" button as per the image below.</p> <p></p> <p>Your Synthetic Data generator is now training and listed under \"Synthetic Data\". While the model is being trained, the Status will be \ud83d\udfe1, as soon as the training is completed successfully it will transition to \ud83d\udfe2 as per the image below.</p> <p></p> <p>Once the Synthetic Data generator has finished training, you're ready to start generating your first synthetic dataset. You can start by exploring an overview of the model configurations and even download a PDF report with a comprehensive overview of your Synthetic Data Quality Metrics. Next, you can generate synthetic data samples by accessing the Generation tab or click on \"Go to Generation\".</p> <p></p> <p>In this section, you are able to generate as many synthetic samples as you want. For that you need to define the number rows to generate and click \"Generate\", as depicted in the image below.</p> <p></p> <p>A new line in your \"Sample History\" will be shown and as soon as the sample generation is completed you will be able to \"Compare\" your synthetic data with the original data, add as a Dataset with \"Add to Data Catalog\" and last but not the least download it as a file with \"Download csv\".</p> <p></p> <p>Congrats! \ud83d\ude80 You have now successfully created your first Synthetic Data generator with Fabric. Get ready for your journey of improved quality data for AI.</p>"},{"location":"get-started/fabric_community/","title":"Get started with Fabric Community","text":"<p>Fabric Community is a SaaS version that allows you to explore all the functionalities of Fabric first-hand: free, forever, for everyone. You\u2019ll be able to validate your data quality with automated profiling, unlock data sharing and improve your ML models with synthetic data, and increase your productivity with seamless integration:</p> <ul> <li>Build 1 personal project;</li> <li>Create your first Data Catalog and benefit from automated data profiling;</li> <li>Train and generate synthetic data up to 2 models and datasets with 50 columns and 100K rows;</li> <li>Optimize synthetic data quality for your use cases with an evaluation PDF report;</li> <li>Create 1 development environment (Labs) and integrate it with your familiar ML packages and workflows.</li> </ul>"},{"location":"get-started/fabric_community/#register","title":"Register","text":"<p>To register for Fabric Community:</p> <ul> <li>Access the Fabric Community Try Now and create your YData account by submitting the form</li> <li>Check your email for your login credentials</li> <li>Login into fabric.ydata.ai and enjoy!</li> </ul> <p></p> <p>Once you login, you'll access the Home page and get started with your data preparation!</p> <p></p>"},{"location":"get-started/upload_csv/","title":"How to create your first Dataset from a CSV file","text":"<p> Check this quickstart video on how to create your first Dataset from a CSV file.</p> <p>To create your first dataset in the Data Catalog, you can start by clicking on \"Add Dataset\" from the Home section. Or click to Data Catalog (on the left side menu) and click \u201cAdd Dataset\u201d.</p> <p></p> <p>After that the below modal will be shown. You will need to select a connector. To upload a CSV file, we need to select \u201cUpload CSV\u201d.</p> <p></p> <p>Once you've selected the \u201cUpload CSV\u201d connector, a new screen will appear, enabling you to upload your file and designate a name for your connector. This file upload connector will subsequently empower you to create one or more datasets from the same file at a later stage.</p> Loading area Upload csv file <p>With the Connector created, you'll be able to add a dataset and specify its properties:</p> <ul> <li>Name: The name of your dataset;</li> <li>Separator: This is an important parameter to make sure that we can parse your CSV correctly. The default value is \u201c,\u201d.</li> <li>Data Type: Whether your dataset contains tabular or time-series (i.e., containing temporal dependency) data.</li> </ul> <p></p> <p>Your created Connector (\u201cCensus File\u201d) and Dataset (\u201cCensus\u201d) will be added to the Data Catalog. As soon as the status is green, you can navigate your Dataset. Click in Open Dataset as per the image below.</p> <p></p> <p>Within the Dataset details, you can gain valuable insights through our automated data quality profiling. This includes comprehensive metadata and an overview of your data, encompassing details like row count, identification of duplicates, and insights into the overall quality of your dataset.</p> <p></p> <p>Or perhaps, you want to further explore through visualization, the profile of your data with both univariate and multivariate of your data.</p> <p></p> <p>Congrats! \ud83d\ude80 You have now successfully created your first Connector and Dataset in Fabric\u2019s Data Catalog. Get ready for your journey of improved quality data for AI.</p>"},{"location":"sdk/","title":"Overview","text":"<p>Fabric SDK for improved data quality everywhere!</p> <p>ydata-sdk is here! Create a YData account so you can start using today!</p> <p>Create account</p>"},{"location":"sdk/#overview","title":"Overview","text":"<p>The Fabric SDK is an ecosystem of methods that allows users to, through a python interface, adopt data development focused on improving the quality of the data. The solution includes a set of integrated components for data ingestion, standardized data quality evaluation and data improvement, such as synthetic data generation, allowing an iterative improvement of the datasets used in high-impact business applications.</p>"},{"location":"sdk/#benefits","title":"Benefits","text":"<p>Fabric SDK interface enables the ability to integrate data quality tooling with other platforms offering several beneficts in the realm of data science development and data management:</p> <ul> <li>Interoperability: seamless integration with other data platform and systems like Databricks, Snowflake, etc. This ensures that all your software will work cohesively with all the elements from your data architecture.</li> <li>Collaboration: ease of integration with a multitude of tools and services, reducing the need to reinvent the wheel and fostering a collaborative environment for all developers (data scientists, data engineers, software developers, etc.)</li> <li>Improved usage experience: Fabric SDK enables a well-integrated software solution, which allows a seamless transition between different tools or platforms without facing compatibility issues.</li> </ul>"},{"location":"sdk/#current-functionality","title":"Current functionality","text":"<p>Fabric SDK is currently composed by the following main modules:</p> <ul> <li> <p>Datasources</p> <ul> <li>YData\u2019s SDK includes several connectors for easy integration with existing data sources. It supports several storage types, like filesystems and RDBMS. Check the list of connectors.</li> <li>SDK\u2019s Datasources run on top of Dask, which allows it to deal with not only small workloads but also larger volumes of data.</li> </ul> </li> <li> <p>Synthetic data generators</p> <ul> <li>Simplified interface to train a generative model and learn in a data-driven manner the behavior, the patterns and original data distribution. Optimize your model for privacy or utility use-cases.</li> <li>From a trained synthetic data generator, you can generate synthetic samples as needed and parametrise the number of records needed.</li> <li>Anonymization and privacy preserving capabilities to ensure that synthetic datasets does not contain Personal Identifiable Information (PII) and can safely be shared!</li> <li>Conditional sampling can be used to restrict the domain and values of specific features in the sampled data.</li> </ul> </li> <li> <p>Synthetic data quality report Coming soon</p> <ul> <li>An extensive synthetic data quality report that measures 3 dimensions: privacy, utility and fidelity of the generated data. The report can be downloaded in PDF format for ease of sharing and compliance purposes or as a JSON to enable the integration in data flows.</li> </ul> </li> <li> <p>Profiling Coming soon</p> <ul> <li>A set of metrics and algorithms summarizes datasets quality in three main dimensions: warnings, univariate analysis and a multivariate perspective.</li> </ul> </li> </ul>"},{"location":"sdk/#supported-data-formats","title":"Supported data formats","text":"TabularTime-SeriesTransactionalRelational databases <p> The RegularSynthesizer is perfect to synthesize high-dimensional data, that is time-indepentent with high quality results.</p> <p>Know more</p> <p> The TimeSeriesSynthesizer is perfect to synthesize both regularly and not evenly spaced time-series, from smart-sensors to stock.</p> <p>Know more</p> <p> The TimeSeriesSynthesizer supports transactional data, known to have highly irregular time intervals between records and directional relations between entities.</p> <p>Coming soon</p> <p>Know more</p> <p> The MultiTableSynthesizer is perfect to learn how to replicate the data within a relational database schema.</p> <p>Know more</p>"},{"location":"sdk/installation/","title":"Installation","text":"<p>YData SDK is generally available through both Pypi and Conda allowing an easy process of installation. This experience allows combining YData SDK with other packages such as Pandas, Numpy or Scikit-Learn.</p> <p>YData SDK is available for the public through a token-based authentication system. If you don\u2019t have one yet, you can get your free license key during the installation process. You can check what features are available in the free version here.</p>"},{"location":"sdk/installation/#installing-the-package","title":"Installing the package","text":"<p>YData SDK supports python versions bigger than python 3.8, and can be installed in Windows, Linux or MacOS operating systems.</p> <p>Prior to the package installation, it is recommended the creation of a virtual or conda environment:</p> pyenv <pre><code>pyenv virtualenv 3.10 ydatasdk\n</code></pre> <p>And install <code>ydata-sdk</code></p> pypi <pre><code>pip install ydata-sdk\n</code></pre>"},{"location":"sdk/installation/#authentication","title":"Authentication","text":"<p>Once you've installed <code>ydata-sdk</code> package you will need a token to run the functionalities. YData SDK uses a token based authentication system. To get access to your token, you need to create a YData account.</p> <p>YData SDK offers a free-trial and an enterprise version. To access your free-trial token, you need to create a YData account.</p> <p>The token will be available here, after login:</p> <p></p> <p>With your account toke copied, you can set a new environment variable <code>YDATA_TOKEN</code> in the beginning of your development session.</p> <pre><code>    import os\nos.setenv['YDATA_TOKEN'] = '{add-your-token}'\n</code></pre> <p>Once you have set your token, you are good to go to start exploring the incredible world of data-centric AI and smart synthetic data generation!</p> <p>Check out our quickstart guide!</p>"},{"location":"sdk/quickstart/","title":"Quickstart","text":"<p>YData SDK allows you to with an easy and familiar interface, to adopt a Data-Centric AI approach for the development of Machine Learning solutions. YData SDK features were designed to support structure data, including tabular data, time-series and transactional data.</p>"},{"location":"sdk/quickstart/#read-data","title":"Read data","text":"<p>To start leveraging the package features you should consume your data either through the Connectors or pandas.Dataframe. The list of available connectors can be found here [add a link].</p> From pandas dataframeFrom a connector <pre><code>    # Example for a Google Cloud Storage Connector\ncredentials = \"{insert-credentials-file-path}\"\n# We create a new connector for Google Cloud Storage\nconnector = Connector(connector_type='gcs', credentials=credentials)\n# Create a Datasource from the connector\n# Note that a connector can be re-used for several datasources\nX = DataSource(connector=connector, path='gs://&lt;my_bucket&gt;.csv')\n</code></pre> <pre><code>    # Load a small dataset\nX = pd.read_csv('{insert-file-path.csv}')\n# Init a synthesizer\nsynth = RegularSynthesizer()\n# Train the synthesizer with the pandas Dataframe as input\n# The data is then sent to the cluster for processing\nsynth.fit(X)\n</code></pre> <p>The synthesis process returns a <code>pandas.DataFrame</code> object. Note that if you are using the <code>ydata-sdk</code> free version, all of your data is sent to a remote cluster on YData's infrastructure.</p>"},{"location":"sdk/quickstart/#data-synthesis-flow","title":"Data synthesis flow","text":"<p>The process of data synthesis can be described into the following steps:</p> <pre><code>stateDiagram-v2\n  state read_data\n  read_data --&gt; init_synth\n  init_synth --&gt; train_synth\n  train_synth --&gt; generate_samples\n  generate_samples --&gt; [*]</code></pre> <p>The code snippet below shows how easy can be to start generating new synthetic data. The package includes a set of examples datasets for a quickstart.</p> <pre><code>    from ydata.sdk.dataset import get_dataset\n#read the example data\nX = get_dataset('census')\n# Init a synthesizer\nsynth = RegularSynthesizer()\n# Fit the synthesizer to the input data\nsynth.fit(X)\n# Sample new synthetic data. The below request ask for new 1000 synthetic rows\nsynth.sample(n_samples=1000)\n</code></pre> <p>Do I need to prepare my data before synthesis?</p> <p>The sdk ensures that the original behaviour is replicated. For that reason, there is no need to preprocess outlier observations or missing data.</p> <p>By default all the missing data is replicated as NaN.</p>"},{"location":"sdk/examples/synthesize_tabular_data/","title":"Synthesize tabular data","text":"<p>Use YData's RegularSynthesizer to generate tabular synthetic data</p> <pre><code>import os\nfrom ydata.sdk.dataset import get_dataset\nfrom ydata.sdk.synthesizers import RegularSynthesizer\n# Do not forget to add your token as env variables\nos.environ[\"YDATA_TOKEN\"] = '&lt;TOKEN&gt;'  # Remove if already defined\ndef main():\n\"\"\"In this example, we demonstrate how to train a synthesizer from a pandas\n    DataFrame.\n    After training a Regular Synthesizer, we request a sample.\n    \"\"\"\nX = get_dataset('census')\n# We initialize a regular synthesizer\n# As long as the synthesizer does not call `fit`, it exists only locally\nsynth = RegularSynthesizer()\n# We train the synthesizer on our dataset\nsynth.fit(X)\n# We request a synthetic dataset with 50 rows\nsample = synth.sample(n_samples=50)\nprint(sample.shape)\nif __name__ == \"__main__\":\nmain()\n</code></pre>"},{"location":"sdk/examples/synthesize_timeseries_data/","title":"Synthesize time-series data","text":"<p>Use YData's TimeSeriesSynthesizer to generate time-series synthetic data</p> <p>Tabular data is the most common type of data we encounter in data problems.</p> <p>When thinking about tabular data, we assume independence between different records, but this does not happen in reality. Suppose we check events from our day-to-day life, such as room temperature changes, bank account transactions, stock price fluctuations, and air quality measurements in our neighborhood. In that case, we might end up with datasets where measures and records evolve and are related through time. This type of data is known to be sequential or time-series data.</p> <p>Thus, sequential or time-series data refers to any data containing elements ordered into sequences in a structured format. Dissecting any time-series dataset, we see differences in variables' behavior that need to be understood for an effective generation of synthetic data. Typically any time-series dataset is composed of the following:</p> <ul> <li>Variables that define the order of time (these can be simple with one variable or composed)</li> <li>Time-variant variables</li> <li>Variables that refer to entities (single or multiple entities)</li> <li>Variables that are attributes (those that don't depend on time but rather on the entity)</li> </ul> <p>Below find an example:</p> <pre><code>import os\nfrom ydata.sdk.dataset import get_dataset\nfrom ydata.sdk.synthesizers import TimeSeriesSynthesizer\n# Do not forget to add your token as env variable\nos.environ[\"YDATA_TOKEN\"] = '&lt;TOKEN&gt;'\nX = get_dataset('occupancy')\n# We initialize a time series synthesizer\n# As long as the synthesizer does not call `fit`, it exists only locally\nsynth = TimeSeriesSynthesizer()\n# We train the synthesizer on our dataset\n# sortbykey -&gt; variable that define the time order for the sequence\nsynth.fit(X, sortbykey='date')\n# By default it is requested a synthetic sample with the same length as the original data\n# The TimeSeriesSynthesizer is designed to replicate temporal series and therefore the original time-horizon is respected\nsample = synth.sample(n_entities=1)\n</code></pre>"},{"location":"sdk/examples/synthesize_with_anonymization/","title":"Anonymization","text":"<p>YData Synthesizers offers a way to anonymize sensitive information such that the original values are not present in the synthetic data but replaced by fake values.</p> <p>Does the model retain the original values?</p> <p>No! The anonymization is performed before the model training such that it never sees the original values.</p> <p>The anonymization is performed by specifying which columns need to be anonymized and how to perform the anonymization. The anonymization rules are defined as a dictionary with the following format:</p> <p><code>{column_name: anonymization_rule}</code></p> <p>While here are some predefined anonymization rules such as <code>name</code>, <code>email</code>, <code>company</code>, it is also possible to create a rule using a regular expression. The anonymization rules have to be passed to a synthesizer in its <code>fit</code> method using the parameter <code>anonymize</code>.</p> <p>What is the difference between anonymization and privacy?</p> <p>Anonymization makes sure sensitive information are hidden from the data. Privacy makes sure it is not possible to infer the original data points from the synthetic data points via statistical attacks.</p> <p>Therefore, for data sharing anonymization and privacy controls are complementary.</p> <p>The example below demonstrates how to anonymize the column <code>Name</code> by fake names and the column <code>Ticket</code> by a regular expression: <pre><code>import os\nfrom ydata.sdk.dataset import get_dataset\nfrom ydata.sdk.synthesizers import RegularSynthesizer\n# Do not forget to add your token as env variables\nos.environ[\"YDATA_TOKEN\"] = '&lt;TOKEN&gt;'  # Remove if already defined\ndef main():\n\"\"\"In this example, we demonstrate how to train a synthesizer from a pandas\n    DataFrame.\n    After training a Regular Synthesizer, we request a sample.\n    \"\"\"\nX = get_dataset('titanic')\n# We initialize a regular synthesizer\n# As long as the synthesizer does not call `fit`, it exists only locally\nsynth = RegularSynthesizer()\n# We define anonymization rules, which is a dictionary with format:\n# {column_name: anonymization_rule, ...}\n# while here are some predefined anonymization rules like: name, email, company\n# it is also possible to create a rule using a regular expression\nrules = {\n\"Name\": \"name\",\n\"Ticket\": \"[A-Z]{2}-[A-Z]{4}\"\n}\n# We train the synthesizer on our dataset\nsynth.fit(\nX,\nname=\"titanic_synthesizer\",\nanonymize=rules\n)\n# We request a synthetic dataset with 50 rows\nsample = synth.sample(n_samples=50)\nprint(sample[[\"Name\", \"Ticket\"]].head(3))\nif __name__ == \"__main__\":\nmain()\n</code></pre></p>"},{"location":"sdk/examples/synthesize_with_conditional_sampling/","title":"Conditional sampling","text":"<p>YData Synthesizers support conditional sampling. The <code>fit</code> method has an optional parameter named <code>condition_on</code>, which receives a list of features to condition upon. Furthermore, the <code>sample</code> method receives the conditions to be applied through another optional parameter also named <code>condition_on</code>. For now, two types of conditions are supported:</p> <ul> <li>Condition upon a categorical (or string) feature. The parameters are the name of the feature and a list of values (i.e., categories) to be considered. Each category also has its percentage of representativeness. For example, if we want to condition upon two categories, we need to define the percentage of rows each of these categories will have on the synthetic dataset. Naturally, the sum of such percentages needs to be 1. The default percentage is also 1 since it is the required value for a single category.</li> <li>Condition upon a numerical feature. The parameters are the name of the feature and the minimum and maximum of the range to be considered. This feature will present a uniform distribution on the synthetic dataset, limited by the specified range.</li> </ul> <p>The example below demonstrates how to train and sample from a synthesizer using conditional sampling:</p> <pre><code>import os\nfrom ydata.sdk.dataset import get_dataset\nfrom ydata.sdk.synthesizers import RegularSynthesizer\n# Do not forget to add your token as env variables.\nos.environ[\"YDATA_TOKEN\"] = '&lt;TOKEN&gt;'  # Remove if already defined.\ndef main():\n\"\"\"In this example, we demonstrate how to train and\n    sample from a synthesizer using conditional sampling.\"\"\"\nX = get_dataset('census')\n# We initialize a regular synthesizer.\n# As long as the synthesizer does not call `fit`, it exists only locally.\nsynth = RegularSynthesizer()\n# We train the synthesizer on our dataset setting\n# the features to condition upon.\nsynth.fit(\nX,\nname=\"census_synthesizer\",\ncondition_on=[\"sex\", \"native-country\", \"age\"]\n)\n# We request a synthetic dataset with specific condition rules.\nsample = synth.sample(\nn_samples=500,\ncondition_on={\n\"sex\": {\n\"categories\": [\"Female\"]\n},\n\"native-country\": {\n\"categories\": [(\"United-States\", 0.6),\n(\"Mexico\", 0.4)]\n},\n\"age\": {\n\"minimum\": 55,\n\"maximum\": 60\n}\n}\n)\nprint(sample)\nif __name__ == \"__main__\":\nmain()\n</code></pre>"},{"location":"sdk/examples/synthesize_with_privacy_control/","title":"Privacy control","text":"<p>YData Synthesizers offers 3 different levels of privacy:</p> <ol> <li>high privacy: the model is optimized for privacy purposes,</li> <li>high fidelity (default): the model is optimized for high fidelity,</li> <li>balanced: tradeoff between privacy and fidelity.</li> </ol> <p>The default privacy level is high fidelity. The privacy level can be changed by the user at the moment a synthesizer level is trained by using the parameter <code>privacy_level</code>. The parameter expect a <code>PrivacyLevel</code> value.</p> <p>What is the difference between anonymization and privacy?</p> <p>Anonymization makes sure sensitive information are hidden from the data. Privacy makes sure it is not possible to infer the original data points from the synthetic data points via statistical attacks.</p> <p>Therefore, for data sharing anonymization and privacy controls are complementary.</p> <p>The example below demonstrates how to train a synthesizer configured for high privacy:</p> <pre><code>import os\nfrom ydata.sdk.dataset import get_dataset\nfrom ydata.sdk.synthesizers import PrivacyLevel, RegularSynthesizer\n# Do not forget to add your token as env variables\nos.environ[\"YDATA_TOKEN\"] = '&lt;TOKEN&gt;'  # Remove if already defined\ndef main():\n\"\"\"In this example, we demonstrate how to train a synthesizer\n    with a high-privacy setting from a pandas DataFrame.\n    After training a Regular Synthesizer, we request a sample.\n    \"\"\"\nX = get_dataset('titanic')\n# We initialize a regular synthesizer\n# As long as the synthesizer does not call `fit`, it exists only locally\nsynth = RegularSynthesizer()\n# We train the synthesizer on our dataset setting the privacy level to high\nsynth.fit(\nX,\nname=\"titanic_synthesizer\",\nprivacy_level=PrivacyLevel.HIGH_PRIVACY\n)\n# We request a synthetic dataset with 50 rows\nsample = synth.sample(n_samples=50)\nprint(sample)\nif __name__ == \"__main__\":\nmain()\n</code></pre>"},{"location":"sdk/examples/synthesizer_multitable/","title":"Synthesize Relational databases","text":"<p>Integrate Fabric's MultiTableSynthesizer in your data flows and generate synthetic relational databases or multi-table datasets</p> <p>The capability to generate synthetic data from relational databases is a powerful and innovative approach to streamline the access to data and improve data democratization strategy within the organization. Fabric's SDK makes available an easy-to-use code interface to integrate the process of generating synthetic multi-table databases into your existing data flows.</p> <p>How to get your datasource?</p> <p>Learn how to create your multi-table data in Fabric here before creating your first multi-table synthetic data generator!</p> <p>Get your datasource and connector ID</p> <p>Datasource uid: You can find your datasource ID through Fabric UI. Open your relational dataset and click in the \"Explore in Labs\" button. Copy the uid that you find available in the code snippet.</p> <p>Connector uid: You can find your connector ID through Fabric UI. Open the connector tab from your Data Catalog. Under the connector \"Actions\" select \"Explore in Lab\". Copy the uid available in the code snippet.</p> <p>Quickstart example:</p> <pre><code>import os\nfrom ydata.sdk.datasources import DataSource\nfrom ydata.sdk.synthesizers import MultiTableSynthesizer\n# Authenticate to Fabric to leverage the SDK - https://docs.sdk.ydata.ai/latest/sdk/installation/\n# Make sure to add your token as env variable.\nos.environ[\"YDATA_TOKEN\"] = '&lt;TOKEN&gt;'  # Remove if already defined\n# In this example, we demonstrate how to train a synthesizer from an existing RDBMS Dataset.\n# Make sure to follow the step-by-step guide to create a Dataset in Fabric's catalog: https://docs.sdk.ydata.ai/latest/get-started/create_multitable_dataset/\nX = DataSource.get('&lt;DATASOURCE_UID&gt;')\n# Init a multi-table synthesizer. Provide a connector so that the process of data synthesis write the\n# synthetic data into the destination database\n# Provide a connector ID as the write_connector argument. See in this tutorial how to get a connector ID\nsynth = MultiTableSynthesizer(write_connector='&lt;CONNECTOR_UID')\n# Start the training of your synthetic data generator\nsynth.fit(X)\n# As soon as the training process is completed you are able to sample a synthetic database\n# The input expected is a percentage of the original database size\n# In this case it was requested a synthetic database with the same size as the original\n# Your synthetic sample was written to the database provided in the write_connector\nsynth.sample(frac=1.)\n</code></pre>"},{"location":"sdk/modules/connectors/","title":"Connectors","text":"<p>YData SDK allows users to consume data assets from remote storages through Connectors. YData Connectors support different types of storages, from filesystems to RDBMS'.</p> <p>Below the list of available connectors:</p> Connector Name Type Supported File Types Useful Links Notes AWS S3 Remote object storage CSV, Parquet https://aws.amazon.com/s3/ Google Cloud Storage Remote object storage CSV, Parquet https://cloud.google.com/storage Azure Blob Storage Remote object storage CSV, Parquet https://azure.microsoft.com/en-us/services/storage/blobs/ File Upload Local CSV - Maximum file size is 220MB. Bigger files should be uploaded and read from remote object storages MySQL RDBMS Not applicable https://www.mysql.com/ Supports reading whole schemas or specifying a query Azure SQL Server RDBMS Not applicable https://azure.microsoft.com/en-us/services/sql-database/campaign/ Supports reading whole schemas or specifying a query PostgreSQL RDBMS Not applicable https://www.postgresql.org/ Supports reading whole schemas or specifying a query Snowflake RDBMS Not applicable https://docs.snowflake.com/en/sql-reference-commands Supports reading whole schemas or specifying a query Google BigQuery Data warehouse Not applicable https://cloud.google.com/bigquery Azure Data Lake Data lake CSV, Parquet https://azure.microsoft.com/en-us/services/storage/data-lake-storage/ <p>More details can be found at Connectors APi Reference Docs.</p>"},{"location":"sdk/modules/synthetic_data/","title":"Synthetic data generation","text":""},{"location":"sdk/modules/synthetic_data/#data-formats","title":"Data formats","text":""},{"location":"sdk/modules/synthetic_data/#tabular-data","title":"Tabular data","text":""},{"location":"sdk/modules/synthetic_data/#time-series-data","title":"Time-series data","text":""},{"location":"sdk/modules/synthetic_data/#transactions-data","title":"Transactions data","text":""},{"location":"sdk/modules/synthetic_data/#best-practices","title":"Best practices","text":""},{"location":"sdk/reference/api/common/client/","title":"Get client","text":"<p>Deduce how to initialize or retrieve the client.</p> <p>This is meant to be a zero configuration for the user.</p> Create and set a client globally <pre><code>from ydata.sdk.client import get_client\nget_client(set_as_global=True)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>client_or_creds</code> <code>Optional[Union[Client, dict, str, Path]]</code> <p>Client to forward or credentials for initialization</p> <code>None</code> <code>set_as_global</code> <code>bool</code> <p>If <code>True</code>, set client as global</p> <code>False</code> <code>wait_for_auth</code> <code>bool</code> <p>If <code>True</code>, wait for the user to authenticate</p> <code>True</code> <p>Returns:</p> Type Description <code>Client</code> <p>Client instance</p> Source code in <code>ydata/sdk/common/client/utils.py</code> <pre><code>def get_client(client_or_creds: Optional[Union[Client, Dict, str, Path]] = None, set_as_global: bool = False, wait_for_auth: bool = True) -&gt; Client:\n\"\"\"Deduce how to initialize or retrieve the client.\n    This is meant to be a zero configuration for the user.\n    Example: Create and set a client globally\n            ```py\n            from ydata.sdk.client import get_client\n            get_client(set_as_global=True)\n            ```\n    Args:\n        client_or_creds (Optional[Union[Client, dict, str, Path]]): Client to forward or credentials for initialization\n        set_as_global (bool): If `True`, set client as global\n        wait_for_auth (bool): If `True`, wait for the user to authenticate\n    Returns:\n        Client instance\n    \"\"\"\nclient = None\nglobal WAITING_FOR_CLIENT\ntry:\n# If a client instance is set globally, return it\nif not set_as_global and Client.GLOBAL_CLIENT is not None:\nreturn Client.GLOBAL_CLIENT\n# Client exists, forward it\nif isinstance(client_or_creds, Client):\nreturn client_or_creds\n# Explicit credentials\n''' # For the first version, we deactivate explicit credentials via string or file for env var only\n        if isinstance(client_or_creds, (dict, str, Path)):\n            if isinstance(client_or_creds, str):  # noqa: SIM102\n                if Path(client_or_creds).is_file():\n                    client_or_creds = Path(client_or_creds)\n            if isinstance(client_or_creds, Path):\n                client_or_creds = json.loads(client_or_creds.open().read())\n            return Client(credentials=client_or_creds)\n        # Last try with environment variables\n        #if client_or_creds is None:\n        client = _client_from_env(wait_for_auth=wait_for_auth)\n        '''\ncredentials = environ.get(TOKEN_VAR)\nif credentials is not None:\nclient = Client(credentials=credentials)\nexcept ClientHandshakeError as e:\nwait_for_auth = False  # For now deactivate wait_for_auth until the backend is ready\nif wait_for_auth:\nWAITING_FOR_CLIENT = True\nstart = time()\nlogin_message_printed = False\nwhile client is None:\nif not login_message_printed:\nprint(\nf\"The token needs to be refreshed - please validate your token by browsing at the following URL:\\n\\n\\t{e.auth_link}\")\nlogin_message_printed = True\nwith suppress(ClientCreationError):\nsleep(BACKOFF)\nclient = get_client(wait_for_auth=False)\nnow = time()\nif now - start &gt; CLIENT_INIT_TIMEOUT:\nWAITING_FOR_CLIENT = False\nbreak\nif client is None and not WAITING_FOR_CLIENT:\nsys.tracebacklimit = None\nraise ClientCreationError\nreturn client\n</code></pre> <p>Main Client class used to abstract the connection to the backend.</p> <p>A normal user should not have to instanciate a <code>Client</code> by itself. However, in the future it will be useful for power-users to manage projects and connections.</p> <p>Parameters:</p> Name Type Description Default <code>credentials</code> <code>Optional[dict]</code> <p>(optional) Credentials to connect</p> <code>None</code> <code>project</code> <code>Optional[Project]</code> <p>(optional) Project to connect to. If not specified, the client will connect to the default user's project.</p> <code>None</code> Source code in <code>ydata/sdk/common/client/client.py</code> <pre><code>@typechecked\nclass Client(metaclass=SingletonClient):\n\"\"\"Main Client class used to abstract the connection to the backend.\n    A normal user should not have to instanciate a [`Client`][ydata.sdk.common.client.Client] by itself.\n    However, in the future it will be useful for power-users to manage projects and connections.\n    Args:\n        credentials (Optional[dict]): (optional) Credentials to connect\n        project (Optional[Project]): (optional) Project to connect to. If not specified, the client will connect to the default user's project.\n    \"\"\"\ncodes = codes\nDEFAULT_PROJECT: Optional[Project] = environ.get(\"DEFAULT_PROJECT\", None)\ndef __init__(self, credentials: Optional[Union[str, Dict]] = None, project: Optional[Project] = None, set_as_global: bool = False):\nself._base_url = environ.get(\"YDATA_BASE_URL\", DEFAULT_URL).removesuffix('/')\nself._headers = {'Authorization': credentials}\nself._http_client = httpClient(\nheaders=self._headers, timeout=Timeout(10, read=None))\nself._handshake()\nself._default_project = project or Client.DEFAULT_PROJECT or self._get_default_project(\ncredentials)\nif set_as_global:\nself.__set_global()\n@property\ndef project(self) -&gt; Project:\nreturn Client.DEFAULT_PROJECT or self._default_project\n@project.setter\ndef project(self, value: Project):\nself._default_project = value\ndef post(\nself, endpoint: str, content: Optional[RequestContent] = None, data: Optional[Dict] = None,\njson: Optional[Dict] = None, project: Optional[Project] = None, files: Optional[Dict] = None,\nraise_for_status: bool = True\n) -&gt; Response:\n\"\"\"POST request to the backend.\n        Args:\n            endpoint (str): POST endpoint\n            content (Optional[RequestContent])\n            data (Optional[dict]): (optional) multipart form data\n            json (Optional[dict]): (optional) json data\n            files (Optional[dict]): (optional) files to be sent\n            raise_for_status (bool): raise an exception on error\n        Returns:\n            Response object\n        \"\"\"\nurl_data = self.__build_url(\nendpoint, data=data, json=json, files=files, project=project)\nresponse = self._http_client.post(**url_data)\nif response.status_code != Client.codes.OK and raise_for_status:\nself.__raise_for_status(response)\nreturn response\ndef patch(\nself, endpoint: str, content: Optional[RequestContent] = None, data: Optional[Dict] = None,\njson: Optional[Dict] = None, project: Optional[Project] = None, files: Optional[Dict] = None,\nraise_for_status: bool = True\n) -&gt; Response:\n\"\"\"PATCH request to the backend.\n        Args:\n            endpoint (str): POST endpoint\n            content (Optional[RequestContent])\n            data (Optional[dict]): (optional) multipart form data\n            json (Optional[dict]): (optional) json data\n            files (Optional[dict]): (optional) files to be sent\n            raise_for_status (bool): raise an exception on error\n        Returns:\n            Response object\n        \"\"\"\nurl_data = self.__build_url(\nendpoint, data=data, json=json, files=files, project=project)\nresponse = self._http_client.patch(**url_data, content=content)\nif response.status_code != Client.codes.OK and raise_for_status:\nself.__raise_for_status(response)\nreturn response\ndef get(\nself, endpoint: str, params: Optional[Dict] = None, project: Optional[Project] = None,\ncookies: Optional[Dict] = None, raise_for_status: bool = True\n) -&gt; Response:\n\"\"\"GET request to the backend.\n        Args:\n            endpoint (str): GET endpoint\n            cookies (Optional[dict]): (optional) cookies data\n            raise_for_status (bool): raise an exception on error\n        Returns:\n            Response object\n        \"\"\"\nurl_data = self.__build_url(endpoint, params=params,\ncookies=cookies, project=project)\nresponse = self._http_client.get(**url_data)\nif response.status_code != Client.codes.OK and raise_for_status:\nself.__raise_for_status(response)\nreturn response\ndef get_static_file(\nself, endpoint: str, project: Optional[Project] = None, raise_for_status: bool = True\n) -&gt; Response:\n\"\"\"Retrieve a static file from the backend.\n        Args:\n            endpoint (str): GET endpoint\n            raise_for_status (bool): raise an exception on error\n        Returns:\n            Response object\n        \"\"\"\nurl_data = self.__build_url(endpoint, project=project)\nurl_data['url'] = f'{self._base_url}/static-content{endpoint}'\nresponse = self._http_client.get(**url_data)\nif response.status_code != Client.codes.OK and raise_for_status:\nself.__raise_for_status(response)\nreturn response\ndef _handshake(self):\n\"\"\"Client handshake.\n        It is used to determine is the client can connect with its\n        current authorization token.\n        \"\"\"\nresponse = self.get('/profiles', params={}, raise_for_status=False)\nif response.status_code == Client.codes.FOUND:\nparser = LinkExtractor()\nparser.feed(response.text)\nraise ClientHandshakeError(auth_link=parser.link)\ndef _get_default_project(self, token: str):\nresponse = self.get('/profiles/me', params={}, cookies={'access_token': token})\ndata: Dict = response.json()\nreturn data['myWorkspace']\ndef __build_url(self, endpoint: str, params: Optional[Dict] = None, data: Optional[Dict] = None,\njson: Optional[Dict] = None, project: Optional[Project] = None, files: Optional[Dict] = None,\ncookies: Optional[Dict] = None) -&gt; Dict:\n\"\"\"Build a request for the backend.\n        Args:\n            endpoint (str): backend endpoint\n            params (Optional[dict]): URL parameters\n            data (Optional[Project]): (optional) multipart form data\n            json (Optional[dict]): (optional) json data\n            files (Optional[dict]): (optional) files to be sent\n            cookies (Optional[dict]): (optional) cookies data\n        Returns:\n            dictionary containing the information to perform a request\n        \"\"\"\n_params = params if params is not None else {\n'ns': project or self._default_project\n}\nurl_data = {\n'url': f'{self._base_url}/{endpoint.removeprefix(\"/\")}',\n'headers': self._headers,\n'params': _params,\n}\nif data is not None:\nurl_data['data'] = data\nif json is not None:\nurl_data['json'] = json\nif files is not None:\nurl_data['files'] = files\nif cookies is not None:\nurl_data['cookies'] = cookies\nreturn url_data\ndef __set_global(self) -&gt; None:\n\"\"\"Sets a client instance as global.\"\"\"\n# If the client is stateful, close it gracefully!\nClient.GLOBAL_CLIENT = self\ndef __raise_for_status(self, response: Response) -&gt; None:\n\"\"\"Raise an exception if the response is not OK.\n        When an exception is raised, we try to convert it to a ResponseError which is\n        a wrapper around a backend error. This usually gives enough context and provides\n        nice error message.\n        If it cannot be converted to ResponseError, it is re-raised.\n        Args:\n            response (Response): response to analyze\n        \"\"\"\ntry:\nresponse.raise_for_status()\nexcept HTTPStatusError as e:\nwith suppress(Exception):\ne = ResponseError(**response.json())\nraise e\n</code></pre>"},{"location":"sdk/reference/api/common/client/#ydata.sdk.common.client.client.Client.__build_url","title":"<code>__build_url(endpoint, params=None, data=None, json=None, project=None, files=None, cookies=None)</code>","text":"<p>Build a request for the backend.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>backend endpoint</p> required <code>params</code> <code>Optional[dict]</code> <p>URL parameters</p> <code>None</code> <code>data</code> <code>Optional[Project]</code> <p>(optional) multipart form data</p> <code>None</code> <code>json</code> <code>Optional[dict]</code> <p>(optional) json data</p> <code>None</code> <code>files</code> <code>Optional[dict]</code> <p>(optional) files to be sent</p> <code>None</code> <code>cookies</code> <code>Optional[dict]</code> <p>(optional) cookies data</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict</code> <p>dictionary containing the information to perform a request</p> Source code in <code>ydata/sdk/common/client/client.py</code> <pre><code>def __build_url(self, endpoint: str, params: Optional[Dict] = None, data: Optional[Dict] = None,\njson: Optional[Dict] = None, project: Optional[Project] = None, files: Optional[Dict] = None,\ncookies: Optional[Dict] = None) -&gt; Dict:\n\"\"\"Build a request for the backend.\n    Args:\n        endpoint (str): backend endpoint\n        params (Optional[dict]): URL parameters\n        data (Optional[Project]): (optional) multipart form data\n        json (Optional[dict]): (optional) json data\n        files (Optional[dict]): (optional) files to be sent\n        cookies (Optional[dict]): (optional) cookies data\n    Returns:\n        dictionary containing the information to perform a request\n    \"\"\"\n_params = params if params is not None else {\n'ns': project or self._default_project\n}\nurl_data = {\n'url': f'{self._base_url}/{endpoint.removeprefix(\"/\")}',\n'headers': self._headers,\n'params': _params,\n}\nif data is not None:\nurl_data['data'] = data\nif json is not None:\nurl_data['json'] = json\nif files is not None:\nurl_data['files'] = files\nif cookies is not None:\nurl_data['cookies'] = cookies\nreturn url_data\n</code></pre>"},{"location":"sdk/reference/api/common/client/#ydata.sdk.common.client.client.Client.__raise_for_status","title":"<code>__raise_for_status(response)</code>","text":"<p>Raise an exception if the response is not OK.</p> <p>When an exception is raised, we try to convert it to a ResponseError which is a wrapper around a backend error. This usually gives enough context and provides nice error message.</p> <p>If it cannot be converted to ResponseError, it is re-raised.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>Response</code> <p>response to analyze</p> required Source code in <code>ydata/sdk/common/client/client.py</code> <pre><code>def __raise_for_status(self, response: Response) -&gt; None:\n\"\"\"Raise an exception if the response is not OK.\n    When an exception is raised, we try to convert it to a ResponseError which is\n    a wrapper around a backend error. This usually gives enough context and provides\n    nice error message.\n    If it cannot be converted to ResponseError, it is re-raised.\n    Args:\n        response (Response): response to analyze\n    \"\"\"\ntry:\nresponse.raise_for_status()\nexcept HTTPStatusError as e:\nwith suppress(Exception):\ne = ResponseError(**response.json())\nraise e\n</code></pre>"},{"location":"sdk/reference/api/common/client/#ydata.sdk.common.client.client.Client.__set_global","title":"<code>__set_global()</code>","text":"<p>Sets a client instance as global.</p> Source code in <code>ydata/sdk/common/client/client.py</code> <pre><code>def __set_global(self) -&gt; None:\n\"\"\"Sets a client instance as global.\"\"\"\n# If the client is stateful, close it gracefully!\nClient.GLOBAL_CLIENT = self\n</code></pre>"},{"location":"sdk/reference/api/common/client/#ydata.sdk.common.client.client.Client.get","title":"<code>get(endpoint, params=None, project=None, cookies=None, raise_for_status=True)</code>","text":"<p>GET request to the backend.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>GET endpoint</p> required <code>cookies</code> <code>Optional[dict]</code> <p>(optional) cookies data</p> <code>None</code> <code>raise_for_status</code> <code>bool</code> <p>raise an exception on error</p> <code>True</code> <p>Returns:</p> Type Description <code>Response</code> <p>Response object</p> Source code in <code>ydata/sdk/common/client/client.py</code> <pre><code>def get(\nself, endpoint: str, params: Optional[Dict] = None, project: Optional[Project] = None,\ncookies: Optional[Dict] = None, raise_for_status: bool = True\n) -&gt; Response:\n\"\"\"GET request to the backend.\n    Args:\n        endpoint (str): GET endpoint\n        cookies (Optional[dict]): (optional) cookies data\n        raise_for_status (bool): raise an exception on error\n    Returns:\n        Response object\n    \"\"\"\nurl_data = self.__build_url(endpoint, params=params,\ncookies=cookies, project=project)\nresponse = self._http_client.get(**url_data)\nif response.status_code != Client.codes.OK and raise_for_status:\nself.__raise_for_status(response)\nreturn response\n</code></pre>"},{"location":"sdk/reference/api/common/client/#ydata.sdk.common.client.client.Client.get_static_file","title":"<code>get_static_file(endpoint, project=None, raise_for_status=True)</code>","text":"<p>Retrieve a static file from the backend.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>GET endpoint</p> required <code>raise_for_status</code> <code>bool</code> <p>raise an exception on error</p> <code>True</code> <p>Returns:</p> Type Description <code>Response</code> <p>Response object</p> Source code in <code>ydata/sdk/common/client/client.py</code> <pre><code>def get_static_file(\nself, endpoint: str, project: Optional[Project] = None, raise_for_status: bool = True\n) -&gt; Response:\n\"\"\"Retrieve a static file from the backend.\n    Args:\n        endpoint (str): GET endpoint\n        raise_for_status (bool): raise an exception on error\n    Returns:\n        Response object\n    \"\"\"\nurl_data = self.__build_url(endpoint, project=project)\nurl_data['url'] = f'{self._base_url}/static-content{endpoint}'\nresponse = self._http_client.get(**url_data)\nif response.status_code != Client.codes.OK and raise_for_status:\nself.__raise_for_status(response)\nreturn response\n</code></pre>"},{"location":"sdk/reference/api/common/client/#ydata.sdk.common.client.client.Client.patch","title":"<code>patch(endpoint, content=None, data=None, json=None, project=None, files=None, raise_for_status=True)</code>","text":"<p>PATCH request to the backend.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>POST endpoint</p> required <code>data</code> <code>Optional[dict]</code> <p>(optional) multipart form data</p> <code>None</code> <code>json</code> <code>Optional[dict]</code> <p>(optional) json data</p> <code>None</code> <code>files</code> <code>Optional[dict]</code> <p>(optional) files to be sent</p> <code>None</code> <code>raise_for_status</code> <code>bool</code> <p>raise an exception on error</p> <code>True</code> <p>Returns:</p> Type Description <code>Response</code> <p>Response object</p> Source code in <code>ydata/sdk/common/client/client.py</code> <pre><code>def patch(\nself, endpoint: str, content: Optional[RequestContent] = None, data: Optional[Dict] = None,\njson: Optional[Dict] = None, project: Optional[Project] = None, files: Optional[Dict] = None,\nraise_for_status: bool = True\n) -&gt; Response:\n\"\"\"PATCH request to the backend.\n    Args:\n        endpoint (str): POST endpoint\n        content (Optional[RequestContent])\n        data (Optional[dict]): (optional) multipart form data\n        json (Optional[dict]): (optional) json data\n        files (Optional[dict]): (optional) files to be sent\n        raise_for_status (bool): raise an exception on error\n    Returns:\n        Response object\n    \"\"\"\nurl_data = self.__build_url(\nendpoint, data=data, json=json, files=files, project=project)\nresponse = self._http_client.patch(**url_data, content=content)\nif response.status_code != Client.codes.OK and raise_for_status:\nself.__raise_for_status(response)\nreturn response\n</code></pre>"},{"location":"sdk/reference/api/common/client/#ydata.sdk.common.client.client.Client.post","title":"<code>post(endpoint, content=None, data=None, json=None, project=None, files=None, raise_for_status=True)</code>","text":"<p>POST request to the backend.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>POST endpoint</p> required <code>data</code> <code>Optional[dict]</code> <p>(optional) multipart form data</p> <code>None</code> <code>json</code> <code>Optional[dict]</code> <p>(optional) json data</p> <code>None</code> <code>files</code> <code>Optional[dict]</code> <p>(optional) files to be sent</p> <code>None</code> <code>raise_for_status</code> <code>bool</code> <p>raise an exception on error</p> <code>True</code> <p>Returns:</p> Type Description <code>Response</code> <p>Response object</p> Source code in <code>ydata/sdk/common/client/client.py</code> <pre><code>def post(\nself, endpoint: str, content: Optional[RequestContent] = None, data: Optional[Dict] = None,\njson: Optional[Dict] = None, project: Optional[Project] = None, files: Optional[Dict] = None,\nraise_for_status: bool = True\n) -&gt; Response:\n\"\"\"POST request to the backend.\n    Args:\n        endpoint (str): POST endpoint\n        content (Optional[RequestContent])\n        data (Optional[dict]): (optional) multipart form data\n        json (Optional[dict]): (optional) json data\n        files (Optional[dict]): (optional) files to be sent\n        raise_for_status (bool): raise an exception on error\n    Returns:\n        Response object\n    \"\"\"\nurl_data = self.__build_url(\nendpoint, data=data, json=json, files=files, project=project)\nresponse = self._http_client.post(**url_data)\nif response.status_code != Client.codes.OK and raise_for_status:\nself.__raise_for_status(response)\nreturn response\n</code></pre>"},{"location":"sdk/reference/api/common/types/","title":"Types","text":""},{"location":"sdk/reference/api/connectors/connector/","title":"Connector","text":"<p>             Bases: <code>ModelFactoryMixin</code></p> <p>A <code>Connector</code> allows to connect and access data stored in various places. The list of available connectors can be found here.</p> <p>Parameters:</p> Name Type Description Default <code>connector_type</code> <code>Union[ConnectorType, str]</code> <p>Type of the connector to be created</p> <code>None</code> <code>credentials</code> <code>dict</code> <p>Connector credentials</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>(optional) Connector name</p> <code>None</code> <code>project</code> <code>Optional[Project]</code> <p>(optional) Project name for this Connector</p> <code>None</code> <code>client</code> <code>Client</code> <p>(optional) Client to connect to the backend</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>uid</code> <code>UID</code> <p>UID fo the connector instance (creating internally)</p> <code>type</code> <code>ConnectorType</code> <p>Type of the connector</p> Source code in <code>ydata/sdk/connectors/connector.py</code> <pre><code>class Connector(ModelFactoryMixin):\n\"\"\"A [`Connector`][ydata.sdk.connectors.Connector] allows to connect and\n    access data stored in various places. The list of available connectors can\n    be found [here][ydata.sdk.connectors.ConnectorType].\n    Arguments:\n        connector_type (Union[ConnectorType, str]): Type of the connector to be created\n        credentials (dict): Connector credentials\n        name (Optional[str]): (optional) Connector name\n        project (Optional[Project]): (optional) Project name for this Connector\n        client (Client): (optional) Client to connect to the backend\n    Attributes:\n        uid (UID): UID fo the connector instance (creating internally)\n        type (ConnectorType): Type of the connector\n    \"\"\"\n_MODEL_CLASS = mConnector\n_model: Optional[mConnector]\ndef __init__(\nself, connector_type: Union[ConnectorType, str, None] = None, credentials: Optional[Dict] = None,\nname: Optional[str] = None, project: Optional[Project] = None, client: Optional[Client] = None):\nself._init_common(client=client)\nself._model = _connector_type_to_model(ConnectorType._init_connector_type(connector_type))._create_model(\nconnector_type, credentials, name, client=client)\nself._project = project\n@init_client\ndef _init_common(self, client: Optional[Client] = None):\nself._client = client\nself._logger = create_logger(__name__, level=LOG_LEVEL)\n@property\ndef uid(self) -&gt; UID:\nreturn self._model.uid\n@property\ndef name(self) -&gt; str:\nreturn self._model.name\n@property\ndef type(self) -&gt; ConnectorType:\nreturn ConnectorType(self._model.type)\n@property\ndef project(self) -&gt; Project:\nreturn self._project or self._client.project\n@staticmethod\n@init_client\ndef get(\nuid: UID, project: Optional[Project] = None, client: Optional[Client] = None\n) -&gt; _T:\n\"\"\"Get an existing connector.\n        Arguments:\n            uid (UID): Connector identifier\n            project (Optional[Project]): (optional) Project name from where to get the connector\n            client (Optional[Client]): (optional) Client to connect to the backend\n        Returns:\n            Connector\n        \"\"\"\nresponse = client.get(f'/connector/{uid}', project=project)\ndata = response.json()\ndata_type = data[\"type\"]\nconnector_class = _connector_type_to_model(\nConnectorType._init_connector_type(data_type))\nconnector = connector_class._init_from_model_data(\nconnector_class._MODEL_CLASS(**data))\nconnector._project = project\nreturn connector\n@staticmethod\ndef _init_credentials(\nconnector_type: ConnectorType, credentials: Union[str, Path, Dict, Credentials]\n) -&gt; Credentials:\n_credentials = None\nif isinstance(credentials, str):\ncredentials = Path(credentials)\nif isinstance(credentials, Path):\ntry:\n_credentials = json_loads(credentials.open().read())\nexcept Exception:\nraise CredentialTypeError(\n'Could not read the credentials. Please, check your path or credentials structure.')\ntry:\nfrom ydata.sdk.connectors._models.connector_map import TYPE_TO_CLASS\ncredential_cls = TYPE_TO_CLASS.get(connector_type.value)\n_credentials = credential_cls(**_credentials)\nexcept Exception:\nraise CredentialTypeError(\n\"Could not create the credentials. Verify the path or the structure your credentials.\")\nreturn _credentials\n@staticmethod\ndef create(\nconnector_type: Union[ConnectorType, str], credentials: Union[str, Path, Dict, Credentials],\nname: Optional[str] = None, project: Optional[Project] = None, client: Optional[Client] = None\n) -&gt; _T:\n\"\"\"Create a new connector.\n        Arguments:\n            connector_type (Union[ConnectorType, str]): Type of the connector to be created\n            credentials (dict): Connector credentials\n            name (Optional[str]): (optional) Connector name\n            project (Optional[Project]): (optional) Project where to create the connector\n            client (Client): (optional) Client to connect to the backend\n        Returns:\n            New connector\n        \"\"\"\nconnector_type = ConnectorType._init_connector_type(connector_type)\nconnector_class = _connector_type_to_model(connector_type)\npayload = {\n\"type\": connector_type.value,\n\"credentials\": credentials.dict(by_alias=True)\n}\nmodel = connector_class._create(payload, name, project, client)\nconnector = connector_class._init_from_model_data(model)\nconnector._project = project\nreturn connector\n@classmethod\n@init_client\ndef _create(\ncls, payload: dict, name: Optional[str] = None, project: Optional[Project] = None,\nclient: Optional[Client] = None\n) -&gt; _MODEL_CLASS:\n_name = name if name is not None else str(uuid4())\npayload[\"name\"] = _name\nresponse = client.post('/connector/', project=project, json=payload)\ndata = response.json()\nreturn cls._MODEL_CLASS(**data)\n@staticmethod\n@init_client\ndef list(project: Optional[Project] = None, client: Optional[Client] = None) -&gt; ConnectorsList:\n\"\"\"List the connectors instances.\n        Arguments:\n            project (Optional[Project]): (optional) Project name from where to list the connectors\n            client (Client): (optional) Client to connect to the backend\n        Returns:\n            List of connectors\n        \"\"\"\nresponse = client.get('/connector', project=project)\ndata: list = response.json()\nreturn ConnectorsList(data)\ndef __repr__(self):\nreturn self._model.__repr__()\n</code></pre>"},{"location":"sdk/reference/api/connectors/connector/#ydata.sdk.connectors.connector.Connector.create","title":"<code>create(connector_type, credentials, name=None, project=None, client=None)</code>  <code>staticmethod</code>","text":"<p>Create a new connector.</p> <p>Parameters:</p> Name Type Description Default <code>connector_type</code> <code>Union[ConnectorType, str]</code> <p>Type of the connector to be created</p> required <code>credentials</code> <code>dict</code> <p>Connector credentials</p> required <code>name</code> <code>Optional[str]</code> <p>(optional) Connector name</p> <code>None</code> <code>project</code> <code>Optional[Project]</code> <p>(optional) Project where to create the connector</p> <code>None</code> <code>client</code> <code>Client</code> <p>(optional) Client to connect to the backend</p> <code>None</code> <p>Returns:</p> Type Description <code>_T</code> <p>New connector</p> Source code in <code>ydata/sdk/connectors/connector.py</code> <pre><code>@staticmethod\ndef create(\nconnector_type: Union[ConnectorType, str], credentials: Union[str, Path, Dict, Credentials],\nname: Optional[str] = None, project: Optional[Project] = None, client: Optional[Client] = None\n) -&gt; _T:\n\"\"\"Create a new connector.\n    Arguments:\n        connector_type (Union[ConnectorType, str]): Type of the connector to be created\n        credentials (dict): Connector credentials\n        name (Optional[str]): (optional) Connector name\n        project (Optional[Project]): (optional) Project where to create the connector\n        client (Client): (optional) Client to connect to the backend\n    Returns:\n        New connector\n    \"\"\"\nconnector_type = ConnectorType._init_connector_type(connector_type)\nconnector_class = _connector_type_to_model(connector_type)\npayload = {\n\"type\": connector_type.value,\n\"credentials\": credentials.dict(by_alias=True)\n}\nmodel = connector_class._create(payload, name, project, client)\nconnector = connector_class._init_from_model_data(model)\nconnector._project = project\nreturn connector\n</code></pre>"},{"location":"sdk/reference/api/connectors/connector/#ydata.sdk.connectors.connector.Connector.get","title":"<code>get(uid, project=None, client=None)</code>  <code>staticmethod</code>","text":"<p>Get an existing connector.</p> <p>Parameters:</p> Name Type Description Default <code>uid</code> <code>UID</code> <p>Connector identifier</p> required <code>project</code> <code>Optional[Project]</code> <p>(optional) Project name from where to get the connector</p> <code>None</code> <code>client</code> <code>Optional[Client]</code> <p>(optional) Client to connect to the backend</p> <code>None</code> <p>Returns:</p> Type Description <code>_T</code> <p>Connector</p> Source code in <code>ydata/sdk/connectors/connector.py</code> <pre><code>@staticmethod\n@init_client\ndef get(\nuid: UID, project: Optional[Project] = None, client: Optional[Client] = None\n) -&gt; _T:\n\"\"\"Get an existing connector.\n    Arguments:\n        uid (UID): Connector identifier\n        project (Optional[Project]): (optional) Project name from where to get the connector\n        client (Optional[Client]): (optional) Client to connect to the backend\n    Returns:\n        Connector\n    \"\"\"\nresponse = client.get(f'/connector/{uid}', project=project)\ndata = response.json()\ndata_type = data[\"type\"]\nconnector_class = _connector_type_to_model(\nConnectorType._init_connector_type(data_type))\nconnector = connector_class._init_from_model_data(\nconnector_class._MODEL_CLASS(**data))\nconnector._project = project\nreturn connector\n</code></pre>"},{"location":"sdk/reference/api/connectors/connector/#ydata.sdk.connectors.connector.Connector.list","title":"<code>list(project=None, client=None)</code>  <code>staticmethod</code>","text":"<p>List the connectors instances.</p> <p>Parameters:</p> Name Type Description Default <code>project</code> <code>Optional[Project]</code> <p>(optional) Project name from where to list the connectors</p> <code>None</code> <code>client</code> <code>Client</code> <p>(optional) Client to connect to the backend</p> <code>None</code> <p>Returns:</p> Type Description <code>ConnectorsList</code> <p>List of connectors</p> Source code in <code>ydata/sdk/connectors/connector.py</code> <pre><code>@staticmethod\n@init_client\ndef list(project: Optional[Project] = None, client: Optional[Client] = None) -&gt; ConnectorsList:\n\"\"\"List the connectors instances.\n    Arguments:\n        project (Optional[Project]): (optional) Project name from where to list the connectors\n        client (Client): (optional) Client to connect to the backend\n    Returns:\n        List of connectors\n    \"\"\"\nresponse = client.get('/connector', project=project)\ndata: list = response.json()\nreturn ConnectorsList(data)\n</code></pre>"},{"location":"sdk/reference/api/connectors/connector/#connectortype","title":"ConnectorType","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p>"},{"location":"sdk/reference/api/connectors/connector/#ydata.sdk.connectors.ConnectorType.AWS_S3","title":"<code>AWS_S3 = 'aws-s3'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>AWS S3 connector</p>"},{"location":"sdk/reference/api/connectors/connector/#ydata.sdk.connectors.ConnectorType.AZURE_BLOB","title":"<code>AZURE_BLOB = 'azure-blob'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Azure Blob connector</p>"},{"location":"sdk/reference/api/connectors/connector/#ydata.sdk.connectors.ConnectorType.AZURE_SQL","title":"<code>AZURE_SQL = 'azure-sql'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>AzureSQL connector</p>"},{"location":"sdk/reference/api/connectors/connector/#ydata.sdk.connectors.ConnectorType.BIGQUERY","title":"<code>BIGQUERY = 'google-bigquery'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>BigQuery connector</p>"},{"location":"sdk/reference/api/connectors/connector/#ydata.sdk.connectors.ConnectorType.FILE","title":"<code>FILE = 'file'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>File connector (placeholder)</p>"},{"location":"sdk/reference/api/connectors/connector/#ydata.sdk.connectors.ConnectorType.GCS","title":"<code>GCS = 'gcs'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Google Cloud Storage connector</p>"},{"location":"sdk/reference/api/connectors/connector/#ydata.sdk.connectors.ConnectorType.MYSQL","title":"<code>MYSQL = 'mysql'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>MySQL connector</p>"},{"location":"sdk/reference/api/connectors/connector/#ydata.sdk.connectors.ConnectorType.SNOWFLAKE","title":"<code>SNOWFLAKE = 'snowflake'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Snowflake connector</p>"},{"location":"sdk/reference/api/datasources/datasource/","title":"DataSource","text":"<p>             Bases: <code>ModelFactoryMixin</code></p> <p>A <code>DataSource</code> represents a dataset to be used by a Synthesizer as training data.</p> <p>Parameters:</p> Name Type Description Default <code>connector</code> <code>Connector</code> <p>Connector from which the datasource is created</p> required <code>datatype</code> <code>Optional[Union[DataSourceType, str]]</code> <p>(optional) DataSource type</p> <code>TABULAR</code> <code>name</code> <code>Optional[str]</code> <p>(optional) DataSource name</p> <code>None</code> <code>project</code> <code>Optional[Project]</code> <p>(optional) Project name for this datasource</p> <code>None</code> <code>wait_for_metadata</code> <code>bool</code> <p>If <code>True</code>, wait until the metadata is fully calculated</p> <code>True</code> <code>client</code> <code>Client</code> <p>(optional) Client to connect to the backend</p> <code>None</code> <code>**config</code> <p>Datasource specific configuration</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>uid</code> <code>UID</code> <p>UID fo the datasource instance</p> <code>datatype</code> <code>DataSourceType</code> <p>Data source type</p> <code>status</code> <code>Status</code> <p>Status of the datasource</p> <code>metadata</code> <code>Metadata</code> <p>Metadata associated to the datasource</p> Source code in <code>ydata/sdk/datasources/datasource.py</code> <pre><code>class DataSource(ModelFactoryMixin):\n\"\"\"A [`DataSource`][ydata.sdk.datasources.DataSource] represents a dataset\n    to be used by a Synthesizer as training data.\n    Arguments:\n        connector (Connector): Connector from which the datasource is created\n        datatype (Optional[Union[DataSourceType, str]]): (optional) DataSource type\n        name (Optional[str]): (optional) DataSource name\n        project (Optional[Project]): (optional) Project name for this datasource\n        wait_for_metadata (bool): If `True`, wait until the metadata is fully calculated\n        client (Client): (optional) Client to connect to the backend\n        **config: Datasource specific configuration\n    Attributes:\n        uid (UID): UID fo the datasource instance\n        datatype (DataSourceType): Data source type\n        status (Status): Status of the datasource\n        metadata (Metadata): Metadata associated to the datasource\n    \"\"\"\ndef __init__(\nself, connector: Connector, datatype: Optional[Union[DataSourceType, str]] = DataSourceType.TABULAR,\nname: Optional[str] = None, project: Optional[Project] = None, wait_for_metadata: bool = True,\nclient: Optional[Client] = None, **config\n):\ndatasource_type = CONNECTOR_TO_DATASOURCE.get(connector.type)\nself._init_common(client=client)\nself._model: Optional[mDataSource] = self._create_model(\nconnector=connector, datasource_type=datasource_type, datatype=datatype,\nconfig=config, name=name, client=self._client)\nif wait_for_metadata:\nself._model = DataSource._wait_for_metadata(self)._model\nself._project = project\n@init_client\ndef _init_common(self, client: Optional[Client] = None):\nself._client = client\nself._logger = create_logger(__name__, level=LOG_LEVEL)\n@property\ndef uid(self) -&gt; UID:\nreturn self._model.uid\n@property\ndef datatype(self) -&gt; DataSourceType:\nreturn self._model.datatype\n@property\ndef project(self) -&gt; Project:\nreturn self._project or self._client.project\n@property\ndef status(self) -&gt; Status:\ntry:\nself._model = self.get(uid=self._model.uid,\nproject=self.project, client=self._client)._model\nreturn self._model.status\nexcept Exception:  # noqa: PIE786\nreturn Status.unknown()\n@property\ndef metadata(self) -&gt; Optional[Metadata]:\nreturn self._model.metadata\n@staticmethod\n@init_client\ndef list(project: Optional[Project] = None, client: Optional[Client] = None) -&gt; DataSourceList:\n\"\"\"List the  [`DataSource`][ydata.sdk.datasources.DataSource]\n        instances.\n        Arguments:\n            project (Optional[Project]): (optional) Project name from where to list the datasources\n            client (Client): (optional) Client to connect to the backend\n        Returns:\n            List of datasources\n        \"\"\"\ndef __process_data(data: list) -&gt; list:\nto_del = ['metadata']\nfor e in data:\nfor k in to_del:\ne.pop(k, None)\nreturn data\nresponse = client.get('/datasource', project=project)\ndata: list = response.json()\ndata = __process_data(data)\nreturn DataSourceList(data)\n@staticmethod\n@init_client\ndef get(uid: UID, project: Optional[Project] = None, client: Optional[Client] = None) -&gt; \"DataSource\":\n\"\"\"Get an existing [`DataSource`][ydata.sdk.datasources.DataSource].\n        Arguments:\n            uid (UID): DataSource identifier\n            project (Optional[Project]): (optional) Project name from where to get the connector\n            client (Client): (optional) Client to connect to the backend\n        Returns:\n            DataSource\n        \"\"\"\nresponse = client.get(f'/datasource/{uid}', project=project)\ndata: list = response.json()\ndatasource_type = CONNECTOR_TO_DATASOURCE.get(\nConnectorType(data['connector']['type']))\nmodel = DataSource._model_from_api(data, datasource_type)\ndatasource = DataSource._init_from_model_data(model)\ndatasource._project = project\nreturn datasource\n@classmethod\ndef create(\ncls, connector: Connector, datatype: Optional[Union[DataSourceType, str]] = DataSourceType.TABULAR,\nname: Optional[str] = None, project: Optional[Project] = None, wait_for_metadata: bool = True,\nclient: Optional[Client] = None, **config\n) -&gt; \"DataSource\":\n\"\"\"Create a new [`DataSource`][ydata.sdk.datasources.DataSource].\n        Arguments:\n            connector (Connector): Connector from which the datasource is created\n            datatype (Optional[Union[DataSourceType, str]]): (optional) DataSource type\n            name (Optional[str]): (optional) DataSource name\n            project (Optional[Project]): (optional) Project name for this datasource\n            wait_for_metadata (bool): If `True`, wait until the metadata is fully calculated\n            client (Client): (optional) Client to connect to the backend\n            **config: Datasource specific configuration\n        Returns:\n            DataSource\n        \"\"\"\ndatasource_type = CONNECTOR_TO_DATASOURCE.get(connector.type)\nreturn cls._create(\nconnector=connector, datasource_type=datasource_type, datatype=datatype, config=config, name=name,\nproject=project, wait_for_metadata=wait_for_metadata, client=client)\n@classmethod\ndef _create(\ncls, connector: Connector, datasource_type: Type[mDataSource],\ndatatype: Optional[Union[DataSourceType, str]] = DataSourceType.TABULAR, config: Optional[Dict] = None,\nname: Optional[str] = None, project: Optional[Project] = None, wait_for_metadata: bool = True,\nclient: Optional[Client] = None\n) -&gt; \"DataSource\":\nmodel = DataSource._create_model(\nconnector, datasource_type, datatype, config, name, project, client)\ndatasource = DataSource._init_from_model_data(model)\nif wait_for_metadata:\ndatasource._model = DataSource._wait_for_metadata(datasource)._model\ndatasource._project = project\nreturn datasource\n@classmethod\n@init_client\ndef _create_model(\ncls, connector: Connector, datasource_type: Type[mDataSource],\ndatatype: Optional[Union[DataSourceType, str]] = DataSourceType.TABULAR, config: Optional[Dict] = None,\nname: Optional[str] = None, project: Optional[Project] = None, client: Optional[Client] = None\n) -&gt; mDataSource:\n_name = name if name is not None else str(uuid4())\n_config = config if config is not None else {}\npayload = {\n\"name\": _name,\n\"connector\": {\n\"uid\": connector.uid,\n\"type\": ConnectorType(connector.type).value\n},\n\"dataType\": DataSourceType(datatype).value\n}\nif connector.type != ConnectorType.FILE:\n_config = datasource_type(**config).to_payload()\npayload.update(_config)\nresponse = client.post('/datasource/', project=project, json=payload)\ndata: list = response.json()\nreturn DataSource._model_from_api(data, datasource_type)\n@staticmethod\ndef _wait_for_metadata(datasource):\nlogger = create_logger(__name__, level=LOG_LEVEL)\nwhile State(datasource.status.state) not in [State.AVAILABLE, State.FAILED, State.UNAVAILABLE]:\nlogger.info(f'Calculating metadata [{datasource.status}]')\ndatasource = DataSource.get(uid=datasource.uid, client=datasource._client)\nsleep(BACKOFF)\nreturn datasource\n@staticmethod\ndef _model_from_api(data: Dict, datasource_type: Type[mDataSource]) -&gt; mDataSource:\ndata['datatype'] = data.pop('dataType', None)\ndata = filter_dict(datasource_type, data)\nmodel = datasource_type(**data)\nreturn model\ndef __repr__(self):\nreturn self._model.__repr__()\n</code></pre>"},{"location":"sdk/reference/api/datasources/datasource/#ydata.sdk.datasources.datasource.DataSource.create","title":"<code>create(connector, datatype=DataSourceType.TABULAR, name=None, project=None, wait_for_metadata=True, client=None, **config)</code>  <code>classmethod</code>","text":"<p>Create a new <code>DataSource</code>.</p> <p>Parameters:</p> Name Type Description Default <code>connector</code> <code>Connector</code> <p>Connector from which the datasource is created</p> required <code>datatype</code> <code>Optional[Union[DataSourceType, str]]</code> <p>(optional) DataSource type</p> <code>TABULAR</code> <code>name</code> <code>Optional[str]</code> <p>(optional) DataSource name</p> <code>None</code> <code>project</code> <code>Optional[Project]</code> <p>(optional) Project name for this datasource</p> <code>None</code> <code>wait_for_metadata</code> <code>bool</code> <p>If <code>True</code>, wait until the metadata is fully calculated</p> <code>True</code> <code>client</code> <code>Client</code> <p>(optional) Client to connect to the backend</p> <code>None</code> <code>**config</code> <p>Datasource specific configuration</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataSource</code> <p>DataSource</p> Source code in <code>ydata/sdk/datasources/datasource.py</code> <pre><code>@classmethod\ndef create(\ncls, connector: Connector, datatype: Optional[Union[DataSourceType, str]] = DataSourceType.TABULAR,\nname: Optional[str] = None, project: Optional[Project] = None, wait_for_metadata: bool = True,\nclient: Optional[Client] = None, **config\n) -&gt; \"DataSource\":\n\"\"\"Create a new [`DataSource`][ydata.sdk.datasources.DataSource].\n    Arguments:\n        connector (Connector): Connector from which the datasource is created\n        datatype (Optional[Union[DataSourceType, str]]): (optional) DataSource type\n        name (Optional[str]): (optional) DataSource name\n        project (Optional[Project]): (optional) Project name for this datasource\n        wait_for_metadata (bool): If `True`, wait until the metadata is fully calculated\n        client (Client): (optional) Client to connect to the backend\n        **config: Datasource specific configuration\n    Returns:\n        DataSource\n    \"\"\"\ndatasource_type = CONNECTOR_TO_DATASOURCE.get(connector.type)\nreturn cls._create(\nconnector=connector, datasource_type=datasource_type, datatype=datatype, config=config, name=name,\nproject=project, wait_for_metadata=wait_for_metadata, client=client)\n</code></pre>"},{"location":"sdk/reference/api/datasources/datasource/#ydata.sdk.datasources.datasource.DataSource.get","title":"<code>get(uid, project=None, client=None)</code>  <code>staticmethod</code>","text":"<p>Get an existing <code>DataSource</code>.</p> <p>Parameters:</p> Name Type Description Default <code>uid</code> <code>UID</code> <p>DataSource identifier</p> required <code>project</code> <code>Optional[Project]</code> <p>(optional) Project name from where to get the connector</p> <code>None</code> <code>client</code> <code>Client</code> <p>(optional) Client to connect to the backend</p> <code>None</code> <p>Returns:</p> Type Description <code>DataSource</code> <p>DataSource</p> Source code in <code>ydata/sdk/datasources/datasource.py</code> <pre><code>@staticmethod\n@init_client\ndef get(uid: UID, project: Optional[Project] = None, client: Optional[Client] = None) -&gt; \"DataSource\":\n\"\"\"Get an existing [`DataSource`][ydata.sdk.datasources.DataSource].\n    Arguments:\n        uid (UID): DataSource identifier\n        project (Optional[Project]): (optional) Project name from where to get the connector\n        client (Client): (optional) Client to connect to the backend\n    Returns:\n        DataSource\n    \"\"\"\nresponse = client.get(f'/datasource/{uid}', project=project)\ndata: list = response.json()\ndatasource_type = CONNECTOR_TO_DATASOURCE.get(\nConnectorType(data['connector']['type']))\nmodel = DataSource._model_from_api(data, datasource_type)\ndatasource = DataSource._init_from_model_data(model)\ndatasource._project = project\nreturn datasource\n</code></pre>"},{"location":"sdk/reference/api/datasources/datasource/#ydata.sdk.datasources.datasource.DataSource.list","title":"<code>list(project=None, client=None)</code>  <code>staticmethod</code>","text":"<p>List the  <code>DataSource</code> instances.</p> <p>Parameters:</p> Name Type Description Default <code>project</code> <code>Optional[Project]</code> <p>(optional) Project name from where to list the datasources</p> <code>None</code> <code>client</code> <code>Client</code> <p>(optional) Client to connect to the backend</p> <code>None</code> <p>Returns:</p> Type Description <code>DataSourceList</code> <p>List of datasources</p> Source code in <code>ydata/sdk/datasources/datasource.py</code> <pre><code>@staticmethod\n@init_client\ndef list(project: Optional[Project] = None, client: Optional[Client] = None) -&gt; DataSourceList:\n\"\"\"List the  [`DataSource`][ydata.sdk.datasources.DataSource]\n    instances.\n    Arguments:\n        project (Optional[Project]): (optional) Project name from where to list the datasources\n        client (Client): (optional) Client to connect to the backend\n    Returns:\n        List of datasources\n    \"\"\"\ndef __process_data(data: list) -&gt; list:\nto_del = ['metadata']\nfor e in data:\nfor k in to_del:\ne.pop(k, None)\nreturn data\nresponse = client.get('/datasource', project=project)\ndata: list = response.json()\ndata = __process_data(data)\nreturn DataSourceList(data)\n</code></pre>"},{"location":"sdk/reference/api/datasources/datasource/#status","title":"Status","text":"<p>             Bases: <code>BaseModel</code></p>"},{"location":"sdk/reference/api/datasources/datasource/#datasourcetype","title":"DataSourceType","text":"<p>             Bases: <code>StringEnum</code></p>"},{"location":"sdk/reference/api/datasources/datasource/#ydata.sdk.datasources.DataSourceType.MULTITABLE","title":"<code>MULTITABLE = 'multiTable'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The <code>DataSource</code> is a multi table RDBMS.</p>"},{"location":"sdk/reference/api/datasources/datasource/#ydata.sdk.datasources.DataSourceType.TABULAR","title":"<code>TABULAR = 'tabular'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The <code>DataSource</code> is tabular (i.e. it does not have a temporal dimension).</p>"},{"location":"sdk/reference/api/datasources/datasource/#ydata.sdk.datasources.DataSourceType.TIMESERIES","title":"<code>TIMESERIES = 'timeseries'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The <code>DataSource</code> has a temporal dimension.</p>"},{"location":"sdk/reference/api/datasources/metadata/","title":"Metadata","text":"<p>             Bases: <code>BaseModel</code></p> <p>The Metadata object contains descriptive information about a.</p> <p><code>DataSource</code></p> <p>Attributes:</p> Name Type Description <code>columns</code> <code>List[Column]</code> <p>columns information</p>"},{"location":"sdk/reference/api/synthesizers/base/","title":"Synthesizer","text":"<p>             Bases: <code>ABC</code>, <code>ModelFactoryMixin</code></p> <p>Main synthesizer class.</p> <p>This class cannot be directly instanciated because of the specificities between <code>RegularSynthesizer</code>, <code>TimeSeriesSynthesizer</code> or <code>MultiTableSynthesizer</code> <code>sample</code> methods.</p>"},{"location":"sdk/reference/api/synthesizers/base/#ydata.sdk.synthesizers.synthesizer.BaseSynthesizer--methods","title":"Methods","text":"<ul> <li><code>fit</code>: train a synthesizer instance.</li> <li><code>sample</code>: request synthetic data.</li> <li><code>status</code>: current status of the synthesizer instance.</li> </ul> Note <p>The synthesizer instance is created in the backend only when the <code>fit</code> method is called.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>(optional) Client to connect to the backend</p> <code>None</code> Source code in <code>ydata/sdk/synthesizers/synthesizer.py</code> <pre><code>@typechecked\nclass BaseSynthesizer(ABC, ModelFactoryMixin):\n\"\"\"Main synthesizer class.\n    This class cannot be directly instanciated because of the specificities between [`RegularSynthesizer`][ydata.sdk.synthesizers.RegularSynthesizer], [`TimeSeriesSynthesizer`][ydata.sdk.synthesizers.TimeSeriesSynthesizer] or [`MultiTableSynthesizer`][ydata.sdk.synthesizers.MultiTableSynthesizer] `sample` methods.\n    Methods\n    -------\n    - `fit`: train a synthesizer instance.\n    - `sample`: request synthetic data.\n    - `status`: current status of the synthesizer instance.\n    Note:\n            The synthesizer instance is created in the backend only when the `fit` method is called.\n    Arguments:\n        client (Client): (optional) Client to connect to the backend\n    \"\"\"\ndef __init__(\nself, uid: Optional[UID] = None, name: Optional[str] = None,\nproject: Optional[Project] = None, client: Optional[Client] = None):\nself._init_common(client=client)\nself._model = mSynthesizer(uid=uid, name=name or str(uuid4()))\nself._project = project\n@init_client\ndef _init_common(self, client: Optional[Client] = None):\nself._client = client\nself._logger = create_logger(__name__, level=LOG_LEVEL)\n@property\ndef project(self) -&gt; Project:\nreturn self._project or self._client.project\ndef fit(self, X: Union[DataSource, pdDataFrame],\nprivacy_level: PrivacyLevel = PrivacyLevel.HIGH_FIDELITY,\ndatatype: Optional[Union[DataSourceType, str]] = None,\nsortbykey: Optional[Union[str, List[str]]] = None,\nentities: Optional[Union[str, List[str]]] = None,\ngenerate_cols: Optional[List[str]] = None,\nexclude_cols: Optional[List[str]] = None,\ndtypes: Optional[Dict[str, Union[str, DataType]]] = None,\ntarget: Optional[str] = None,\nanonymize: Optional[dict] = None,\ncondition_on: Optional[List[str]] = None) -&gt; None:\n\"\"\"Fit the synthesizer.\n        The synthesizer accepts as training dataset either a pandas [`DataFrame`][pandas.DataFrame] directly or a YData [`DataSource`][ydata.sdk.datasources.DataSource].\n        When the training dataset is a pandas [`DataFrame`][pandas.DataFrame], the argument `datatype` is required as it cannot be deduced.\n        The argument`sortbykey` is mandatory for [`TimeSeries`][ydata.sdk.datasources.DataSourceType.TIMESERIES].\n        By default, if `generate_cols` or `exclude_cols` are not specified, all columns are generated by the synthesizer.\n        The argument `exclude_cols` has precedence over `generate_cols`, i.e. a column `col` will not be generated if it is in both list.\n        Arguments:\n            X (Union[DataSource, pandas.DataFrame]): Training dataset\n            privacy_level (PrivacyLevel): Synthesizer privacy level (defaults to high fidelity)\n            datatype (Optional[Union[DataSourceType, str]]): (optional) Dataset datatype - required if `X` is a [`pandas.DataFrame`][pandas.DataFrame]\n            sortbykey (Union[str, List[str]]): (optional) column(s) to use to sort timeseries datasets\n            entities (Union[str, List[str]]): (optional) columns representing entities ID\n            generate_cols (List[str]): (optional) columns that should be synthesized\n            exclude_cols (List[str]): (optional) columns that should not be synthesized\n            dtypes (Dict[str, Union[str, DataType]]): (optional) datatype mapping that will overwrite the datasource metadata column datatypes\n            target (Optional[str]): (optional) Target for the dataset\n            name (Optional[str]): (optional) Synthesizer instance name\n            anonymize (Optional[str]): (optional) fields to anonymize and the anonymization strategy\n            condition_on: (Optional[List[str]]): (optional) list of features to condition upon\n        \"\"\"\nif self._already_fitted():\nraise AlreadyFittedError()\ndatatype = DataSourceType(datatype)\ndataset_attrs = self._init_datasource_attributes(\nsortbykey, entities, generate_cols, exclude_cols, dtypes)\nself._validate_datasource_attributes(X, dataset_attrs, datatype, target)\n# If the training data is a pandas dataframe, we first need to create a data source and then the instance\nif isinstance(X, pdDataFrame):\nif X.empty:\nraise EmptyDataError(\"The DataFrame is empty\")\nself._logger.info('creating local connector with pandas dataframe')\nconnector = LocalConnector.create(\nsource=X, project=self._project, client=self._client)\nself._logger.info(\nf'created local connector. creating datasource with {connector}')\n_X = LocalDataSource(connector=connector, project=self._project,\ndatatype=datatype, client=self._client)\nself._logger.info(f'created datasource {_X}')\nelse:\n_X = X\nif dsState(_X.status.state) != dsState.AVAILABLE:\nraise DataSourceNotAvailableError(\nf\"The datasource '{_X.uid}' is not available (status = {_X.status})\")\nif isinstance(dataset_attrs, dict):\ndataset_attrs = DataSourceAttrs(**dataset_attrs)\nself._fit_from_datasource(\nX=_X, datatype=datatype, dataset_attrs=dataset_attrs, target=target,\nanonymize=anonymize, privacy_level=privacy_level, condition_on=condition_on)\n@staticmethod\ndef _init_datasource_attributes(\nsortbykey: Optional[Union[str, List[str]]],\nentities: Optional[Union[str, List[str]]],\ngenerate_cols: Optional[List[str]],\nexclude_cols: Optional[List[str]],\ndtypes: Optional[Dict[str, Union[str, DataType]]]) -&gt; DataSourceAttrs:\ndataset_attrs = {\n'sortbykey': sortbykey if sortbykey is not None else [],\n'entities': entities if entities is not None else [],\n'generate_cols': generate_cols if generate_cols is not None else [],\n'exclude_cols': exclude_cols if exclude_cols is not None else [],\n'dtypes': {k: DataType(v) for k, v in dtypes.items()} if dtypes is not None else {}\n}\nreturn DataSourceAttrs(**dataset_attrs)\n@staticmethod\ndef _validate_datasource_attributes(X: Union[DataSource, pdDataFrame], dataset_attrs: DataSourceAttrs, datatype: DataSourceType, target: Optional[str]):\ncolumns = []\nif isinstance(X, pdDataFrame):\ncolumns = X.columns\nif datatype is None:\nraise DataTypeMissingError(\n\"Argument `datatype` is mandatory for pandas.DataFrame training data\")\nelse:\ncolumns = [c.name for c in X.metadata.columns]\nif target is not None and target not in columns:\nraise DataSourceAttrsError(\n\"Invalid target: column '{target}' does not exist\")\nif datatype == DataSourceType.TIMESERIES:\nif not dataset_attrs.sortbykey:\nraise DataSourceAttrsError(\n\"The argument `sortbykey` is mandatory for timeseries datasource.\")\ninvalid_fields = {}\nfor field, v in dataset_attrs.dict().items():\nfield_columns = v if field != 'dtypes' else v.keys()\nnot_in_cols = [c for c in field_columns if c not in columns]\nif len(not_in_cols) &gt; 0:\ninvalid_fields[field] = not_in_cols\nif len(invalid_fields) &gt; 0:\nerror_msgs = [\"\\t- Field '{}': columns {} do not exist\".format(\nf, ', '.join(v)) for f, v in invalid_fields.items()]\nraise DataSourceAttrsError(\n\"The dataset attributes are invalid:\\n {}\".format('\\n'.join(error_msgs)))\n@staticmethod\ndef _metadata_to_payload(\ndatatype: DataSourceType, ds_metadata: Metadata,\ndataset_attrs: Optional[DataSourceAttrs] = None, target: Optional[str] = None\n) -&gt; dict:\n\"\"\"Transform a the metadata and dataset attributes into a valid\n        payload.\n        Arguments:\n            datatype (DataSourceType): datasource type\n            ds_metadata (Metadata): datasource metadata object\n            dataset_attrs ( Optional[DataSourceAttrs] ): (optional) Dataset attributes\n            target (Optional[str]): (optional) target column name\n        Returns:\n            metadata payload dictionary\n        \"\"\"\ncolumns = [\n{\n'name': c.name,\n'generation': True and c.name not in dataset_attrs.exclude_cols,\n'dataType': DataType(dataset_attrs.dtypes[c.name]).value if c.name in dataset_attrs.dtypes else c.datatype,\n'varType': c.vartype,\n}\nfor c in ds_metadata.columns]\nmetadata = {\n'columns': columns,\n'target': target\n}\nif dataset_attrs is not None:\nif datatype == DataSourceType.TIMESERIES:\nmetadata['sortBy'] = [c for c in dataset_attrs.sortbykey]\nmetadata['entity'] = [c for c in dataset_attrs.entities]\nreturn metadata\ndef _fit_from_datasource(\nself,\nX: DataSource,\ndatatype: DataSourceType,\nprivacy_level: Optional[PrivacyLevel] = None,\ndataset_attrs: Optional[DataSourceAttrs] = None,\ntarget: Optional[str] = None,\nanonymize: Optional[dict] = None,\ncondition_on: Optional[List[str]] = None\n) -&gt; None:\npayload = self._create_payload()\npayload['dataSourceUID'] = X.uid\nif privacy_level:\npayload['privacyLevel'] = privacy_level.value\nif X.metadata is not None:\npayload['metadata'] = self._metadata_to_payload(\ndatatype, X.metadata, dataset_attrs, target)\npayload['type'] = str(datatype.value)\nif anonymize is not None:\npayload[\"extraData\"][\"anonymize\"] = anonymize\nif condition_on is not None:\npayload[\"extraData\"][\"condition_on\"] = condition_on\nresponse = self._client.post(\n'/synthesizer/', json=payload, project=self._project)\ndata = response.json()\nself._model = mSynthesizer(**data)\nwhile self._check_fitting_not_finished(self.status):\nself._logger.info('Training the synthesizer...')\nsleep(BACKOFF)\ndef _create_payload(self) -&gt; dict:\npayload = {\n'extraData': {}\n}\nif self._model and self._model.name:\npayload['name'] = self._model.name\nreturn payload\ndef _check_fitting_not_finished(self, status: Status) -&gt; bool:\nself._logger.debug(f'checking status {status}')\nif Status.State(status.state) in [Status.State.READY, Status.State.REPORT]:\nreturn False\nself._logger.debug(f'status not ready yet {status.state}')\nif status.prepare and PrepareState(status.prepare.state) == PrepareState.FAILED:\nraise FittingError('Could not train the synthesizer')\nif status.training and TrainingState(status.training.state) == TrainingState.FAILED:\nraise FittingError('Could not train the synthesizer')\nreturn True\n@abstractmethod\ndef sample(self) -&gt; pdDataFrame:\n\"\"\"Abstract method to sample from a synthesizer.\"\"\"\ndef _sample(self, payload: Dict) -&gt; pdDataFrame:\n\"\"\"Sample from a synthesizer.\n        Arguments:\n            payload (dict): payload configuring the sample request\n        Returns:\n            pandas `DataFrame`\n        \"\"\"\nresponse = self._client.post(\nf\"/synthesizer/{self.uid}/sample\", json=payload, project=self._project)\ndata: Dict = response.json()\nsample_uid = data.get('uid')\nsample_status = None\nwhile sample_status not in ['finished', 'failed']:\nself._logger.info('Sampling from the synthesizer...')\nresponse = self._client.get(\nf'/synthesizer/{self.uid}/history', project=self._project)\nhistory: Dict = response.json()\nsample_data = next((s for s in history if s.get('uid') == sample_uid), None)\nsample_status = sample_data.get('status', {}).get('state')\nsleep(BACKOFF)\nresponse = self._client.get_static_file(\nf'/synthesizer/{self.uid}/sample/{sample_uid}/sample.csv', project=self._project)\ndata = StringIO(response.content.decode())\nreturn read_csv(data)\n@property\ndef uid(self) -&gt; UID:\n\"\"\"Get the status of a synthesizer instance.\n        Returns:\n            Synthesizer status\n        \"\"\"\nif not self._is_initialized():\nreturn Status.State.NOT_INITIALIZED\nreturn self._model.uid\n@property\ndef status(self) -&gt; Status:\n\"\"\"Get the status of a synthesizer instance.\n        Returns:\n            Synthesizer status\n        \"\"\"\nif not self._is_initialized():\nreturn Status.not_initialized()\ntry:\nself = self.get()\nreturn self._model.status\nexcept Exception:  # noqa: PIE786\nreturn Status.unknown()\ndef get(self):\nassert self._is_initialized() and self._model.uid, InputError(\n\"Please provide the synthesizer `uid`\")\nresponse = self._client.get(f'/synthesizer/{self.uid}', project=self._project)\ndata = response.json()\nself._model = mSynthesizer(**data)\nreturn self\n@staticmethod\n@init_client\ndef list(client: Optional[Client] = None) -&gt; SynthesizersList:\n\"\"\"List the synthesizer instances.\n        Arguments:\n            client (Client): (optional) Client to connect to the backend\n        Returns:\n            List of synthesizers\n        \"\"\"\ndef __process_data(data: list) -&gt; list:\nto_del = ['metadata', 'report', 'mode']\nfor e in data:\nfor k in to_del:\ne.pop(k, None)\nreturn data\nresponse = client.get('/synthesizer')\ndata: list = response.json()\ndata = __process_data(data)\nreturn SynthesizersList(data)\ndef _is_initialized(self) -&gt; bool:\n\"\"\"Determine if a synthesizer is instanciated or not.\n        Returns:\n            True if the synthesizer is instanciated\n        \"\"\"\nreturn self._model is not None\ndef _already_fitted(self) -&gt; bool:\n\"\"\"Determine if a synthesizer is already fitted.\n        Returns:\n            True if the synthesizer is instanciated\n        \"\"\"\nreturn self._is_initialized() and \\\n            (self._model.status is not None\nand self._model.status.training is not None\nand self._model.status.training.state is not [TrainingState.PREPARING])\n@staticmethod\ndef _resolve_api_status(api_status: Dict) -&gt; Status:\n\"\"\"Determine the status of the Synthesizer.\n        The status of the synthesizer instance is determined by the state of\n        its different components.\n        Arguments:\n            api_status (dict): json from the endpoint GET /synthesizer\n        Returns:\n            Synthesizer Status\n        \"\"\"\nstatus = Status(api_status.get('state', Status.UNKNOWN.name))\nif status == Status.PREPARE:\nif PrepareState(api_status.get('prepare', {}).get(\n'state', PrepareState.UNKNOWN.name)) == PrepareState.FAILED:\nreturn Status.FAILED\nelif status == Status.TRAIN:\nif TrainingState(api_status.get('training', {}).get(\n'state', TrainingState.UNKNOWN.name)) == TrainingState.FAILED:\nreturn Status.FAILED\nelif status == Status.REPORT:\nreturn Status.READY\nreturn status\n</code></pre>"},{"location":"sdk/reference/api/synthesizers/base/#ydata.sdk.synthesizers.synthesizer.BaseSynthesizer.status","title":"<code>status: Status</code>  <code>property</code>","text":"<p>Get the status of a synthesizer instance.</p> <p>Returns:</p> Type Description <code>Status</code> <p>Synthesizer status</p>"},{"location":"sdk/reference/api/synthesizers/base/#ydata.sdk.synthesizers.synthesizer.BaseSynthesizer.uid","title":"<code>uid: UID</code>  <code>property</code>","text":"<p>Get the status of a synthesizer instance.</p> <p>Returns:</p> Type Description <code>UID</code> <p>Synthesizer status</p>"},{"location":"sdk/reference/api/synthesizers/base/#ydata.sdk.synthesizers.synthesizer.BaseSynthesizer.fit","title":"<code>fit(X, privacy_level=PrivacyLevel.HIGH_FIDELITY, datatype=None, sortbykey=None, entities=None, generate_cols=None, exclude_cols=None, dtypes=None, target=None, anonymize=None, condition_on=None)</code>","text":"<p>Fit the synthesizer.</p> <p>The synthesizer accepts as training dataset either a pandas <code>DataFrame</code> directly or a YData <code>DataSource</code>. When the training dataset is a pandas <code>DataFrame</code>, the argument <code>datatype</code> is required as it cannot be deduced.</p> <p>The argument<code>sortbykey</code> is mandatory for <code>TimeSeries</code>.</p> <p>By default, if <code>generate_cols</code> or <code>exclude_cols</code> are not specified, all columns are generated by the synthesizer. The argument <code>exclude_cols</code> has precedence over <code>generate_cols</code>, i.e. a column <code>col</code> will not be generated if it is in both list.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[DataSource, DataFrame]</code> <p>Training dataset</p> required <code>privacy_level</code> <code>PrivacyLevel</code> <p>Synthesizer privacy level (defaults to high fidelity)</p> <code>HIGH_FIDELITY</code> <code>datatype</code> <code>Optional[Union[DataSourceType, str]]</code> <p>(optional) Dataset datatype - required if <code>X</code> is a <code>pandas.DataFrame</code></p> <code>None</code> <code>sortbykey</code> <code>Union[str, List[str]]</code> <p>(optional) column(s) to use to sort timeseries datasets</p> <code>None</code> <code>entities</code> <code>Union[str, List[str]]</code> <p>(optional) columns representing entities ID</p> <code>None</code> <code>generate_cols</code> <code>List[str]</code> <p>(optional) columns that should be synthesized</p> <code>None</code> <code>exclude_cols</code> <code>List[str]</code> <p>(optional) columns that should not be synthesized</p> <code>None</code> <code>dtypes</code> <code>Dict[str, Union[str, DataType]]</code> <p>(optional) datatype mapping that will overwrite the datasource metadata column datatypes</p> <code>None</code> <code>target</code> <code>Optional[str]</code> <p>(optional) Target for the dataset</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>(optional) Synthesizer instance name</p> required <code>anonymize</code> <code>Optional[str]</code> <p>(optional) fields to anonymize and the anonymization strategy</p> <code>None</code> <code>condition_on</code> <code>Optional[List[str]]</code> <p>(Optional[List[str]]): (optional) list of features to condition upon</p> <code>None</code> Source code in <code>ydata/sdk/synthesizers/synthesizer.py</code> <pre><code>def fit(self, X: Union[DataSource, pdDataFrame],\nprivacy_level: PrivacyLevel = PrivacyLevel.HIGH_FIDELITY,\ndatatype: Optional[Union[DataSourceType, str]] = None,\nsortbykey: Optional[Union[str, List[str]]] = None,\nentities: Optional[Union[str, List[str]]] = None,\ngenerate_cols: Optional[List[str]] = None,\nexclude_cols: Optional[List[str]] = None,\ndtypes: Optional[Dict[str, Union[str, DataType]]] = None,\ntarget: Optional[str] = None,\nanonymize: Optional[dict] = None,\ncondition_on: Optional[List[str]] = None) -&gt; None:\n\"\"\"Fit the synthesizer.\n    The synthesizer accepts as training dataset either a pandas [`DataFrame`][pandas.DataFrame] directly or a YData [`DataSource`][ydata.sdk.datasources.DataSource].\n    When the training dataset is a pandas [`DataFrame`][pandas.DataFrame], the argument `datatype` is required as it cannot be deduced.\n    The argument`sortbykey` is mandatory for [`TimeSeries`][ydata.sdk.datasources.DataSourceType.TIMESERIES].\n    By default, if `generate_cols` or `exclude_cols` are not specified, all columns are generated by the synthesizer.\n    The argument `exclude_cols` has precedence over `generate_cols`, i.e. a column `col` will not be generated if it is in both list.\n    Arguments:\n        X (Union[DataSource, pandas.DataFrame]): Training dataset\n        privacy_level (PrivacyLevel): Synthesizer privacy level (defaults to high fidelity)\n        datatype (Optional[Union[DataSourceType, str]]): (optional) Dataset datatype - required if `X` is a [`pandas.DataFrame`][pandas.DataFrame]\n        sortbykey (Union[str, List[str]]): (optional) column(s) to use to sort timeseries datasets\n        entities (Union[str, List[str]]): (optional) columns representing entities ID\n        generate_cols (List[str]): (optional) columns that should be synthesized\n        exclude_cols (List[str]): (optional) columns that should not be synthesized\n        dtypes (Dict[str, Union[str, DataType]]): (optional) datatype mapping that will overwrite the datasource metadata column datatypes\n        target (Optional[str]): (optional) Target for the dataset\n        name (Optional[str]): (optional) Synthesizer instance name\n        anonymize (Optional[str]): (optional) fields to anonymize and the anonymization strategy\n        condition_on: (Optional[List[str]]): (optional) list of features to condition upon\n    \"\"\"\nif self._already_fitted():\nraise AlreadyFittedError()\ndatatype = DataSourceType(datatype)\ndataset_attrs = self._init_datasource_attributes(\nsortbykey, entities, generate_cols, exclude_cols, dtypes)\nself._validate_datasource_attributes(X, dataset_attrs, datatype, target)\n# If the training data is a pandas dataframe, we first need to create a data source and then the instance\nif isinstance(X, pdDataFrame):\nif X.empty:\nraise EmptyDataError(\"The DataFrame is empty\")\nself._logger.info('creating local connector with pandas dataframe')\nconnector = LocalConnector.create(\nsource=X, project=self._project, client=self._client)\nself._logger.info(\nf'created local connector. creating datasource with {connector}')\n_X = LocalDataSource(connector=connector, project=self._project,\ndatatype=datatype, client=self._client)\nself._logger.info(f'created datasource {_X}')\nelse:\n_X = X\nif dsState(_X.status.state) != dsState.AVAILABLE:\nraise DataSourceNotAvailableError(\nf\"The datasource '{_X.uid}' is not available (status = {_X.status})\")\nif isinstance(dataset_attrs, dict):\ndataset_attrs = DataSourceAttrs(**dataset_attrs)\nself._fit_from_datasource(\nX=_X, datatype=datatype, dataset_attrs=dataset_attrs, target=target,\nanonymize=anonymize, privacy_level=privacy_level, condition_on=condition_on)\n</code></pre>"},{"location":"sdk/reference/api/synthesizers/base/#ydata.sdk.synthesizers.synthesizer.BaseSynthesizer.list","title":"<code>list(client=None)</code>  <code>staticmethod</code>","text":"<p>List the synthesizer instances.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>(optional) Client to connect to the backend</p> <code>None</code> <p>Returns:</p> Type Description <code>SynthesizersList</code> <p>List of synthesizers</p> Source code in <code>ydata/sdk/synthesizers/synthesizer.py</code> <pre><code>@staticmethod\n@init_client\ndef list(client: Optional[Client] = None) -&gt; SynthesizersList:\n\"\"\"List the synthesizer instances.\n    Arguments:\n        client (Client): (optional) Client to connect to the backend\n    Returns:\n        List of synthesizers\n    \"\"\"\ndef __process_data(data: list) -&gt; list:\nto_del = ['metadata', 'report', 'mode']\nfor e in data:\nfor k in to_del:\ne.pop(k, None)\nreturn data\nresponse = client.get('/synthesizer')\ndata: list = response.json()\ndata = __process_data(data)\nreturn SynthesizersList(data)\n</code></pre>"},{"location":"sdk/reference/api/synthesizers/base/#ydata.sdk.synthesizers.synthesizer.BaseSynthesizer.sample","title":"<code>sample()</code>  <code>abstractmethod</code>","text":"<p>Abstract method to sample from a synthesizer.</p> Source code in <code>ydata/sdk/synthesizers/synthesizer.py</code> <pre><code>@abstractmethod\ndef sample(self) -&gt; pdDataFrame:\n\"\"\"Abstract method to sample from a synthesizer.\"\"\"\n</code></pre>"},{"location":"sdk/reference/api/synthesizers/base/#privacylevel","title":"PrivacyLevel","text":"<p>             Bases: <code>StringEnum</code></p> <p>Privacy level exposed to the end-user.</p>"},{"location":"sdk/reference/api/synthesizers/base/#ydata.sdk.synthesizers.PrivacyLevel.BALANCED_PRIVACY_FIDELITY","title":"<code>BALANCED_PRIVACY_FIDELITY = 'BALANCED_PRIVACY_FIDELITY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Balanced privacy/fidelity</p>"},{"location":"sdk/reference/api/synthesizers/base/#ydata.sdk.synthesizers.PrivacyLevel.HIGH_FIDELITY","title":"<code>HIGH_FIDELITY = 'HIGH_FIDELITY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>High fidelity</p>"},{"location":"sdk/reference/api/synthesizers/base/#ydata.sdk.synthesizers.PrivacyLevel.HIGH_PRIVACY","title":"<code>HIGH_PRIVACY = 'HIGH_PRIVACY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>High privacy</p>"},{"location":"sdk/reference/api/synthesizers/multitable/","title":"MultiTable","text":"<p>             Bases: <code>BaseSynthesizer</code></p> <p>MultiTable synthesizer class.</p>"},{"location":"sdk/reference/api/synthesizers/multitable/#ydata.sdk.synthesizers.multitable.MultiTableSynthesizer--methods","title":"Methods","text":"<ul> <li><code>fit</code>: train a synthesizer instance.</li> <li><code>sample</code>: request synthetic data.</li> <li><code>status</code>: current status of the synthesizer instance.</li> </ul> Note <p>The synthesizer instance is created in the backend only when the <code>fit</code> method is called.</p> <p>Parameters:</p> Name Type Description Default <code>write_connector</code> <code>UID | Connector</code> <p>Connector of type RDBMS to be used to write the samples</p> required <code>uid</code> <code>UID</code> <p>(optional) UID to identify this synthesizer</p> <code>None</code> <code>name</code> <code>str</code> <p>(optional) Name to be used when creating the synthesizer. Calculated internally if not provided</p> <code>None</code> <code>client</code> <code>Client</code> <p>(optional) Client to connect to the backend</p> <code>None</code> Source code in <code>ydata/sdk/synthesizers/multitable.py</code> <pre><code>class MultiTableSynthesizer(BaseSynthesizer):\n\"\"\"MultiTable synthesizer class.\n    Methods\n    -------\n    - `fit`: train a synthesizer instance.\n    - `sample`: request synthetic data.\n    - `status`: current status of the synthesizer instance.\n    Note:\n            The synthesizer instance is created in the backend only when the `fit` method is called.\n    Arguments:\n        write_connector (UID | Connector): Connector of type RDBMS to be used to write the samples\n        uid (UID): (optional) UID to identify this synthesizer\n        name (str): (optional) Name to be used when creating the synthesizer. Calculated internally if not provided\n        client (Client): (optional) Client to connect to the backend\n    \"\"\"\ndef __init__(\nself, write_connector: Union[Connector, UID], uid: Optional[UID] = None, name: Optional[str] = None,\nproject: Optional[Project] = None, client: Optional[Client] = None):\nsuper().__init__(uid, name, project, client)\nconnector = self._check_or_fetch_connector(write_connector)\nself.__write_connector = connector.uid\ndef fit(self, X: DataSource,\nprivacy_level: PrivacyLevel = PrivacyLevel.HIGH_FIDELITY,\ndatatype: Optional[Union[DataSourceType, str]] = None,\nsortbykey: Optional[Union[str, List[str]]] = None,\nentities: Optional[Union[str, List[str]]] = None,\ngenerate_cols: Optional[List[str]] = None,\nexclude_cols: Optional[List[str]] = None,\ndtypes: Optional[Dict[str, Union[str, DataType]]] = None,\ntarget: Optional[str] = None,\nanonymize: Optional[dict] = None,\ncondition_on: Optional[List[str]] = None) -&gt; None:\n\"\"\"Fit the synthesizer.\n        The synthesizer accepts as training dataset a YData [`DataSource`][ydata.sdk.datasources.DataSource].\n        Except X, all the other arguments are for now ignored until they are supported.\n        Arguments:\n            X (DataSource): DataSource to Train\n        \"\"\"\nself._fit_from_datasource(X, datatype=DataSourceType.MULTITABLE)\ndef sample(self, frac: Union[int, float] = 1, write_connector: Optional[Union[Connector, UID]] = None) -&gt; None:\n\"\"\"Sample from a [`MultiTableSynthesizer`][ydata.sdk.synthesizers.MultiTableSynthesizer]\n        instance.\n        The sample is saved in the connector that was provided in the synthesizer initialization\n        or in the\n        Arguments:\n            frac (int | float): fraction of the sample to be returned\n        \"\"\"\nassert frac &gt;= 0.1, InputError(\n\"It is not possible to generate an empty synthetic data schema. Please validate the input provided. \")\nassert frac &lt;= 5, InputError(\n\"It is not possible to generate a database that is 5x bigger than the original dataset. Please validate the input provided.\")\npayload = {\n'fraction': frac,\n}\nif write_connector is not None:\nconnector = self._check_or_fetch_connector(write_connector)\npayload['writeConnector'] = connector.uid\nresponse = self._client.post(\nf\"/synthesizer/{self.uid}/sample\", json=payload, project=self._project)\ndata = response.json()\nsample_uid = data.get('uid')\nsample_status = None\nwhile sample_status not in ['finished', 'failed']:\nself._logger.info('Sampling from the synthesizer...')\nresponse = self._client.get(\nf'/synthesizer/{self.uid}/history', project=self._project)\nhistory = response.json()\nsample_data = next((s for s in history if s.get('uid') == sample_uid), None)\nsample_status = sample_data.get('status', {}).get('state')\nsleep(BACKOFF)\nprint(\nf\"Sample created and saved into connector with ID {self.__write_connector or write_connector}\")\ndef _create_payload(self) -&gt; dict:\npayload = super()._create_payload()\npayload['writeConnector'] = self.__write_connector\nreturn payload\ndef _check_or_fetch_connector(self, write_connector: Union[Connector, UID]) -&gt; Connector:\nself._logger.debug(f'Write connector is {write_connector}')\nif isinstance(write_connector, str):\nself._logger.debug(f'Write connector is of type `UID` {write_connector}')\nwrite_connector = Connector.get(write_connector)\nself._logger.debug(f'Using fetched connector {write_connector}')\nif write_connector.uid is None:\nraise InputError(\"Invalid connector provided as input for write\")\nif write_connector.type not in [ConnectorType.AZURE_SQL, ConnectorType.MYSQL, ConnectorType.SNOWFLAKE]:\nraise ConnectorError(\nf\"Invalid type `{write_connector.type}` for the provided connector\")\nreturn write_connector\n</code></pre>"},{"location":"sdk/reference/api/synthesizers/multitable/#ydata.sdk.synthesizers.multitable.MultiTableSynthesizer.fit","title":"<code>fit(X, privacy_level=PrivacyLevel.HIGH_FIDELITY, datatype=None, sortbykey=None, entities=None, generate_cols=None, exclude_cols=None, dtypes=None, target=None, anonymize=None, condition_on=None)</code>","text":"<p>Fit the synthesizer.</p> <p>The synthesizer accepts as training dataset a YData <code>DataSource</code>. Except X, all the other arguments are for now ignored until they are supported.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataSource</code> <p>DataSource to Train</p> required Source code in <code>ydata/sdk/synthesizers/multitable.py</code> <pre><code>def fit(self, X: DataSource,\nprivacy_level: PrivacyLevel = PrivacyLevel.HIGH_FIDELITY,\ndatatype: Optional[Union[DataSourceType, str]] = None,\nsortbykey: Optional[Union[str, List[str]]] = None,\nentities: Optional[Union[str, List[str]]] = None,\ngenerate_cols: Optional[List[str]] = None,\nexclude_cols: Optional[List[str]] = None,\ndtypes: Optional[Dict[str, Union[str, DataType]]] = None,\ntarget: Optional[str] = None,\nanonymize: Optional[dict] = None,\ncondition_on: Optional[List[str]] = None) -&gt; None:\n\"\"\"Fit the synthesizer.\n    The synthesizer accepts as training dataset a YData [`DataSource`][ydata.sdk.datasources.DataSource].\n    Except X, all the other arguments are for now ignored until they are supported.\n    Arguments:\n        X (DataSource): DataSource to Train\n    \"\"\"\nself._fit_from_datasource(X, datatype=DataSourceType.MULTITABLE)\n</code></pre>"},{"location":"sdk/reference/api/synthesizers/multitable/#ydata.sdk.synthesizers.multitable.MultiTableSynthesizer.sample","title":"<code>sample(frac=1, write_connector=None)</code>","text":"<p>Sample from a <code>MultiTableSynthesizer</code> instance. The sample is saved in the connector that was provided in the synthesizer initialization or in the</p> <p>Parameters:</p> Name Type Description Default <code>frac</code> <code>int | float</code> <p>fraction of the sample to be returned</p> <code>1</code> Source code in <code>ydata/sdk/synthesizers/multitable.py</code> <pre><code>def sample(self, frac: Union[int, float] = 1, write_connector: Optional[Union[Connector, UID]] = None) -&gt; None:\n\"\"\"Sample from a [`MultiTableSynthesizer`][ydata.sdk.synthesizers.MultiTableSynthesizer]\n    instance.\n    The sample is saved in the connector that was provided in the synthesizer initialization\n    or in the\n    Arguments:\n        frac (int | float): fraction of the sample to be returned\n    \"\"\"\nassert frac &gt;= 0.1, InputError(\n\"It is not possible to generate an empty synthetic data schema. Please validate the input provided. \")\nassert frac &lt;= 5, InputError(\n\"It is not possible to generate a database that is 5x bigger than the original dataset. Please validate the input provided.\")\npayload = {\n'fraction': frac,\n}\nif write_connector is not None:\nconnector = self._check_or_fetch_connector(write_connector)\npayload['writeConnector'] = connector.uid\nresponse = self._client.post(\nf\"/synthesizer/{self.uid}/sample\", json=payload, project=self._project)\ndata = response.json()\nsample_uid = data.get('uid')\nsample_status = None\nwhile sample_status not in ['finished', 'failed']:\nself._logger.info('Sampling from the synthesizer...')\nresponse = self._client.get(\nf'/synthesizer/{self.uid}/history', project=self._project)\nhistory = response.json()\nsample_data = next((s for s in history if s.get('uid') == sample_uid), None)\nsample_status = sample_data.get('status', {}).get('state')\nsleep(BACKOFF)\nprint(\nf\"Sample created and saved into connector with ID {self.__write_connector or write_connector}\")\n</code></pre>"},{"location":"sdk/reference/api/synthesizers/regular/","title":"Regular","text":"<p>             Bases: <code>BaseSynthesizer</code></p> Source code in <code>ydata/sdk/synthesizers/regular.py</code> <pre><code>class RegularSynthesizer(BaseSynthesizer):\ndef sample(self, n_samples: int = 1, condition_on: Optional[dict] = None) -&gt; pdDataFrame:\n\"\"\"Sample from a [`RegularSynthesizer`][ydata.sdk.synthesizers.RegularSynthesizer]\n        instance.\n        Arguments:\n            n_samples (int): number of rows in the sample\n            condition_on: (Optional[dict]): (optional) conditional sampling parameters\n        Returns:\n            synthetic data\n        \"\"\"\nif n_samples &lt; 1:\nraise InputError(\"Parameter 'n_samples' must be greater than 0\")\npayload = {\"numberOfRecords\": n_samples}\nif condition_on is not None:\npayload[\"extraData\"] = {\n\"condition_on\": condition_on\n}\nreturn self._sample(payload=payload)\ndef fit(self, X: Union[DataSource, pdDataFrame],\nprivacy_level: PrivacyLevel = PrivacyLevel.HIGH_FIDELITY,\nentities: Optional[Union[str, List[str]]] = None,\ngenerate_cols: Optional[List[str]] = None,\nexclude_cols: Optional[List[str]] = None,\ndtypes: Optional[Dict[str, Union[str, DataType]]] = None,\ntarget: Optional[str] = None,\nanonymize: Optional[dict] = None,\ncondition_on: Optional[List[str]] = None) -&gt; None:\n\"\"\"Fit the synthesizer.\n        The synthesizer accepts as training dataset either a pandas [`DataFrame`][pandas.DataFrame] directly or a YData [`DataSource`][ydata.sdk.datasources.DataSource].\n        Arguments:\n            X (Union[DataSource, pandas.DataFrame]): Training dataset\n            privacy_level (PrivacyLevel): Synthesizer privacy level (defaults to high fidelity)\n            entities (Union[str, List[str]]): (optional) columns representing entities ID\n            generate_cols (List[str]): (optional) columns that should be synthesized\n            exclude_cols (List[str]): (optional) columns that should not be synthesized\n            dtypes (Dict[str, Union[str, DataType]]): (optional) datatype mapping that will overwrite the datasource metadata column datatypes\n            target (Optional[str]): (optional) Target column\n            name (Optional[str]): (optional) Synthesizer instance name\n            anonymize (Optional[str]): (optional) fields to anonymize and the anonymization strategy\n            condition_on: (Optional[List[str]]): (optional) list of features to condition upon\n        \"\"\"\nBaseSynthesizer.fit(self, X=X, datatype=DataSourceType.TABULAR, entities=entities,\ngenerate_cols=generate_cols, exclude_cols=exclude_cols, dtypes=dtypes,\ntarget=target, anonymize=anonymize, privacy_level=privacy_level,\ncondition_on=condition_on)\ndef __repr__(self):\nif self._model is not None:\nreturn self._model.__repr__()\nelse:\nreturn \"RegularSynthesizer(Not Initialized)\"\n</code></pre>"},{"location":"sdk/reference/api/synthesizers/regular/#ydata.sdk.synthesizers.regular.RegularSynthesizer.fit","title":"<code>fit(X, privacy_level=PrivacyLevel.HIGH_FIDELITY, entities=None, generate_cols=None, exclude_cols=None, dtypes=None, target=None, anonymize=None, condition_on=None)</code>","text":"<p>Fit the synthesizer.</p> <p>The synthesizer accepts as training dataset either a pandas <code>DataFrame</code> directly or a YData <code>DataSource</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[DataSource, DataFrame]</code> <p>Training dataset</p> required <code>privacy_level</code> <code>PrivacyLevel</code> <p>Synthesizer privacy level (defaults to high fidelity)</p> <code>HIGH_FIDELITY</code> <code>entities</code> <code>Union[str, List[str]]</code> <p>(optional) columns representing entities ID</p> <code>None</code> <code>generate_cols</code> <code>List[str]</code> <p>(optional) columns that should be synthesized</p> <code>None</code> <code>exclude_cols</code> <code>List[str]</code> <p>(optional) columns that should not be synthesized</p> <code>None</code> <code>dtypes</code> <code>Dict[str, Union[str, DataType]]</code> <p>(optional) datatype mapping that will overwrite the datasource metadata column datatypes</p> <code>None</code> <code>target</code> <code>Optional[str]</code> <p>(optional) Target column</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>(optional) Synthesizer instance name</p> required <code>anonymize</code> <code>Optional[str]</code> <p>(optional) fields to anonymize and the anonymization strategy</p> <code>None</code> <code>condition_on</code> <code>Optional[List[str]]</code> <p>(Optional[List[str]]): (optional) list of features to condition upon</p> <code>None</code> Source code in <code>ydata/sdk/synthesizers/regular.py</code> <pre><code>def fit(self, X: Union[DataSource, pdDataFrame],\nprivacy_level: PrivacyLevel = PrivacyLevel.HIGH_FIDELITY,\nentities: Optional[Union[str, List[str]]] = None,\ngenerate_cols: Optional[List[str]] = None,\nexclude_cols: Optional[List[str]] = None,\ndtypes: Optional[Dict[str, Union[str, DataType]]] = None,\ntarget: Optional[str] = None,\nanonymize: Optional[dict] = None,\ncondition_on: Optional[List[str]] = None) -&gt; None:\n\"\"\"Fit the synthesizer.\n    The synthesizer accepts as training dataset either a pandas [`DataFrame`][pandas.DataFrame] directly or a YData [`DataSource`][ydata.sdk.datasources.DataSource].\n    Arguments:\n        X (Union[DataSource, pandas.DataFrame]): Training dataset\n        privacy_level (PrivacyLevel): Synthesizer privacy level (defaults to high fidelity)\n        entities (Union[str, List[str]]): (optional) columns representing entities ID\n        generate_cols (List[str]): (optional) columns that should be synthesized\n        exclude_cols (List[str]): (optional) columns that should not be synthesized\n        dtypes (Dict[str, Union[str, DataType]]): (optional) datatype mapping that will overwrite the datasource metadata column datatypes\n        target (Optional[str]): (optional) Target column\n        name (Optional[str]): (optional) Synthesizer instance name\n        anonymize (Optional[str]): (optional) fields to anonymize and the anonymization strategy\n        condition_on: (Optional[List[str]]): (optional) list of features to condition upon\n    \"\"\"\nBaseSynthesizer.fit(self, X=X, datatype=DataSourceType.TABULAR, entities=entities,\ngenerate_cols=generate_cols, exclude_cols=exclude_cols, dtypes=dtypes,\ntarget=target, anonymize=anonymize, privacy_level=privacy_level,\ncondition_on=condition_on)\n</code></pre>"},{"location":"sdk/reference/api/synthesizers/regular/#ydata.sdk.synthesizers.regular.RegularSynthesizer.sample","title":"<code>sample(n_samples=1, condition_on=None)</code>","text":"<p>Sample from a <code>RegularSynthesizer</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>number of rows in the sample</p> <code>1</code> <code>condition_on</code> <code>Optional[dict]</code> <p>(Optional[dict]): (optional) conditional sampling parameters</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>synthetic data</p> Source code in <code>ydata/sdk/synthesizers/regular.py</code> <pre><code>def sample(self, n_samples: int = 1, condition_on: Optional[dict] = None) -&gt; pdDataFrame:\n\"\"\"Sample from a [`RegularSynthesizer`][ydata.sdk.synthesizers.RegularSynthesizer]\n    instance.\n    Arguments:\n        n_samples (int): number of rows in the sample\n        condition_on: (Optional[dict]): (optional) conditional sampling parameters\n    Returns:\n        synthetic data\n    \"\"\"\nif n_samples &lt; 1:\nraise InputError(\"Parameter 'n_samples' must be greater than 0\")\npayload = {\"numberOfRecords\": n_samples}\nif condition_on is not None:\npayload[\"extraData\"] = {\n\"condition_on\": condition_on\n}\nreturn self._sample(payload=payload)\n</code></pre>"},{"location":"sdk/reference/api/synthesizers/regular/#privacylevel","title":"PrivacyLevel","text":"<p>             Bases: <code>StringEnum</code></p> <p>Privacy level exposed to the end-user.</p>"},{"location":"sdk/reference/api/synthesizers/regular/#ydata.sdk.synthesizers.PrivacyLevel.BALANCED_PRIVACY_FIDELITY","title":"<code>BALANCED_PRIVACY_FIDELITY = 'BALANCED_PRIVACY_FIDELITY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Balanced privacy/fidelity</p>"},{"location":"sdk/reference/api/synthesizers/regular/#ydata.sdk.synthesizers.PrivacyLevel.HIGH_FIDELITY","title":"<code>HIGH_FIDELITY = 'HIGH_FIDELITY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>High fidelity</p>"},{"location":"sdk/reference/api/synthesizers/regular/#ydata.sdk.synthesizers.PrivacyLevel.HIGH_PRIVACY","title":"<code>HIGH_PRIVACY = 'HIGH_PRIVACY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>High privacy</p>"},{"location":"sdk/reference/api/synthesizers/timeseries/","title":"TimeSeries","text":"<p>             Bases: <code>BaseSynthesizer</code></p> Source code in <code>ydata/sdk/synthesizers/timeseries.py</code> <pre><code>class TimeSeriesSynthesizer(BaseSynthesizer):\ndef sample(self, n_entities: int, condition_on: Optional[dict] = None) -&gt; pdDataFrame:\n\"\"\"Sample from a [`TimeSeriesSynthesizer`][ydata.sdk.synthesizers.TimeSeriesSynthesizer] instance.\n        If a training dataset was not using any `entity` column, the Synthesizer assumes a single entity.\n        A [`TimeSeriesSynthesizer`][ydata.sdk.synthesizers.TimeSeriesSynthesizer] always sample the full trajectory of its entities.\n        Arguments:\n            n_entities (int): number of entities to sample\n            condition_on: (Optional[dict]): (optional) conditional sampling parameters\n        Returns:\n            synthetic data\n        \"\"\"\nif n_entities is not None and n_entities &lt; 1:\nraise InputError(\"Parameter 'n_entities' must be greater than 0\")\npayload = {\"numberOfRecords\": n_entities}\nif condition_on is not None:\npayload[\"extraData\"] = {\n\"condition_on\": condition_on\n}\nreturn self._sample(payload=payload)\ndef fit(self, X: Union[DataSource, pdDataFrame],\nsortbykey: Optional[Union[str, List[str]]],\nprivacy_level: PrivacyLevel = PrivacyLevel.HIGH_FIDELITY,\nentities: Optional[Union[str, List[str]]] = None,\ngenerate_cols: Optional[List[str]] = None,\nexclude_cols: Optional[List[str]] = None,\ndtypes: Optional[Dict[str, Union[str, DataType]]] = None,\ntarget: Optional[str] = None,\nanonymize: Optional[dict] = None,\ncondition_on: Optional[List[str]] = None) -&gt; None:\n\"\"\"Fit the synthesizer.\n        The synthesizer accepts as training dataset either a pandas [`DataFrame`][pandas.DataFrame] directly or a YData [`DataSource`][ydata.sdk.datasources.DataSource].\n        Arguments:\n            X (Union[DataSource, pandas.DataFrame]): Training dataset\n            sortbykey (Union[str, List[str]]): column(s) to use to sort timeseries datasets\n            privacy_level (PrivacyLevel): Synthesizer privacy level (defaults to high fidelity)\n            entities (Union[str, List[str]]): (optional) columns representing entities ID\n            generate_cols (List[str]): (optional) columns that should be synthesized\n            exclude_cols (List[str]): (optional) columns that should not be synthesized\n            dtypes (Dict[str, Union[str, DataType]]): (optional) datatype mapping that will overwrite the datasource metadata column datatypes\n            target (Optional[str]): (optional) Metadata associated to the datasource\n            name (Optional[str]): (optional) Synthesizer instance name\n            anonymize (Optional[str]): (optional) fields to anonymize and the anonymization strategy\n            condition_on: (Optional[List[str]]): (optional) list of features to condition upon\n        \"\"\"\nBaseSynthesizer.fit(self, X=X, datatype=DataSourceType.TIMESERIES, sortbykey=sortbykey,\nentities=entities, generate_cols=generate_cols, exclude_cols=exclude_cols,\ndtypes=dtypes, target=target, anonymize=anonymize, privacy_level=privacy_level,\ncondition_on=condition_on)\ndef __repr__(self):\nif self._model is not None:\nreturn self._model.__repr__()\nelse:\nreturn \"TimeSeriesSynthesizer(Not Initialized)\"\n</code></pre>"},{"location":"sdk/reference/api/synthesizers/timeseries/#ydata.sdk.synthesizers.timeseries.TimeSeriesSynthesizer.fit","title":"<code>fit(X, sortbykey, privacy_level=PrivacyLevel.HIGH_FIDELITY, entities=None, generate_cols=None, exclude_cols=None, dtypes=None, target=None, anonymize=None, condition_on=None)</code>","text":"<p>Fit the synthesizer.</p> <p>The synthesizer accepts as training dataset either a pandas <code>DataFrame</code> directly or a YData <code>DataSource</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[DataSource, DataFrame]</code> <p>Training dataset</p> required <code>sortbykey</code> <code>Union[str, List[str]]</code> <p>column(s) to use to sort timeseries datasets</p> required <code>privacy_level</code> <code>PrivacyLevel</code> <p>Synthesizer privacy level (defaults to high fidelity)</p> <code>HIGH_FIDELITY</code> <code>entities</code> <code>Union[str, List[str]]</code> <p>(optional) columns representing entities ID</p> <code>None</code> <code>generate_cols</code> <code>List[str]</code> <p>(optional) columns that should be synthesized</p> <code>None</code> <code>exclude_cols</code> <code>List[str]</code> <p>(optional) columns that should not be synthesized</p> <code>None</code> <code>dtypes</code> <code>Dict[str, Union[str, DataType]]</code> <p>(optional) datatype mapping that will overwrite the datasource metadata column datatypes</p> <code>None</code> <code>target</code> <code>Optional[str]</code> <p>(optional) Metadata associated to the datasource</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>(optional) Synthesizer instance name</p> required <code>anonymize</code> <code>Optional[str]</code> <p>(optional) fields to anonymize and the anonymization strategy</p> <code>None</code> <code>condition_on</code> <code>Optional[List[str]]</code> <p>(Optional[List[str]]): (optional) list of features to condition upon</p> <code>None</code> Source code in <code>ydata/sdk/synthesizers/timeseries.py</code> <pre><code>def fit(self, X: Union[DataSource, pdDataFrame],\nsortbykey: Optional[Union[str, List[str]]],\nprivacy_level: PrivacyLevel = PrivacyLevel.HIGH_FIDELITY,\nentities: Optional[Union[str, List[str]]] = None,\ngenerate_cols: Optional[List[str]] = None,\nexclude_cols: Optional[List[str]] = None,\ndtypes: Optional[Dict[str, Union[str, DataType]]] = None,\ntarget: Optional[str] = None,\nanonymize: Optional[dict] = None,\ncondition_on: Optional[List[str]] = None) -&gt; None:\n\"\"\"Fit the synthesizer.\n    The synthesizer accepts as training dataset either a pandas [`DataFrame`][pandas.DataFrame] directly or a YData [`DataSource`][ydata.sdk.datasources.DataSource].\n    Arguments:\n        X (Union[DataSource, pandas.DataFrame]): Training dataset\n        sortbykey (Union[str, List[str]]): column(s) to use to sort timeseries datasets\n        privacy_level (PrivacyLevel): Synthesizer privacy level (defaults to high fidelity)\n        entities (Union[str, List[str]]): (optional) columns representing entities ID\n        generate_cols (List[str]): (optional) columns that should be synthesized\n        exclude_cols (List[str]): (optional) columns that should not be synthesized\n        dtypes (Dict[str, Union[str, DataType]]): (optional) datatype mapping that will overwrite the datasource metadata column datatypes\n        target (Optional[str]): (optional) Metadata associated to the datasource\n        name (Optional[str]): (optional) Synthesizer instance name\n        anonymize (Optional[str]): (optional) fields to anonymize and the anonymization strategy\n        condition_on: (Optional[List[str]]): (optional) list of features to condition upon\n    \"\"\"\nBaseSynthesizer.fit(self, X=X, datatype=DataSourceType.TIMESERIES, sortbykey=sortbykey,\nentities=entities, generate_cols=generate_cols, exclude_cols=exclude_cols,\ndtypes=dtypes, target=target, anonymize=anonymize, privacy_level=privacy_level,\ncondition_on=condition_on)\n</code></pre>"},{"location":"sdk/reference/api/synthesizers/timeseries/#ydata.sdk.synthesizers.timeseries.TimeSeriesSynthesizer.sample","title":"<code>sample(n_entities, condition_on=None)</code>","text":"<p>Sample from a <code>TimeSeriesSynthesizer</code> instance.</p> <p>If a training dataset was not using any <code>entity</code> column, the Synthesizer assumes a single entity. A <code>TimeSeriesSynthesizer</code> always sample the full trajectory of its entities.</p> <p>Parameters:</p> Name Type Description Default <code>n_entities</code> <code>int</code> <p>number of entities to sample</p> required <code>condition_on</code> <code>Optional[dict]</code> <p>(Optional[dict]): (optional) conditional sampling parameters</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>synthetic data</p> Source code in <code>ydata/sdk/synthesizers/timeseries.py</code> <pre><code>def sample(self, n_entities: int, condition_on: Optional[dict] = None) -&gt; pdDataFrame:\n\"\"\"Sample from a [`TimeSeriesSynthesizer`][ydata.sdk.synthesizers.TimeSeriesSynthesizer] instance.\n    If a training dataset was not using any `entity` column, the Synthesizer assumes a single entity.\n    A [`TimeSeriesSynthesizer`][ydata.sdk.synthesizers.TimeSeriesSynthesizer] always sample the full trajectory of its entities.\n    Arguments:\n        n_entities (int): number of entities to sample\n        condition_on: (Optional[dict]): (optional) conditional sampling parameters\n    Returns:\n        synthetic data\n    \"\"\"\nif n_entities is not None and n_entities &lt; 1:\nraise InputError(\"Parameter 'n_entities' must be greater than 0\")\npayload = {\"numberOfRecords\": n_entities}\nif condition_on is not None:\npayload[\"extraData\"] = {\n\"condition_on\": condition_on\n}\nreturn self._sample(payload=payload)\n</code></pre>"},{"location":"sdk/reference/api/synthesizers/timeseries/#privacylevel","title":"PrivacyLevel","text":"<p>             Bases: <code>StringEnum</code></p> <p>Privacy level exposed to the end-user.</p>"},{"location":"sdk/reference/api/synthesizers/timeseries/#ydata.sdk.synthesizers.PrivacyLevel.BALANCED_PRIVACY_FIDELITY","title":"<code>BALANCED_PRIVACY_FIDELITY = 'BALANCED_PRIVACY_FIDELITY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Balanced privacy/fidelity</p>"},{"location":"sdk/reference/api/synthesizers/timeseries/#ydata.sdk.synthesizers.PrivacyLevel.HIGH_FIDELITY","title":"<code>HIGH_FIDELITY = 'HIGH_FIDELITY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>High fidelity</p>"},{"location":"sdk/reference/api/synthesizers/timeseries/#ydata.sdk.synthesizers.PrivacyLevel.HIGH_PRIVACY","title":"<code>HIGH_PRIVACY = 'HIGH_PRIVACY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>High privacy</p>"},{"location":"support/help-troubleshooting/","title":"Help &amp; Troubleshooting","text":""}]}
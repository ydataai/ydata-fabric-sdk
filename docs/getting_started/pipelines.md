# Pipelines

Pipelines allow data scientists to treat code snippets as building blocks, enabling them to **build scalable and complex workflows** while leveraging Jupyter Notebooks or Python scripts. From data transformations to data synthesis, model training, or inference, **pipelines enable the orchestration of any job**, allowing data teams to explore and compare the effects of different data preparation decisions. 

<p align="center"><iframe width="600" height="400" src="https://www.youtube.com/embed/feNoXv34waM" title="How to build data quality pipelines with YData Fabric" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>

Additionally, pipelines offer the following:

- Customizable infrastructure configurations per step for optimal performance;
- Git versioning for reproducibility and version control;
- Comparison and versioning of pipeline runs;
- Configurable recurrency to automate repeated tasks.

???+ example "Learn how to use Pipelines with YData's Academy examples"
    To understand how to best apply the full capabilities of Pipelines in real world use cases, check out the [use cases section of YDataâ€™s Academy](https://github.com/ydataai/academy/tree/master/4%20-%20Use%20Cases). Most use cases include a pipeline leveraging common and use-case specific features of the Pipelines module. These pipelines are offered in `.pipeline` files which can be interactively explored in Jupyter Lab, inside Labs.